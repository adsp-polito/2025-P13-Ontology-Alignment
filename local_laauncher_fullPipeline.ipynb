{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53d98fe",
   "metadata": {},
   "source": [
    "# Ontology Alignment — Full Pipeline (Local Notebook)\n",
    "\n",
    "This notebook runs the **entire pipeline locally**:\n",
    "\n",
    "1. **Dataset construction** (build source/target CSV + build training dataset + splits)\n",
    "2. **Training** (cross-encoder fine-tuning)\n",
    "3. **Offline bundle building** (ontology_internal.csv + offline_bundle.pkl, with optional semantic index)\n",
    "4. **Inference** (retrieval + cross-encoder scoring → predictions.csv)\n",
    "\n",
    "It is designed to be:\n",
    "- **reproducible**: every run writes to `outputs/<RUN_ID>/...`\n",
    "- **modular**: you can run only the stages you need via flags\n",
    "- **terminal-free**: commands are launched via notebook cells and logged to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d101a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e2014",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "Run this section once per environment.\n",
    "\n",
    "Two typical workflows:\n",
    "\n",
    "- **You are already inside the repo**  \n",
    "  Just install dependencies (once) and continue.\n",
    "\n",
    "- **You want the notebook to clone the repo**  \n",
    "  Clone, `cd` into it, install dependencies.\n",
    "\n",
    "Notes:\n",
    "- In Jupyter, `!pip install ...` installs into the kernel environment.\n",
    "- Make sure the notebook kernel is the same env you intend to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a1c3ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.5 (main, Jun 11 2025, 15:36:57) [Clang 17.0.0 (clang-1700.0.13.3)]\n",
      "REPO_ROOT: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- OPTIONAL: clone repo ---\n",
    "# !git clone https://github.com/adsp-polito/2025-P13-Ontology-Alignment.git\n",
    "# %cd 2025-P13-Ontology-Alignment\n",
    "\n",
    "# --- OPTIONAL: install deps ---\n",
    "# If you have requirements.txt:\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "# Fallback minimal (only if needed):\n",
    "# !pip install -U sentence-transformers transformers torch pandas numpy scikit-learn\n",
    "\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011b51c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0916a",
   "metadata": {},
   "source": [
    "## 1) Helpers (run_cmd + logs) + robust W&B disable\n",
    "\n",
    "We run the pipeline scripts (`training.py`, `build_ontology_bundle.py`, `run_inference.py`) as subprocesses.\n",
    "Each command writes a log file. If the command fails, the notebook prints the tail of the log.\n",
    "\n",
    "Important:\n",
    "- Some training stacks try to use Weights & Biases (wandb).  \n",
    "  In local setups this can fail (missing API key).  \n",
    "  We force-disable wandb inside subprocess environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc04bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers OK.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def print_tail(path: Path, n=120):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"[tail] log not found: {p}\")\n",
    "        return\n",
    "    lines = p.read_text(errors=\"replace\").splitlines()\n",
    "    print(\"\\n\".join(lines[-n:]))\n",
    "\n",
    "def run_cmd(cmd, log_path: Path, cwd: Path):\n",
    "    cmd = [str(x) for x in cmd]\n",
    "    log_path = Path(log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"CWD:\", Path(cwd).resolve())\n",
    "    print(\"Log:\", log_path.resolve())\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env[\"WANDB_MODE\"] = \"disabled\"\n",
    "    env[\"WANDB_SILENT\"] = \"true\"\n",
    "    env[\"PYTHONPATH\"] = str(Path(cwd).resolve()) + os.pathsep + env.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        f.write(\"CMD: \" + \" \".join(cmd) + \"\\n\")\n",
    "        f.write(\"CWD: \" + str(Path(cwd).resolve()) + \"\\n\\n\")\n",
    "        proc = subprocess.run(\n",
    "            cmd,\n",
    "            stdout=f,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            cwd=str(cwd),\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"!!! Error occurred. Last lines of log:\")\n",
    "        print_tail(log_path, n=120)\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}. See log: {log_path}\")\n",
    "    return proc.returncode\n",
    "\n",
    "print(\"Helpers OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d111bb4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043265c8",
   "metadata": {},
   "source": [
    "## 2) Run mode flags (choose what to run today)\n",
    "\n",
    "This notebook supports two styles:\n",
    "\n",
    "- **Full pipeline**: Training → Offline → Inference (one run)\n",
    "- **Stage-by-stage**: run only the stages you need\n",
    "\n",
    "You can also “restore” artifacts (point to existing files) and skip rebuilding.\n",
    "\n",
    "Key dependencies:\n",
    "- Inference requires offline artifacts (bundle + ontology CSV).\n",
    "- Inference requires a cross-encoder model id/path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a472c3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flags OK.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RUN MODE FLAGS (choose what to run today)\n",
    "# ============================================\n",
    "\n",
    "# Main toggles: what stages to execute\n",
    "DO_TRAINING  = True\n",
    "DO_OFFLINE   = True\n",
    "DO_INFERENCE = True\n",
    "\n",
    "# Restore toggles: if True, skip building that stage and load artifacts instead\n",
    "RESTORE_MODEL   = False   # restores cross-encoder (+ optionally custom inference input CSV/schema)\n",
    "RESTORE_OFFLINE = False   # restores offline bundle + ontology CSV\n",
    "\n",
    "# If you restore artifacts, you skip rebuilding them.\n",
    "if RESTORE_MODEL and DO_TRAINING:\n",
    "    DO_TRAINING = False\n",
    "    print(\"RESTORE_MODEL=True => forcing DO_TRAINING=False (using restored model).\")\n",
    "\n",
    "if RESTORE_OFFLINE and DO_OFFLINE:\n",
    "    DO_OFFLINE = False\n",
    "    print(\"RESTORE_OFFLINE=True => forcing DO_OFFLINE=False (using restored offline artifacts).\")\n",
    "\n",
    "# Soft reminder (no hard stop)\n",
    "if DO_INFERENCE and not (DO_TRAINING or RESTORE_MODEL):\n",
    "    print(\"Note: inference requires CROSS_ENCODER_MODEL_ID (from training or restore).\")\n",
    "if DO_INFERENCE and not (DO_OFFLINE or RESTORE_OFFLINE):\n",
    "    print(\"Note: inference requires offline artifacts (from offline stage, restore, or existing paths on disk).\")\n",
    "\n",
    "print(\"Flags OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f0d49",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e4704",
   "metadata": {},
   "source": [
    "## 3) Configuration (always run)\n",
    "\n",
    "This cell defines:\n",
    "- the **run directory** (`outputs/<RUN_ID>/...`) used by all stages\n",
    "- all **inputs** (ontologies, alignments)\n",
    "- all **model choices** (cross-encoder + bi-encoder + tokenizer)\n",
    "- all **canonical artifact paths** (dataset CSVs, model dir, offline bundle, inference outputs)\n",
    "\n",
    "Rule of thumb:\n",
    "- Run this cell **every time** you open the notebook.\n",
    "- You can later override specific parameters (e.g., inference top-k, custom input CSV) without changing this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225e7acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject\n",
      "RUN_ID: unified_run_20260109_102452\n",
      "OUT_DIR: outputs/unified_run_20260109_102452\n",
      "Config OK.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION (unified training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "assert \"REPO_ROOT\" in globals(), \"REPO_ROOT not set. Run the Setup cell first.\"\n",
    "REPO_ROOT = Path(REPO_ROOT).resolve()\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# Run id / output layout\n",
    "# -----------------------------\n",
    "RUN_ID = f\"unified_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUT_DIR = Path(\"outputs\") / RUN_ID\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = OUT_DIR / \"training\"\n",
    "OFFLINE_DIR = OUT_DIR / \"offline\"\n",
    "INFER_DIR = OUT_DIR / \"inference\"\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OFFLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INFER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Training mode and model\n",
    "# -----------------------------\n",
    "RUN_MODE = \"full\"  # \"full\" | \"build-dataset\" | \"train-only\"\n",
    "MODEL_TYPE = \"cross-encoder\"  # keep this if you want inference at the end\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "HYPERPARAMETER_TUNING = False\n",
    "N_TRIALS = 5\n",
    "\n",
    "USE_FIXED_HYPERPARAMS = True\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "SPLIT_RATIOS = \"0.75,0.15,0.10\"\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs for dataset building\n",
    "# -----------------------------\n",
    "SRC_PATH = \"datasets/sweet.owl\"\n",
    "TGT_PATH = \"datasets/envo.owl\"\n",
    "ALIGN_PATH = \"datasets/envo-sweet.rdf\"\n",
    "\n",
    "SRC_PREFIX = None\n",
    "TGT_PREFIX = \"http://purl.obolibrary.org/obo/ENVO_\"  # e.g. \"http://purl.obolibrary.org/obo/ENVO_\"\n",
    "\n",
    "USE_DESCRIPTION = True\n",
    "USE_SYNONYMS = True\n",
    "USE_PARENTS = True\n",
    "USE_EQUIVALENT = True\n",
    "USE_DISJOINT = True\n",
    "\n",
    "VISUALIZE = False\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical outputs of STEP 1\n",
    "# -----------------------------\n",
    "OUT_SRC_CSV = str(TRAIN_DIR / \"source_ontology.csv\")\n",
    "OUT_TGT_CSV = str(TRAIN_DIR / \"target_ontology.csv\")\n",
    "OUT_DATASET_CSV = str(TRAIN_DIR / \"training_dataset.csv\")\n",
    "\n",
    "TRAIN_SPLIT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".train.csv\"))\n",
    "VAL_SPLIT_CSV   = str(Path(OUT_DATASET_CSV).with_suffix(\".val.csv\"))\n",
    "TEST_SPLIT_CSV  = str(Path(OUT_DATASET_CSV).with_suffix(\".test.csv\"))\n",
    "\n",
    "# train-only mode\n",
    "DATASET_CSV = TRAIN_SPLIT_CSV  # default, used only if RUN_MODE==\"train-only\"\n",
    "# Tip: if RUN_MODE==\"train-only\", set DATASET_CSV to an existing dataset path (train split or full dataset)\n",
    "\n",
    "# model outputs\n",
    "MODEL_OUT_DIR = str(TRAIN_DIR / \"models\" / f\"{MODEL_TYPE}_custom\")\n",
    "FINAL_CROSS_ENCODER_DIR = str(Path(MODEL_OUT_DIR) / \"final_cross_encoder_model\")\n",
    "\n",
    "# -----------------------------\n",
    "# Offline bundle builder\n",
    "# -----------------------------\n",
    "OFFLINE_EXPORT_CSV = None\n",
    "OFFLINE_ONT_PATH = TGT_PATH\n",
    "OFFLINE_PREFIX = TGT_PREFIX\n",
    "\n",
    "# Tokenizer used by cross-encoder scoring (keep aligned with cross-encoder)\n",
    "CROSS_TOKENIZER_NAME = MODEL_NAME\n",
    "\n",
    "# Bi-encoder used ONLY for semantic embeddings in offline bundle / semantic retrieval\n",
    "BI_ENCODER_MODEL_ID = \"allenai/scibert_scivocab_uncased\"\n",
    "OFFLINE_SEMANTIC_BATCH_SIZE = 64\n",
    "OFFLINE_SEMANTIC_MAX_LENGTH = 256\n",
    "OFFLINE_NO_SEMANTIC_NORMALIZE = False\n",
    "\n",
    "ONTOLOGY_INTERNAL_CSV = str(OFFLINE_DIR / \"ontology_internal.csv\")\n",
    "OFFLINE_BUNDLE_PKL = str(OFFLINE_DIR / \"offline_bundle.pkl\")\n",
    "\n",
    "# -----------------------------\n",
    "# Inference\n",
    "# -----------------------------\n",
    "# In full pipeline: default to the trained model location.\n",
    "# In restore mode: this will be overwritten by the restore cell.\n",
    "CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "\n",
    "INFER_INPUT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".test.queries.csv\"))  # by default, use test split queries\n",
    "INFER_OUT_CSV = str(INFER_DIR / \"predictions.csv\")\n",
    "\n",
    "RETRIEVAL_COL = \"source_label\"\n",
    "SCORING_COL = \"source_text\"\n",
    "ID_COL = \"source_iri\"\n",
    "\n",
    "INFER_MODE = \"hybrid\"\n",
    "RETRIEVAL_LEXICAL_TOP_K = 100\n",
    "RETRIEVAL_SEMANTIC_TOP_K = 100\n",
    "RETRIEVAL_MERGED_TOP_K = 150\n",
    "HYBRID_RATIO_SEMANTIC = 0.2\n",
    "SEMANTIC_BATCH_SIZE = 64\n",
    "\n",
    "CROSS_TOP_K = 20\n",
    "CROSS_BATCH_SIZE = 32\n",
    "CROSS_MAX_LENGTH = 256\n",
    "\n",
    "KEEP_TOP_N = 20\n",
    "\n",
    "print(\"Config OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4973492d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834164b7",
   "metadata": {},
   "source": [
    "## 4) Local restore (optional)\n",
    "\n",
    "If you already have artifacts from a previous run (local disk), you can skip rebuilding:\n",
    "\n",
    "- **Restore offline artifacts**:\n",
    "  - `offline_bundle.pkl`\n",
    "  - `ontology_internal.csv`\n",
    "\n",
    "- **Restore model artifacts**:\n",
    "  - a folder containing a saved SentenceTransformers CrossEncoder (must contain `config.json`)\n",
    "\n",
    "These cells simply **override** the paths used downstream.\n",
    "If you are running the full pipeline today, you can skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55b5068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping offline restore (RESTORE_OFFLINE=False).\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RESTORE OFFLINE ARTIFACTS (local)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "if not RESTORE_OFFLINE:\n",
    "    print(\"Skipping offline restore (RESTORE_OFFLINE=False).\")\n",
    "else:\n",
    "    # Point to a directory that contains offline_bundle.pkl and ontology_internal.csv\n",
    "    # Example:\n",
    "    # OFFLINE_RESTORE_SRC = \"outputs/unified_run_20260101_120000/offline\"\n",
    "    OFFLINE_RESTORE_SRC = None  # <-- set me\n",
    "\n",
    "    if not OFFLINE_RESTORE_SRC:\n",
    "        raise ValueError(\"Set OFFLINE_RESTORE_SRC to a folder containing offline_bundle.pkl and ontology_internal.csv\")\n",
    "\n",
    "    restore_root = Path(OFFLINE_RESTORE_SRC).expanduser().resolve()\n",
    "\n",
    "    if not restore_root.exists():\n",
    "        raise FileNotFoundError(f\"OFFLINE_RESTORE_SRC not found: {restore_root}\")\n",
    "\n",
    "    bundle_pkl = restore_root / \"offline_bundle.pkl\"\n",
    "    onto_csv   = restore_root / \"ontology_internal.csv\"\n",
    "\n",
    "    if not bundle_pkl.exists() or not onto_csv.exists():\n",
    "        # fallback: search recursively (but deterministic: sort)\n",
    "        pkls = sorted(restore_root.rglob(\"offline_bundle.pkl\"))\n",
    "        csvs = sorted(restore_root.rglob(\"ontology_internal.csv\"))\n",
    "        bundle_pkl = pkls[0] if pkls else None\n",
    "        onto_csv = csvs[0] if csvs else None\n",
    "\n",
    "    if bundle_pkl is None or onto_csv is None:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find offline_bundle.pkl and/or ontology_internal.csv under: {restore_root}\"\n",
    "        )\n",
    "\n",
    "    OFFLINE_BUNDLE_PKL = str(bundle_pkl)\n",
    "    ONTOLOGY_INTERNAL_CSV = str(onto_csv)\n",
    "\n",
    "    print(\"Restored offline artifacts:\")\n",
    "    print(\"   OFFLINE_BUNDLE_PKL    =\", OFFLINE_BUNDLE_PKL)\n",
    "    print(\"   ONTOLOGY_INTERNAL_CSV =\", ONTOLOGY_INTERNAL_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e70b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model restore (RESTORE_MODEL=False).\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RESTORE MODEL ARTIFACTS (local)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "if not RESTORE_MODEL:\n",
    "    print(\"Skipping model restore (RESTORE_MODEL=False).\")\n",
    "else:\n",
    "    # Point to a directory that is the saved CrossEncoder folder (contains config.json),\n",
    "    # OR a parent directory that contains such a folder.\n",
    "    # Example:\n",
    "    # MODEL_RESTORE_SRC = \"outputs/unified_run_20260101_120000/training/models/cross-encoder_custom/final_cross_encoder_model\"\n",
    "    MODEL_RESTORE_SRC = None  # <-- set me\n",
    "\n",
    "    if not MODEL_RESTORE_SRC:\n",
    "        raise ValueError(\"Set MODEL_RESTORE_SRC to a saved CrossEncoder folder (or a parent folder containing it).\")\n",
    "\n",
    "    restore_root = Path(MODEL_RESTORE_SRC).expanduser().resolve()\n",
    "    if not restore_root.exists():\n",
    "        raise FileNotFoundError(f\"MODEL_RESTORE_SRC not found: {restore_root}\")\n",
    "\n",
    "    def _find_cross_encoder_dir(root: Path) -> Path:\n",
    "        if (root / \"config.json\").exists():\n",
    "            return root\n",
    "        candidates = list(root.rglob(\"config.json\"))\n",
    "        if not candidates:\n",
    "            raise FileNotFoundError(f\"Could not find config.json under: {root}\")\n",
    "        return candidates[0].parent\n",
    "    \n",
    "    def _is_model_dir(d: Path) -> bool:\n",
    "        return (d / \"config.json\").exists() and ((d / \"pytorch_model.bin\").exists() or (d / \"model.safetensors\").exists())\n",
    "\n",
    "    cross_dir = _find_cross_encoder_dir(restore_root)\n",
    "    flag = _is_model_dir(cross_dir)\n",
    "    if not flag:\n",
    "        raise FileNotFoundError(f\"Restored cross-encoder model dir is invalid (missing model files): {cross_dir}\")\n",
    "    CROSS_ENCODER_MODEL_ID = str(cross_dir)\n",
    "\n",
    "    print(\"Restored cross-encoder model dir:\")\n",
    "    print(\"   CROSS_ENCODER_MODEL_ID =\", CROSS_ENCODER_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b389df4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b45ac6",
   "metadata": {},
   "source": [
    "## 5) Run pipeline (Training → Offline → Inference)\n",
    "\n",
    "This cell executes the stages selected by `DO_TRAINING`, `DO_OFFLINE`, `DO_INFERENCE`.\n",
    "\n",
    "- Each stage writes a log file under the current run folder.\n",
    "- If a stage fails, the notebook prints the last lines of the log and stops.\n",
    "- Artifact paths come from **Configuration**, unless overridden by **Restore** cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bb8ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running command:\n",
      " python training.py --mode full --src datasets/sweet.owl --tgt datasets/envo.owl --align datasets/envo-sweet.rdf --out-src outputs/unified_run_20260109_102452/training/source_ontology.csv --out-tgt outputs/unified_run_20260109_102452/training/target_ontology.csv --out-dataset outputs/unified_run_20260109_102452/training/training_dataset.csv --split-ratios 0.75,0.15,0.10 --tgt-prefix http://purl.obolibrary.org/obo/ENVO_ --src-use-description --src-use-synonyms --src-use-parents --src-use-equivalent --src-use-disjoint --model-type cross-encoder --model-name allenai/scibert_scivocab_uncased --model-output-dir outputs/unified_run_20260109_102452/training/models/cross-encoder_custom --num-epochs 10 --learning-rate 3e-05 --batch-size 16 --weight-decay 0.01\n",
      "CWD: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject\n",
      "Log: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/outputs/unified_run_20260109_102452/training/training.log\n",
      "Return code: 1\n",
      "!!! Error occurred. Last lines of log:\n",
      " 92%|█████████▏| 12/13 [00:05<00:00,  2.53it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 13/13 [00:06<00:00,  1.99it/s]\u001b[ATraceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/training.py\"\u001b[0m, line \u001b[35m333\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/training.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mtrain_model\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mdf_train=df_train,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<8 lines>...\n",
      "        \u001b[1;31mnum_epochs=args.num_epochs\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/training/train.py\"\u001b[0m, line \u001b[35m44\u001b[0m, in \u001b[35mtrain_model\u001b[0m\n",
      "    \u001b[31mtrain_cross_encoder\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mdf_train,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "    ...<7 lines>...\n",
      "        \u001b[1;31moutput_dir=output_dir\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/training/cross_encoder_training.py\"\u001b[0m, line \u001b[35m210\u001b[0m, in \u001b[35mtrain_cross_encoder\u001b[0m\n",
      "    \u001b[31mtrainer.train\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/transformers/trainer.py\"\u001b[0m, line \u001b[35m2325\u001b[0m, in \u001b[35mtrain\u001b[0m\n",
      "    return inner_training_loop(\n",
      "        args=args,\n",
      "    ...<2 lines>...\n",
      "        ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "    )\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/transformers/trainer.py\"\u001b[0m, line \u001b[35m2790\u001b[0m, in \u001b[35m_inner_training_loop\u001b[0m\n",
      "    \u001b[31mself._maybe_log_save_evaluate\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mtr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate=learning_rate\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/transformers/trainer.py\"\u001b[0m, line \u001b[35m3221\u001b[0m, in \u001b[35m_maybe_log_save_evaluate\u001b[0m\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/transformers/trainer.py\"\u001b[0m, line \u001b[35m3170\u001b[0m, in \u001b[35m_evaluate\u001b[0m\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sentence_transformers/trainer.py\"\u001b[0m, line \u001b[35m545\u001b[0m, in \u001b[35mevaluate\u001b[0m\n",
      "    return \u001b[31msuper().evaluate\u001b[0m\u001b[1;31m(eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/transformers/trainer.py\"\u001b[0m, line \u001b[35m4489\u001b[0m, in \u001b[35mevaluate\u001b[0m\n",
      "    output = eval_loop(\n",
      "        eval_dataloader,\n",
      "    ...<5 lines>...\n",
      "        metric_key_prefix=metric_key_prefix,\n",
      "    )\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sentence_transformers/trainer.py\"\u001b[0m, line \u001b[35m581\u001b[0m, in \u001b[35mevaluation_loop\u001b[0m\n",
      "    evaluator_metrics = self.evaluator(\n",
      "        self.model, output_path=output_path, epoch=self.state.epoch, steps=self.state.global_step\n",
      "    )\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sentence_transformers/cross_encoder/evaluation/classification.py\"\u001b[0m, line \u001b[35m121\u001b[0m, in \u001b[35m__call__\u001b[0m\n",
      "    ap = average_precision_score(self.labels, pred_scores)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\"\u001b[0m, line \u001b[35m218\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/metrics/_ranking.py\"\u001b[0m, line \u001b[35m270\u001b[0m, in \u001b[35maverage_precision_score\u001b[0m\n",
      "    return _average_binary_score(\n",
      "        average_precision, y_true, y_score, average, sample_weight=sample_weight\n",
      "    )\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/metrics/_base.py\"\u001b[0m, line \u001b[35m69\u001b[0m, in \u001b[35m_average_binary_score\u001b[0m\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/metrics/_ranking.py\"\u001b[0m, line \u001b[35m231\u001b[0m, in \u001b[35m_binary_uninterpolated_average_precision\u001b[0m\n",
      "    precision, recall, _ = \u001b[31mprecision_recall_curve\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31my_true, y_score, pos_label=pos_label, sample_weight=sample_weight\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\"\u001b[0m, line \u001b[35m191\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/metrics/_ranking.py\"\u001b[0m, line \u001b[35m1100\u001b[0m, in \u001b[35mprecision_recall_curve\u001b[0m\n",
      "    _, fps, _, tps, thresholds = \u001b[31mconfusion_matrix_at_thresholds\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31my_true, y_score, pos_label=pos_label, sample_weight=sample_weight\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\"\u001b[0m, line \u001b[35m191\u001b[0m, in \u001b[35mwrapper\u001b[0m\n",
      "    return func(*args, **kwargs)\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/metrics/_ranking.py\"\u001b[0m, line \u001b[35m934\u001b[0m, in \u001b[35mconfusion_matrix_at_thresholds\u001b[0m\n",
      "    \u001b[31massert_all_finite\u001b[0m\u001b[1;31m(y_score)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/utils/validation.py\"\u001b[0m, line \u001b[35m223\u001b[0m, in \u001b[35massert_all_finite\u001b[0m\n",
      "    \u001b[31m_assert_all_finite\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mX.data if sp.issparse(X) else X,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    ...<2 lines>...\n",
      "        \u001b[1;31minput_name=input_name,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/utils/validation.py\"\u001b[0m, line \u001b[35m133\u001b[0m, in \u001b[35m_assert_all_finite\u001b[0m\n",
      "    \u001b[31m_assert_all_finite_element_wise\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mX,\u001b[0m\n",
      "        \u001b[1;31m^^\u001b[0m\n",
      "    ...<4 lines>...\n",
      "        \u001b[1;31minput_name=input_name,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/lib/python3.13/site-packages/sklearn/utils/validation.py\"\u001b[0m, line \u001b[35m182\u001b[0m, in \u001b[35m_assert_all_finite_element_wise\u001b[0m\n",
      "    raise ValueError(msg_err)\n",
      "\u001b[1;35mValueError\u001b[0m: \u001b[35mInput contains NaN.\u001b[0m\n",
      "\n",
      " 10%|█         | 62/620 [03:22<30:21,  3.26s/it]\n",
      "\n",
      "\n",
      "                                               \u001b[ACMD: python training.py --mode full --src datasets/sweet.owl --tgt datasets/envo.owl --align datasets/envo-sweet.rdf --out-src outputs/unified_run_20260109_102452/training/source_ontology.csv --out-tgt outputs/unified_run_20260109_102452/training/target_ontology.csv --out-dataset outputs/unified_run_20260109_102452/training/training_dataset.csv --split-ratios 0.75,0.15,0.10 --tgt-prefix http://purl.obolibrary.org/obo/ENVO_ --src-use-description --src-use-synonyms --src-use-parents --src-use-equivalent --src-use-disjoint --model-type cross-encoder --model-name allenai/scibert_scivocab_uncased --model-output-dir outputs/unified_run_20260109_102452/training/models/cross-encoder_custom --num-epochs 10 --learning-rate 3e-05 --batch-size 16 --weight-decay 0.01\n",
      "CWD: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Command failed with return code 1. See log: outputs/unified_run_20260109_102452/training/training.log",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RUN_MODE == \u001b[33m\"\u001b[39m\u001b[33mtrain-only\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     52\u001b[39m     train_cmd += [\u001b[33m\"\u001b[39m\u001b[33m--dataset-csv\u001b[39m\u001b[33m\"\u001b[39m, DATASET_CSV]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mrun_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREPO_ROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset CSV:\u001b[39m\u001b[33m\"\u001b[39m, OUT_DATASET_CSV)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mrun_cmd\u001b[39m\u001b[34m(cmd, log_path, cwd)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m!!! Error occurred. Last lines of log:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m     print_tail(log_path, n=\u001b[32m120\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCommand failed with return code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproc.returncode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. See log: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m proc.returncode\n",
      "\u001b[31mRuntimeError\u001b[39m: Command failed with return code 1. See log: outputs/unified_run_20260109_102452/training/training.log"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RUN PIPELINE (training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Guardrails (hard)\n",
    "if RUN_MODE == \"full\" and MODEL_TYPE != \"cross-encoder\":\n",
    "    raise ValueError(\"RUN_MODE='full' ends with inference => needs MODEL_TYPE='cross-encoder'.\")\n",
    "if HYPERPARAMETER_TUNING and RUN_MODE != \"full\":\n",
    "    raise ValueError(\"--tune only allowed in RUN_MODE='full'.\")\n",
    "\n",
    "# -----------------------------\n",
    "# STAGE 1) TRAINING (+dataset)\n",
    "# -----------------------------\n",
    "if not DO_TRAINING:\n",
    "    print(\"Skipping training (DO_TRAINING=False).\")\n",
    "else:\n",
    "    Path(MODEL_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_log = TRAIN_DIR / \"training.log\"\n",
    "    train_cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "    if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "        train_cmd += [\"--src\", SRC_PATH, \"--tgt\", TGT_PATH, \"--align\", ALIGN_PATH]\n",
    "        train_cmd += [\"--out-src\", OUT_SRC_CSV, \"--out-tgt\", OUT_TGT_CSV, \"--out-dataset\", OUT_DATASET_CSV]\n",
    "        train_cmd += [\"--split-ratios\", SPLIT_RATIOS]\n",
    "\n",
    "        if SRC_PREFIX:\n",
    "            train_cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "        if TGT_PREFIX:\n",
    "            train_cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "        if USE_DESCRIPTION: train_cmd.append(\"--src-use-description\")\n",
    "        if USE_SYNONYMS: train_cmd.append(\"--src-use-synonyms\")\n",
    "        if USE_PARENTS: train_cmd.append(\"--src-use-parents\")\n",
    "        if USE_EQUIVALENT: train_cmd.append(\"--src-use-equivalent\")\n",
    "        if USE_DISJOINT: train_cmd.append(\"--src-use-disjoint\")\n",
    "        if VISUALIZE: train_cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "    if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "        train_cmd += [\"--model-type\", MODEL_TYPE, \"--model-name\", MODEL_NAME, \"--model-output-dir\", MODEL_OUT_DIR]\n",
    "        train_cmd += [\"--num-epochs\", str(NUM_EPOCHS)]\n",
    "\n",
    "        if HYPERPARAMETER_TUNING:\n",
    "            train_cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "        elif USE_FIXED_HYPERPARAMS:\n",
    "            train_cmd += [\"--learning-rate\", str(LEARNING_RATE)]\n",
    "            train_cmd += [\"--batch-size\", str(BATCH_SIZE)]\n",
    "            train_cmd += [\"--weight-decay\", str(WEIGHT_DECAY)]\n",
    "\n",
    "    if RUN_MODE == \"train-only\":\n",
    "        train_cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "    run_cmd(train_cmd, train_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(\"Dataset CSV:\", OUT_DATASET_CSV)\n",
    "    print(\"Train split:\", TRAIN_SPLIT_CSV)\n",
    "    print(\"Val split:\", VAL_SPLIT_CSV)\n",
    "    print(\"Test split:\", TEST_SPLIT_CSV)\n",
    "    print(\"Cross-encoder dir:\", FINAL_CROSS_ENCODER_DIR)\n",
    "\n",
    "    # In full pipeline, inference uses the newly trained model (unless overwritten later by restore)\n",
    "    CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# STAGE 2) OFFLINE BUNDLE\n",
    "# -----------------------------\n",
    "if not DO_OFFLINE:\n",
    "    print(\"Skipping offline bundle (DO_OFFLINE=False).\")\n",
    "else:\n",
    "    offline_log = OFFLINE_DIR / \"offline_bundle.log\"\n",
    "    offline_cmd = [\n",
    "        \"python\", \"build_ontology_bundle.py\",\n",
    "        \"--out-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "        \"--out-bundle\", OFFLINE_BUNDLE_PKL,\n",
    "        \"--tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "        \"--bi-encoder-model-id\", BI_ENCODER_MODEL_ID,\n",
    "        \"--semantic-batch-size\", str(OFFLINE_SEMANTIC_BATCH_SIZE),\n",
    "        \"--semantic-max-length\", str(OFFLINE_SEMANTIC_MAX_LENGTH),\n",
    "    ]\n",
    "    if OFFLINE_NO_SEMANTIC_NORMALIZE:\n",
    "        offline_cmd.append(\"--no-semantic-normalize\")\n",
    "\n",
    "    if OFFLINE_EXPORT_CSV:\n",
    "        offline_cmd += [\"--export-csv\", OFFLINE_EXPORT_CSV]\n",
    "    else:\n",
    "        offline_cmd += [\"--ont-path\", OFFLINE_ONT_PATH]\n",
    "        if OFFLINE_PREFIX:\n",
    "            offline_cmd += [\"--prefix\", OFFLINE_PREFIX]\n",
    "\n",
    "    run_cmd(offline_cmd, offline_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nOffline bundle completed.\")\n",
    "    print(\"Ontology internal CSV:\", ONTOLOGY_INTERNAL_CSV)\n",
    "    print(\"Offline bundle PKL:\", OFFLINE_BUNDLE_PKL)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# STAGE 3) INFERENCE\n",
    "# -----------------------------\n",
    "if not DO_INFERENCE:\n",
    "    print(\"Skipping inference (DO_INFERENCE=False).\")\n",
    "else:\n",
    "    # Final sanity checks (runtime)\n",
    "    if \"CROSS_ENCODER_MODEL_ID\" not in globals() or CROSS_ENCODER_MODEL_ID is None:\n",
    "        raise ValueError(\n",
    "            \"CROSS_ENCODER_MODEL_ID is not set. \"\n",
    "            \"Run training (DO_TRAINING=True) or restore model (RESTORE_MODEL=True).\"\n",
    "        )\n",
    "\n",
    "    if not Path(OFFLINE_BUNDLE_PKL).exists():\n",
    "        raise FileNotFoundError(f\"OFFLINE_BUNDLE_PKL not found: {OFFLINE_BUNDLE_PKL}\")\n",
    "    if not Path(ONTOLOGY_INTERNAL_CSV).exists():\n",
    "        raise FileNotFoundError(f\"ONTOLOGY_INTERNAL_CSV not found: {ONTOLOGY_INTERNAL_CSV}\")\n",
    "    if not Path(INFER_INPUT_CSV).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"INFER_INPUT_CSV not found: {INFER_INPUT_CSV}\\n\"\n",
    "            \"In full/build-dataset mode, training should generate *.test.queries.csv. \"\n",
    "            \"Otherwise set INFER_INPUT_CSV to your custom file.\"\n",
    "        )\n",
    "\n",
    "    infer_log = INFER_DIR / \"inference.log\"\n",
    "    infer_cmd = [\n",
    "        \"python\", \"run_inference.py\",\n",
    "        \"--bundle\", OFFLINE_BUNDLE_PKL,\n",
    "        \"--ontology-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "        \"--input-csv\", INFER_INPUT_CSV,\n",
    "        \"--out-csv\", INFER_OUT_CSV,\n",
    "        \"--mode\", INFER_MODE,\n",
    "        \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "        \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "        \"--retrieval-col\", RETRIEVAL_COL,\n",
    "        \"--retrieval-lexical-top-k\", str(RETRIEVAL_LEXICAL_TOP_K),\n",
    "        \"--retrieval-semantic-top-k\", str(RETRIEVAL_SEMANTIC_TOP_K),\n",
    "        \"--retrieval-merged-top-k\", str(RETRIEVAL_MERGED_TOP_K),\n",
    "        \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEMANTIC),\n",
    "        \"--semantic-batch-size\", str(SEMANTIC_BATCH_SIZE),\n",
    "        \"--cross-top-k\", str(CROSS_TOP_K),\n",
    "        \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "        \"--cross-max-length\", str(CROSS_MAX_LENGTH),\n",
    "        \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "    ]\n",
    "    if SCORING_COL:\n",
    "        infer_cmd += [\"--scoring-col\", SCORING_COL]\n",
    "    if ID_COL:\n",
    "        infer_cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "    run_cmd(infer_cmd, infer_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nInference completed.\")\n",
    "    print(\"Predictions CSV:\", INFER_OUT_CSV)\n",
    "\n",
    "print(\"\\nPipeline cell finished.\")\n",
    "print(\"Run folder:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbd2e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98abd88",
   "metadata": {},
   "source": [
    "## 6) Export helpers (ALWAYS RUN)\n",
    "\n",
    "All export cells use the same design:\n",
    "\n",
    "- Every ZIP contains the requested artifacts **plus** a `config.txt` snapshot.\n",
    "- `config.txt` is generated at export time.\n",
    "\n",
    ">ZIPs are created on disk and the path is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT HELPERS (config.txt + zip) — LOCAL\n",
    "# ============================================\n",
    "\n",
    "from ast import List\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import zipfile\n",
    "\n",
    "def write_config_txt(config_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Writes a config.txt snapshot of the current effective configuration.\n",
    "    This is intended to be called right before exporting ZIP artifacts.\n",
    "    \"\"\"\n",
    "    config_dir = Path(config_dir)\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    lines = [\n",
    "        \"# Ontology Alignment – Run Configuration\",\n",
    "        f\"# Generated on: {datetime.now().isoformat()}\",\n",
    "        \"\",\n",
    "        \"[Run]\",\n",
    "        f\"RUN_ID = {globals().get('RUN_ID', None)}\",\n",
    "        f\"OUT_DIR = {globals().get('OUT_DIR', None)}\",\n",
    "        f\"RUN_MODE = {globals().get('RUN_MODE', None)}\",\n",
    "        \"\",\n",
    "        \"[Model]\",\n",
    "        f\"MODEL_TYPE = {globals().get('MODEL_TYPE', None)}\",\n",
    "        f\"MODEL_NAME = {globals().get('MODEL_NAME', None)}\",\n",
    "        f\"CROSS_ENCODER_MODEL_ID = {globals().get('CROSS_ENCODER_MODEL_ID', None)}\",\n",
    "        f\"BI_ENCODER_MODEL_ID = {globals().get('BI_ENCODER_MODEL_ID', None)}\",\n",
    "        f\"CROSS_TOKENIZER_NAME = {globals().get('CROSS_TOKENIZER_NAME', None)}\",\n",
    "        \"\",\n",
    "        \"[Training]\",\n",
    "        f\"NUM_EPOCHS = {globals().get('NUM_EPOCHS', None)}\",\n",
    "        f\"LEARNING_RATE = {globals().get('LEARNING_RATE', None)}\",\n",
    "        f\"BATCH_SIZE = {globals().get('BATCH_SIZE', None)}\",\n",
    "        f\"WEIGHT_DECAY = {globals().get('WEIGHT_DECAY', None)}\",\n",
    "        f\"SPLIT_RATIOS = {globals().get('SPLIT_RATIOS', None)}\",\n",
    "        \"\",\n",
    "        \"[Offline]\",\n",
    "        f\"OFFLINE_ONT_PATH = {globals().get('OFFLINE_ONT_PATH', None)}\",\n",
    "        f\"OFFLINE_PREFIX = {globals().get('OFFLINE_PREFIX', None)}\",\n",
    "        f\"OFFLINE_SEMANTIC_BATCH_SIZE = {globals().get('OFFLINE_SEMANTIC_BATCH_SIZE', None)}\",\n",
    "        f\"OFFLINE_SEMANTIC_MAX_LENGTH = {globals().get('OFFLINE_SEMANTIC_MAX_LENGTH', None)}\",\n",
    "        f\"OFFLINE_NO_SEMANTIC_NORMALIZE = {globals().get('OFFLINE_NO_SEMANTIC_NORMALIZE', None)}\",\n",
    "        f\"OFFLINE_BUNDLE_PKL = {globals().get('OFFLINE_BUNDLE_PKL', None)}\",\n",
    "        f\"ONTOLOGY_INTERNAL_CSV = {globals().get('ONTOLOGY_INTERNAL_CSV', None)}\",\n",
    "        \"\",\n",
    "        \"[Inference]\",\n",
    "        f\"INFER_MODE = {globals().get('INFER_MODE', None)}\",\n",
    "        f\"INFER_INPUT_CSV = {globals().get('INFER_INPUT_CSV', None)}\",\n",
    "        f\"INFER_OUT_CSV = {globals().get('INFER_OUT_CSV', None)}\",\n",
    "        f\"RETRIEVAL_COL = {globals().get('RETRIEVAL_COL', None)}\",\n",
    "        f\"SCORING_COL = {globals().get('SCORING_COL', None)}\",\n",
    "        f\"ID_COL = {globals().get('ID_COL', None)}\",\n",
    "        f\"RETRIEVAL_LEXICAL_TOP_K = {globals().get('RETRIEVAL_LEXICAL_TOP_K', None)}\",\n",
    "        f\"RETRIEVAL_SEMANTIC_TOP_K = {globals().get('RETRIEVAL_SEMANTIC_TOP_K', None)}\",\n",
    "        f\"RETRIEVAL_MERGED_TOP_K = {globals().get('RETRIEVAL_MERGED_TOP_K', None)}\",\n",
    "        f\"HYBRID_RATIO_SEMANTIC = {globals().get('HYBRID_RATIO_SEMANTIC', None)}\",\n",
    "        f\"SEMANTIC_BATCH_SIZE = {globals().get('SEMANTIC_BATCH_SIZE', None)}\",\n",
    "        f\"CROSS_TOP_K = {globals().get('CROSS_TOP_K', None)}\",\n",
    "        f\"CROSS_BATCH_SIZE = {globals().get('CROSS_BATCH_SIZE', None)}\",\n",
    "        f\"CROSS_MAX_LENGTH = {globals().get('CROSS_MAX_LENGTH', None)}\",\n",
    "        f\"KEEP_TOP_N = {globals().get('KEEP_TOP_N', None)}\",\n",
    "    ]\n",
    "\n",
    "    config_path = config_dir / \"config.txt\"\n",
    "    config_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    return config_path\n",
    "\n",
    "\n",
    "def make_zip_with_config(\n",
    "    zip_path: Path,\n",
    "    files_to_include: List[Path],\n",
    "    config_dir: Optional[Path] = None,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Creates a ZIP containing existing artifacts + a config.txt snapshot.\n",
    "    Returns the created zip path.\n",
    "    \"\"\"\n",
    "    zip_path = Path(zip_path)\n",
    "    zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if config_dir is None:\n",
    "        config_dir = zip_path.parent\n",
    "    config_path = write_config_txt(Path(config_dir))\n",
    "\n",
    "    existing = [Path(p) for p in files_to_include if Path(p).exists()]\n",
    "    if not existing:\n",
    "        raise FileNotFoundError(\"None of the requested files exist. Nothing to zip.\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in existing:\n",
    "            z.write(p, arcname=p.name)\n",
    "        z.write(config_path, arcname=\"config.txt\")\n",
    "\n",
    "    print(\"Created ZIP:\", zip_path)\n",
    "    print(\"Included:\", [p.name for p in existing], \"+ config.txt\")\n",
    "    return zip_path\n",
    "\n",
    "\n",
    "print(\"Export helpers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f588931",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e45f67",
   "metadata": {},
   "source": [
    "## 7.1) Export inference outputs + gold (full pipeline)\n",
    "\n",
    "This is a **result extraction** utility intended for the **full pipeline** case:\n",
    "\n",
    "- `predictions.csv` must exist (produced by inference)\n",
    "- `*.test.gold.csv` must exist (produced during dataset build/split)\n",
    "\n",
    "It creates a single ZIP containing:\n",
    "- `predictions.csv`\n",
    "- `*.test.gold.csv`\n",
    "- `config.txt` (effective run config snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c441a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT ARTIFACTS (predictions + gold)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "pred_path = Path(INFER_OUT_CSV)\n",
    "gold_path = Path(str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\")))\n",
    "\n",
    "missing = [str(p) for p in [pred_path, gold_path] if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing required file(s):\\n\"\n",
    "        + \"\\n\".join(f\" - {m}\" for m in missing)\n",
    "        + \"\\n\\nNotes:\\n\"\n",
    "        \" - predictions.csv is produced by the inference stage.\\n\"\n",
    "        \" - *.test.gold.csv is produced during dataset construction/splitting.\\n\"\n",
    "    )\n",
    "\n",
    "zip_path = pred_path.parent / \"predictions_and_gold.zip\"\n",
    "make_zip_with_config(zip_path, [pred_path, gold_path], config_dir=pred_path.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d41e49",
   "metadata": {},
   "source": [
    "## 7.2) Export predictions + optional gold\n",
    "\n",
    "This export cell works in both scenarios:\n",
    "\n",
    "- Full pipeline: gold is auto-detected (`*.test.gold.csv`)\n",
    "- Inference-only: gold may be missing (that's fine), or you can provide a custom gold path\n",
    "\n",
    "It creates a ZIP containing:\n",
    "- `predictions.csv`\n",
    "- (optional) gold CSV if available\n",
    "- `config.txt` (effective run config snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775b776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT ARTIFACTS (predictions + optional gold)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: if you have an external gold file, set it here\n",
    "GOLD_PATH_OVERRIDE = None  # e.g. \"my_eval/gold_truth.csv\"\n",
    "\n",
    "pred_path = Path(INFER_OUT_CSV)\n",
    "if not pred_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"predictions file not found:\\n - {pred_path}\\n\\n\"\n",
    "        \"Run inference first or set INFER_OUT_CSV correctly.\"\n",
    "    )\n",
    "\n",
    "gold_candidates = []\n",
    "if GOLD_PATH_OVERRIDE is not None:\n",
    "    gold_candidates.append(Path(GOLD_PATH_OVERRIDE))\n",
    "\n",
    "gold_candidates.append(Path(str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\"))))\n",
    "\n",
    "gold_path = next((p for p in gold_candidates if p.exists()), None)\n",
    "\n",
    "files_to_zip = [pred_path]\n",
    "zip_name = \"predictions_only.zip\"\n",
    "\n",
    "if gold_path is not None:\n",
    "    files_to_zip.append(gold_path)\n",
    "    zip_name = \"predictions_and_gold.zip\"\n",
    "    print(\"Found gold:\", gold_path)\n",
    "else:\n",
    "    print(\"Gold not found (OK). Tried:\")\n",
    "    for p in gold_candidates:\n",
    "        print(\" -\", p)\n",
    "\n",
    "zip_path = pred_path.parent / zip_name\n",
    "make_zip_with_config(zip_path, files_to_zip, config_dir=pred_path.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb75ae",
   "metadata": {},
   "source": [
    "## 7.3) Export the entire run folder (everything)\n",
    "\n",
    "This creates a ZIP archive of the whole `OUT_DIR` folder (training logs, models, offline bundle, inference outputs, etc.)\n",
    "and also drops a `config.txt` snapshot inside the run folder before zipping.\n",
    "\n",
    "Use this when you want to share or archive the entire run in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6725b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT FULL RUN DIR (OUT_DIR) — LOCAL\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "out_dir = Path(OUT_DIR)\n",
    "if not out_dir.exists():\n",
    "    raise FileNotFoundError(f\"OUT_DIR not found: {out_dir}\")\n",
    "\n",
    "# Ensure config.txt exists inside OUT_DIR before zipping\n",
    "_ = write_config_txt(out_dir)\n",
    "\n",
    "zip_base = str(out_dir)  # shutil.make_archive wants a string base path (without .zip)\n",
    "zip_path = zip_base + \".zip\"\n",
    "\n",
    "print(\"Zipping:\", out_dir, \"->\", zip_path)\n",
    "shutil.make_archive(zip_base, \"zip\", root_dir=str(out_dir))\n",
    "print(\"Created:\", zip_path)\n",
    "print(\"Note: ZIP includes config.txt at the root of OUT_DIR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fbd9d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa7c47",
   "metadata": {},
   "source": [
    "## 8) Quick sanity checks and pointers\n",
    "\n",
    "These cells are lightweight helpers that run **after** the pipeline.\n",
    "\n",
    "They do not recompute anything:\n",
    "- they check that key artifacts exist\n",
    "- they print the most important paths (model, offline bundle, predictions)\n",
    "- they optionally preview a few rows of the output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# POST-RUN SUMMARY (paths + existence)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _exists(p: str | Path) -> bool:\n",
    "    return Path(p).exists()\n",
    "\n",
    "print(\"\\n=== RUN SUMMARY ===\")\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "print(\"\\n--- Training ---\")\n",
    "print(\"OUT_DATASET_CSV:\", OUT_DATASET_CSV, \"| exists =\", _exists(OUT_DATASET_CSV))\n",
    "print(\"TRAIN_SPLIT_CSV:\", TRAIN_SPLIT_CSV, \"| exists =\", _exists(TRAIN_SPLIT_CSV))\n",
    "print(\"VAL_SPLIT_CSV  :\", VAL_SPLIT_CSV,   \"| exists =\", _exists(VAL_SPLIT_CSV))\n",
    "print(\"TEST_SPLIT_CSV :\", TEST_SPLIT_CSV,  \"| exists =\", _exists(TEST_SPLIT_CSV))\n",
    "print(\"CROSS_ENCODER_MODEL_ID:\", CROSS_ENCODER_MODEL_ID, \"| exists =\", _exists(CROSS_ENCODER_MODEL_ID))\n",
    "\n",
    "print(\"\\n--- Offline ---\")\n",
    "print(\"OFFLINE_BUNDLE_PKL   :\", OFFLINE_BUNDLE_PKL, \"| exists =\", _exists(OFFLINE_BUNDLE_PKL))\n",
    "print(\"ONTOLOGY_INTERNAL_CSV:\", ONTOLOGY_INTERNAL_CSV, \"| exists =\", _exists(ONTOLOGY_INTERNAL_CSV))\n",
    "\n",
    "print(\"\\n--- Inference ---\")\n",
    "print(\"INFER_INPUT_CSV:\", INFER_INPUT_CSV, \"| exists =\", _exists(INFER_INPUT_CSV))\n",
    "print(\"INFER_OUT_CSV  :\", INFER_OUT_CSV,   \"| exists =\", _exists(INFER_OUT_CSV))\n",
    "\n",
    "print(\"\\n--- Exports ---\")\n",
    "exports_dir = Path(OUT_DIR) / \"exports\"\n",
    "print(\"Exports dir:\", exports_dir, \"| exists =\", exports_dir.exists())\n",
    "if exports_dir.exists():\n",
    "    zips = sorted([p.name for p in exports_dir.glob(\"*.zip\")])\n",
    "    print(\"ZIPs:\", zips if zips else \"(none yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a64d44",
   "metadata": {},
   "source": [
    "### 8.1) Preview `predictions.csv` (optional)\n",
    "\n",
    "This is a convenience cell to quickly inspect a few rows of the inference output locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREVIEW PREDICTIONS (optional)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd = None\n",
    "\n",
    "pred_path = Path(INFER_OUT_CSV)\n",
    "\n",
    "if not pred_path.exists():\n",
    "    print(\"Predictions not found:\", pred_path)\n",
    "elif pd is None:\n",
    "    print(\"pandas is not installed. Install with: pip install pandas\")\n",
    "else:\n",
    "    df = pd.read_csv(pred_path)\n",
    "    print(\"Predictions shape:\", df.shape)\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15531d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011492e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae922a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f14b8",
   "metadata": {},
   "source": [
    "## A) Stage-by-stage execution (local)\n",
    "\n",
    "This section runs the pipeline **stage by stage**, controlled by the flags in the\n",
    "**RUN MODE FLAGS** cell.\n",
    "\n",
    "The execution logic is:\n",
    "\n",
    "- **Training stage**\n",
    "  - Runs only if `DO_TRAINING=True`\n",
    "  - Produces:\n",
    "    - dataset CSVs and splits\n",
    "    - a trained cross-encoder model\n",
    "  - If training runs, it sets `CROSS_ENCODER_MODEL_ID` automatically\n",
    "\n",
    "- **Offline bundle stage**\n",
    "  - Runs only if `DO_OFFLINE=True`\n",
    "  - Produces:\n",
    "    - `ontology_internal.csv`\n",
    "    - `offline_bundle.pkl`\n",
    "\n",
    "- **Inference stage**\n",
    "  - Runs only if `DO_INFERENCE=True`\n",
    "  - Requires:\n",
    "    - a valid `CROSS_ENCODER_MODEL_ID`\n",
    "      (from training or restore)\n",
    "    - valid offline artifacts\n",
    "      (from offline stage, restore, or existing paths)\n",
    "\n",
    "Restore cells (if enabled) simply **override paths** used by these stages.\n",
    "They do not execute any computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36db27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STAGE 1 — TRAINING (+ dataset construction)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if not DO_TRAINING:\n",
    "    print(\"Skipping training stage (DO_TRAINING=False).\")\n",
    "else:\n",
    "    train_log = TRAIN_DIR / \"training.log\"\n",
    "    train_cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Dataset construction\n",
    "    # -----------------------------\n",
    "    if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "        train_cmd += [\n",
    "            \"--src\", SRC_PATH,\n",
    "            \"--tgt\", TGT_PATH,\n",
    "            \"--align\", ALIGN_PATH,\n",
    "            \"--out-src\", OUT_SRC_CSV,\n",
    "            \"--out-tgt\", OUT_TGT_CSV,\n",
    "            \"--out-dataset\", OUT_DATASET_CSV,\n",
    "            \"--split-ratios\", SPLIT_RATIOS,\n",
    "        ]\n",
    "\n",
    "        if SRC_PREFIX:\n",
    "            train_cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "        if TGT_PREFIX:\n",
    "            train_cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "        if USE_DESCRIPTION: train_cmd.append(\"--src-use-description\")\n",
    "        if USE_SYNONYMS: train_cmd.append(\"--src-use-synonyms\")\n",
    "        if USE_PARENTS: train_cmd.append(\"--src-use-parents\")\n",
    "        if USE_EQUIVALENT: train_cmd.append(\"--src-use-equivalent\")\n",
    "        if USE_DISJOINT: train_cmd.append(\"--src-use-disjoint\")\n",
    "        if VISUALIZE: train_cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Model training\n",
    "    # -----------------------------\n",
    "    if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "        Path(MODEL_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        train_cmd += [\n",
    "            \"--model-type\", MODEL_TYPE,\n",
    "            \"--model-name\", MODEL_NAME,\n",
    "            \"--model-output-dir\", MODEL_OUT_DIR,\n",
    "            \"--num-epochs\", str(NUM_EPOCHS),\n",
    "        ]\n",
    "\n",
    "        if HYPERPARAMETER_TUNING:\n",
    "            train_cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "        elif USE_FIXED_HYPERPARAMS:\n",
    "            train_cmd += [\n",
    "                \"--learning-rate\", str(LEARNING_RATE),\n",
    "                \"--batch-size\", str(BATCH_SIZE),\n",
    "                \"--weight-decay\", str(WEIGHT_DECAY),\n",
    "            ]\n",
    "\n",
    "    if RUN_MODE == \"train-only\":\n",
    "        train_cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "    run_cmd(train_cmd, train_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nTraining stage completed.\")\n",
    "    print(\"Dataset CSV:\", OUT_DATASET_CSV)\n",
    "    print(\"Cross-encoder output dir:\", FINAL_CROSS_ENCODER_DIR)\n",
    "\n",
    "    # In a full run, the freshly trained model becomes the default for inference\n",
    "    CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STAGE 2 — OFFLINE BUNDLE\n",
    "# ============================================\n",
    "\n",
    "if not DO_OFFLINE:\n",
    "    print(\"Skipping offline bundle stage (DO_OFFLINE=False).\")\n",
    "else:\n",
    "    offline_log = OFFLINE_DIR / \"offline_bundle.log\"\n",
    "\n",
    "    offline_cmd = [\n",
    "        \"python\", \"build_ontology_bundle.py\",\n",
    "        \"--out-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "        \"--out-bundle\", OFFLINE_BUNDLE_PKL,\n",
    "        \"--tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "        \"--bi-encoder-model-id\", BI_ENCODER_MODEL_ID,\n",
    "        \"--semantic-batch-size\", str(OFFLINE_SEMANTIC_BATCH_SIZE),\n",
    "        \"--semantic-max-length\", str(OFFLINE_SEMANTIC_MAX_LENGTH),\n",
    "    ]\n",
    "\n",
    "    if OFFLINE_NO_SEMANTIC_NORMALIZE:\n",
    "        offline_cmd.append(\"--no-semantic-normalize\")\n",
    "\n",
    "    if OFFLINE_EXPORT_CSV:\n",
    "        offline_cmd += [\"--export-csv\", OFFLINE_EXPORT_CSV]\n",
    "    else:\n",
    "        offline_cmd += [\"--ont-path\", OFFLINE_ONT_PATH]\n",
    "        if OFFLINE_PREFIX:\n",
    "            offline_cmd += [\"--prefix\", OFFLINE_PREFIX]\n",
    "\n",
    "    run_cmd(offline_cmd, offline_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nOffline bundle stage completed.\")\n",
    "    print(\"Ontology internal CSV:\", ONTOLOGY_INTERNAL_CSV)\n",
    "    print(\"Offline bundle PKL:\", OFFLINE_BUNDLE_PKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2719915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STAGE 3 — INFERENCE\n",
    "# ============================================\n",
    "\n",
    "if not DO_INFERENCE:\n",
    "    print(\"Skipping inference stage (DO_INFERENCE=False).\")\n",
    "else:\n",
    "    # -----------------------------\n",
    "    # Runtime sanity checks\n",
    "    # -----------------------------\n",
    "    if \"CROSS_ENCODER_MODEL_ID\" not in globals() or CROSS_ENCODER_MODEL_ID is None:\n",
    "        raise ValueError(\n",
    "            \"CROSS_ENCODER_MODEL_ID is not set. \"\n",
    "            \"Run training or restore a model before inference.\"\n",
    "        )\n",
    "\n",
    "    if not Path(OFFLINE_BUNDLE_PKL).exists():\n",
    "        raise FileNotFoundError(f\"OFFLINE_BUNDLE_PKL not found: {OFFLINE_BUNDLE_PKL}\")\n",
    "\n",
    "    if not Path(ONTOLOGY_INTERNAL_CSV).exists():\n",
    "        raise FileNotFoundError(f\"ONTOLOGY_INTERNAL_CSV not found: {ONTOLOGY_INTERNAL_CSV}\")\n",
    "\n",
    "    if not Path(INFER_INPUT_CSV).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"INFER_INPUT_CSV not found: {INFER_INPUT_CSV}\\n\"\n",
    "            \"Provide a custom input CSV or run dataset construction first.\"\n",
    "        )\n",
    "\n",
    "    infer_log = INFER_DIR / \"inference.log\"\n",
    "\n",
    "    infer_cmd = [\n",
    "        \"python\", \"run_inference.py\",\n",
    "        \"--bundle\", OFFLINE_BUNDLE_PKL,\n",
    "        \"--ontology-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "        \"--input-csv\", INFER_INPUT_CSV,\n",
    "        \"--out-csv\", INFER_OUT_CSV,\n",
    "        \"--mode\", INFER_MODE,\n",
    "        \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "        \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "        \"--retrieval-col\", RETRIEVAL_COL,\n",
    "        \"--retrieval-lexical-top-k\", str(RETRIEVAL_LEXICAL_TOP_K),\n",
    "        \"--retrieval-semantic-top-k\", str(RETRIEVAL_SEMANTIC_TOP_K),\n",
    "        \"--retrieval-merged-top-k\", str(RETRIEVAL_MERGED_TOP_K),\n",
    "        \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEMANTIC),\n",
    "        \"--semantic-batch-size\", str(SEMANTIC_BATCH_SIZE),\n",
    "        \"--cross-top-k\", str(CROSS_TOP_K),\n",
    "        \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "        \"--cross-max-length\", str(CROSS_MAX_LENGTH),\n",
    "        \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "    ]\n",
    "\n",
    "    if SCORING_COL:\n",
    "        infer_cmd += [\"--scoring-col\", SCORING_COL]\n",
    "    if ID_COL:\n",
    "        infer_cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "    run_cmd(infer_cmd, infer_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nInference stage completed.\")\n",
    "    print(\"Predictions CSV:\", INFER_OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff53c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a76759",
   "metadata": {},
   "source": [
    "### B) Export inference outputs (predictions + optional gold)\n",
    "\n",
    "This export creates one ZIP containing:\n",
    "- `predictions.csv`\n",
    "- (optional) gold CSV if available\n",
    "- `config.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT ARTIFACTS (predictions + optional gold)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional: external gold file (if you have one)\n",
    "GOLD_PATH_OVERRIDE = None  # e.g. \"my_eval/gold_truth.csv\"\n",
    "\n",
    "pred_path = Path(INFER_OUT_CSV)\n",
    "if not pred_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"predictions file not found:\\n - {pred_path}\\n\\n\"\n",
    "        \"Run inference first or set INFER_OUT_CSV correctly.\"\n",
    "    )\n",
    "\n",
    "gold_candidates = []\n",
    "if GOLD_PATH_OVERRIDE is not None:\n",
    "    gold_candidates.append(Path(GOLD_PATH_OVERRIDE))\n",
    "\n",
    "# Default gold path from dataset build/split (full pipeline)\n",
    "gold_candidates.append(Path(str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\"))))\n",
    "\n",
    "gold_path = next((p for p in gold_candidates if p.exists()), None)\n",
    "\n",
    "files_to_zip = [pred_path]\n",
    "zip_name = \"predictions_only.zip\"\n",
    "\n",
    "if gold_path is not None:\n",
    "    files_to_zip.append(gold_path)\n",
    "    zip_name = \"predictions_and_gold.zip\"\n",
    "    print(\"Found gold:\", gold_path)\n",
    "else:\n",
    "    print(\"Gold not found (OK). Tried:\")\n",
    "    for p in gold_candidates:\n",
    "        print(\" -\", p)\n",
    "\n",
    "zip_path = pred_path.parent / zip_name\n",
    "make_zip_with_config(zip_path, files_to_zip, config_dir=pred_path.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a26a1",
   "metadata": {},
   "source": [
    "### B.A) Export the entire run folder (`OUT_DIR`)\n",
    "\n",
    "This creates a ZIP archive of the whole `OUT_DIR` folder:\n",
    "- training logs + model artifacts\n",
    "- offline bundle artifacts\n",
    "- inference outputs\n",
    "\n",
    "A `config.txt` snapshot is written **inside `OUT_DIR`** before zipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7552ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPORT FULL RUN DIR (OUT_DIR) — LOCAL\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "out_dir = Path(OUT_DIR)\n",
    "if not out_dir.exists():\n",
    "    raise FileNotFoundError(f\"OUT_DIR not found: {out_dir}\")\n",
    "\n",
    "# Ensure config.txt exists inside OUT_DIR before zipping\n",
    "_ = write_config_txt(out_dir)\n",
    "\n",
    "zip_base = str(out_dir)  # shutil.make_archive wants base path (without .zip)\n",
    "zip_path = zip_base + \".zip\"\n",
    "\n",
    "print(\"Zipping:\", out_dir, \"->\", zip_path)\n",
    "shutil.make_archive(zip_base, \"zip\", root_dir=str(out_dir))\n",
    "print(\"Created:\", zip_path)\n",
    "print(\"Note: ZIP includes config.txt at the root of OUT_DIR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe4bb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e287973",
   "metadata": {},
   "source": [
    "## C) Quick sanity checks and pointers (post-run)\n",
    "\n",
    "These cells are lightweight helpers that run **after** the staged pipeline.\n",
    "\n",
    "They do not recompute anything:\n",
    "- they check which key artifacts exist **given the current effective paths**\n",
    "- they print the most important locations (model, offline bundle, predictions)\n",
    "- they list any created ZIP exports under `OUT_DIR/exports`\n",
    "- they optionally preview a few rows of `predictions.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e260d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# POST-RUN SUMMARY (stage-aware: paths + existence)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _exists(p: str | Path | None) -> bool:\n",
    "    if p is None:\n",
    "        return False\n",
    "    try:\n",
    "        return Path(p).exists()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"\\n=== RUN SUMMARY (stage-aware) ===\")\n",
    "print(\"RUN_ID :\", globals().get(\"RUN_ID\", None))\n",
    "print(\"OUT_DIR:\", globals().get(\"OUT_DIR\", None))\n",
    "\n",
    "print(\"\\nFlags:\")\n",
    "print(\"  DO_TRAINING   =\", globals().get(\"DO_TRAINING\", None))\n",
    "print(\"  DO_OFFLINE    =\", globals().get(\"DO_OFFLINE\", None))\n",
    "print(\"  DO_INFERENCE  =\", globals().get(\"DO_INFERENCE\", None))\n",
    "print(\"  RESTORE_MODEL =\", globals().get(\"RESTORE_MODEL\", None))\n",
    "print(\"  RESTORE_OFFLINE =\", globals().get(\"RESTORE_OFFLINE\", None))\n",
    "\n",
    "# -----------------------------\n",
    "# Training artifacts (optional)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Training artifacts (may be absent in stage runs) ---\")\n",
    "print(\"OUT_DATASET_CSV:\", globals().get(\"OUT_DATASET_CSV\", None), \"| exists =\", _exists(globals().get(\"OUT_DATASET_CSV\", None)))\n",
    "print(\"TRAIN_SPLIT_CSV:\", globals().get(\"TRAIN_SPLIT_CSV\", None), \"| exists =\", _exists(globals().get(\"TRAIN_SPLIT_CSV\", None)))\n",
    "print(\"VAL_SPLIT_CSV  :\", globals().get(\"VAL_SPLIT_CSV\", None),   \"| exists =\", _exists(globals().get(\"VAL_SPLIT_CSV\", None)))\n",
    "print(\"TEST_SPLIT_CSV :\", globals().get(\"TEST_SPLIT_CSV\", None),  \"| exists =\", _exists(globals().get(\"TEST_SPLIT_CSV\", None)))\n",
    "\n",
    "# Gold/queries are only guaranteed in full/build-dataset mode\n",
    "gold_path = None\n",
    "queries_path = None\n",
    "if \"OUT_DATASET_CSV\" in globals():\n",
    "    gold_path = str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\"))\n",
    "    queries_path = str(Path(OUT_DATASET_CSV).with_suffix(\".test.queries.csv\"))\n",
    "\n",
    "print(\"TEST gold CSV  :\", gold_path,   \"| exists =\", _exists(gold_path))\n",
    "print(\"TEST queries CSV:\", queries_path, \"| exists =\", _exists(queries_path))\n",
    "\n",
    "# Model\n",
    "print(\"CROSS_ENCODER_MODEL_ID:\", globals().get(\"CROSS_ENCODER_MODEL_ID\", None),\n",
    "      \"| exists =\", _exists(globals().get(\"CROSS_ENCODER_MODEL_ID\", None)))\n",
    "\n",
    "# -----------------------------\n",
    "# Offline artifacts (required for inference, but may be restored)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Offline artifacts ---\")\n",
    "print(\"OFFLINE_BUNDLE_PKL   :\", globals().get(\"OFFLINE_BUNDLE_PKL\", None),\n",
    "      \"| exists =\", _exists(globals().get(\"OFFLINE_BUNDLE_PKL\", None)))\n",
    "print(\"ONTOLOGY_INTERNAL_CSV:\", globals().get(\"ONTOLOGY_INTERNAL_CSV\", None),\n",
    "      \"| exists =\", _exists(globals().get(\"ONTOLOGY_INTERNAL_CSV\", None)))\n",
    "\n",
    "# -----------------------------\n",
    "# Inference artifacts (optional)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Inference artifacts ---\")\n",
    "print(\"INFER_INPUT_CSV:\", globals().get(\"INFER_INPUT_CSV\", None),\n",
    "      \"| exists =\", _exists(globals().get(\"INFER_INPUT_CSV\", None)))\n",
    "print(\"INFER_OUT_CSV  :\", globals().get(\"INFER_OUT_CSV\", None),\n",
    "      \"| exists =\", _exists(globals().get(\"INFER_OUT_CSV\", None)))\n",
    "\n",
    "# -----------------------------\n",
    "# Export ZIPs (local design)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Exports ---\")\n",
    "exports_dir = Path(globals().get(\"OUT_DIR\", \".\")) / \"exports\"\n",
    "print(\"Exports dir:\", exports_dir, \"| exists =\", exports_dir.exists())\n",
    "if exports_dir.exists():\n",
    "    zips = sorted([p.name for p in exports_dir.glob(\"*.zip\")])\n",
    "    print(\"ZIPs:\", zips if zips else \"(none yet)\")\n",
    "\n",
    "print(\"\\nTip:\")\n",
    "print(\" - If something is missing, check the stage logs under:\")\n",
    "print(\"   \", Path(globals().get(\"OUT_DIR\", \".\")) / \"training\" / \"training.log\")\n",
    "print(\"   \", Path(globals().get(\"OUT_DIR\", \".\")) / \"offline\" / \"offline_bundle.log\")\n",
    "print(\"   \", Path(globals().get(\"OUT_DIR\", \".\")) / \"inference\" / \"inference.log\")\n",
    "print(\"=== END SUMMARY ===\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1b690",
   "metadata": {},
   "source": [
    "### C.A) Preview `predictions.csv` (optional)\n",
    "\n",
    "This is a convenience cell to quickly inspect a few rows of the inference output locally.\n",
    "It is safe to run even if inference was skipped: it will just print a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13708f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREVIEW PREDICTIONS (optional) — stage-aware\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    pd = None\n",
    "\n",
    "pred_path = Path(globals().get(\"INFER_OUT_CSV\", \"predictions.csv\"))\n",
    "\n",
    "if not pred_path.exists():\n",
    "    print(\"Predictions not found:\", pred_path)\n",
    "elif pd is None:\n",
    "    print(\"pandas is not installed. Install with: pip install pandas\")\n",
    "else:\n",
    "    df = pd.read_csv(pred_path)\n",
    "    print(\"Predictions path :\", pred_path)\n",
    "    print(\"Predictions shape:\", df.shape)\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c7f16",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12672f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d65832",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a6f23",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This section evaluates the inference output (`predictions.csv`) against the ground-truth gold file\n",
    "produced by the dataset builder (`training_dataset.test.gold.csv`) using the **external evaluation script**.\n",
    "\n",
    "## Setup and path resolution\n",
    "\n",
    "This cell configures and resolves all inputs required to run the evaluation script.\n",
    "\n",
    "In particular, it:\n",
    "- sets the evaluation hyperparameter `K`\n",
    "- resolves the paths to:\n",
    "  - `predictions.csv`\n",
    "  - the gold test split (`*.test.gold.csv`)\n",
    "  - the evaluation script\n",
    "- optionally allows overriding prediction and gold paths for custom evaluation\n",
    "- optionally detects and passes a `config.txt` file **as input metadata** (it is never generated here)\n",
    "- configures the export of a **metrics CSV** (the only persistent evaluation artifact)\n",
    "\n",
    "Important notes:\n",
    "- evaluation output is **printed directly to the notebook**\n",
    "- metrics are saved as a CSV for aggregation and comparison across runs\n",
    "- no `evaluation.log` is produced anymore\n",
    "- saving a merged prediction–gold CSV is optional and intended only for debugging\n",
    "\n",
    "All path logic is kept explicit and separate so that the evaluation script itself\n",
    "remains independent from notebook-specific filesystem concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f184095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation setup OK.\n",
      "  pred_path: runs_toEvaluate/predictions_and_gold/predictions.csv\n",
      "  gold_path: runs_toEvaluate/predictions_and_gold/training_dataset.test.gold.csv\n",
      "  eval_log : runs_toEvaluate/predictions_and_gold/evaluation.log\n",
      "  merged   : runs_toEvaluate/predictions_and_gold/merged_eval.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# EVALUATION CONFIG (notebook-side)\n",
    "# ============================================\n",
    "K = int(CROSS_TOP_K) if \"CROSS_TOP_K\" in globals() else 10\n",
    "\n",
    "# If you want to evaluate files not produced by this run, set these:\n",
    "PRED_PATH_OVERRIDE = None  # e.g. \"my_runs/run_20260109/predictions.csv\"\n",
    "GOLD_PATH_OVERRIDE = None  # e.g. \"my_gold/test.gold.csv\"\n",
    "\n",
    "# Optional outputs\n",
    "SAVE_MERGED = False  # if True -> pass --out-merged\n",
    "OUT_MERGED_OVERRIDE = None  # if None -> defaults next to predictions as merged_eval.csv\n",
    "\n",
    "# Metrics CSV export (script feature)\n",
    "SAVE_METRICS = True  # if True -> pass --out-metrics\n",
    "OUT_METRICS_OVERRIDE = None  # if None -> defaults next to predictions as metrics_eval.csv\n",
    "APPEND_METRICS = True  # if True -> pass --append-metrics\n",
    "\n",
    "# Optional config.txt input for metadata (script feature)\n",
    "# IMPORTANT: we DO NOT generate config.txt here. We only optionally consume one.\n",
    "CONFIG_TXT_OVERRIDE = None  # e.g. \"outputs/.../config.txt\" (optional)\n",
    "\n",
    "# Optional join/key overrides (rare)\n",
    "GT_ID_COL = None\n",
    "PRED_ID_COL = None\n",
    "\n",
    "# Column name overrides (defaults match your script)\n",
    "GT_GOLD_COL = \"gold_target_iris\"\n",
    "GT_MATCH_COL = \"match\"\n",
    "GT_TEXT_COL = \"source_text\"\n",
    "PRED_TEXT_COL = \"attribute_text\"\n",
    "\n",
    "# ============================================\n",
    "# Resolve paths\n",
    "# ============================================\n",
    "# predictions\n",
    "if PRED_PATH_OVERRIDE is not None:\n",
    "    pred_path = Path(PRED_PATH_OVERRIDE).expanduser().resolve()\n",
    "else:\n",
    "    if \"INFER_OUT_CSV\" not in globals():\n",
    "        raise ValueError(\"INFER_OUT_CSV is not set. Run inference or set PRED_PATH_OVERRIDE.\")\n",
    "    pred_path = Path(INFER_OUT_CSV).expanduser().resolve()\n",
    "\n",
    "if not pred_path.exists():\n",
    "    raise FileNotFoundError(f\"Predictions file not found: {pred_path}\")\n",
    "\n",
    "# gold\n",
    "if GOLD_PATH_OVERRIDE is not None:\n",
    "    gold_path = Path(GOLD_PATH_OVERRIDE).expanduser().resolve()\n",
    "else:\n",
    "    if \"OUT_DATASET_CSV\" not in globals():\n",
    "        raise ValueError(\"OUT_DATASET_CSV is not set. Build dataset or set GOLD_PATH_OVERRIDE.\")\n",
    "    gold_path = Path(str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\"))).expanduser().resolve()\n",
    "\n",
    "if not gold_path.exists():\n",
    "    raise FileNotFoundError(f\"Gold file not found: {gold_path}\")\n",
    "\n",
    "# script path\n",
    "EVAL_SCRIPT = Path(\"testing/new_evaluate_inference.py\").resolve()\n",
    "if not EVAL_SCRIPT.exists():\n",
    "    raise FileNotFoundError(f\"Evaluation script not found: {EVAL_SCRIPT}\")\n",
    "\n",
    "# outputs (all default next to predictions)\n",
    "EVAL_OUT_DIR = pred_path.parent\n",
    "EVAL_LOG = EVAL_OUT_DIR / \"evaluation.log\"\n",
    "\n",
    "if OUT_MERGED_OVERRIDE is None:\n",
    "    merged_out_path = EVAL_OUT_DIR / \"merged_eval.csv\"\n",
    "else:\n",
    "    merged_out_path = Path(OUT_MERGED_OVERRIDE).expanduser().resolve()\n",
    "\n",
    "if OUT_METRICS_OVERRIDE is None:\n",
    "    metrics_out_path = EVAL_OUT_DIR / \"metrics_eval.csv\"\n",
    "else:\n",
    "    metrics_out_path = Path(OUT_METRICS_OVERRIDE).expanduser().resolve()\n",
    "\n",
    "# config.txt (optional input)\n",
    "config_txt_path = None\n",
    "if CONFIG_TXT_OVERRIDE is not None:\n",
    "    config_txt_path = Path(CONFIG_TXT_OVERRIDE).expanduser().resolve()\n",
    "    if not config_txt_path.exists():\n",
    "        raise FileNotFoundError(f\"CONFIG_TXT_OVERRIDE not found: {config_txt_path}\")\n",
    "if config_txt_path is None:\n",
    "    candidate = pred_path.parent / \"config.txt\"\n",
    "    if candidate.exists():\n",
    "        config_txt_path = candidate\n",
    "\n",
    "print(\"Evaluation setup OK.\")\n",
    "print(\"  pred_path:\", pred_path)\n",
    "print(\"  gold_path:\", gold_path)\n",
    "print(\"  eval_log :\", EVAL_LOG)\n",
    "print(\"  save_merged :\", SAVE_MERGED, \"| merged_out_path:\", merged_out_path)\n",
    "print(\"  save_metrics:\", SAVE_METRICS, \"| metrics_out_path:\", metrics_out_path)\n",
    "print(\"  config_txt:\", config_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862b4ee",
   "metadata": {},
   "source": [
    "### Helper: run_cmd\n",
    "\n",
    "Utility function to execute framework scripts as subprocesses and **stream their stdout/stderr live to the notebook**.\n",
    "\n",
    "Key properties:\n",
    "- output is printed in real time (no buffering)\n",
    "- subprocesses run with Weights & Biases explicitly disabled\n",
    "- execution fails fast if the command exits with a non-zero return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_cmd_tee ready.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def run_cmd(cmd, cwd: Path):\n",
    "    cmd = [str(x) for x in cmd]\n",
    "\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"CWD:\", Path(cwd).resolve())\n",
    "\n",
    "    env = os.environ.copy()\n",
    "    env[\"WANDB_MODE\"] = \"disabled\"\n",
    "    env[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        cwd=str(cwd),\n",
    "        env=env,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "\n",
    "    assert proc.stdout is not None\n",
    "    for line in proc.stdout:\n",
    "        print(line, end=\"\")\n",
    "\n",
    "    proc.wait()\n",
    "\n",
    "    print(\"\\nReturn code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}\")\n",
    "\n",
    "    return proc.returncode\n",
    "\n",
    "print(\"run_cmd ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a60255",
   "metadata": {},
   "source": [
    "## Evaluation execution and metrics export\n",
    "\n",
    "This cell runs the **framework evaluation script** (`new_evaluate_inference.py`) as a subprocess,\n",
    "delegating all evaluation logic to the framework itself.\n",
    "\n",
    "What this step does:\n",
    "- executes the evaluation on `predictions.csv` against the gold test split\n",
    "- applies the framework’s join strategy (explicit IDs → `source_iri` → `row_id` → safe text fallback)\n",
    "- computes coverage and ranking metrics **on positive examples only**:\n",
    "  - Precision@1\n",
    "  - Hits@K\n",
    "  - MRR@K\n",
    "- prints a detailed evaluation report directly in the notebook output\n",
    "- optionally exports:\n",
    "  - a merged predictions–gold CSV (for debugging/inspection)\n",
    "  - a metrics CSV (append-friendly, suitable for cross-run comparison)\n",
    "\n",
    "Notes:\n",
    "- metrics persistence replaces the need for an `evaluation.log`\n",
    "- run metadata can be attached to the metrics CSV via an optional `config.txt`\n",
    "- all output is streamed live to the notebook for transparency and debuggability\n",
    "\n",
    "The notebook acts purely as a launcher: evaluation semantics are fully defined and versioned\n",
    "inside the framework script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e9538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running command:\n",
      " /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/OAvenv/bin/python testing/new_evaluate_inference.py --test-split runs_toEvaluate/predictions_and_gold/training_dataset.test.gold.csv --predictions runs_toEvaluate/predictions_and_gold/predictions.csv --k 20 --gt-gold-col gold_target_iris --gt-match-col None --gt-text-col None --pred-text-col attribute_text --gt-id-col source_iri --pred-id-col source_iri --out-merged runs_toEvaluate/predictions_and_gold/merged_eval.csv\n",
      "CWD: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject\n",
      "Log: /Users/usermastro/Desktop/Primo_Semestre_2526/ADSP/Ontology Alignment Project/OAProject/runs_toEvaluate/predictions_and_gold/evaluation.log\n",
      "\n",
      "=== Evaluation Report ===\n",
      "Join method: id_col | pred[source_iri] == gt[source_iri]\n",
      "Pred rows: 67\n",
      "Coverage (predicted_iri != null): 1.0000\n",
      "GT attach rate (gold present after join): 1.0000\n",
      "\n",
      "Retrieval source distribution:\n",
      "     exact: 0.6567\n",
      "    hybrid: 0.3433\n",
      "\n",
      "Metrics on POSITIVES only (match==1):\n",
      "  n_pos:             67\n",
      "  Precision@1 (pos): 0.6418\n",
      "  Hits@20 (pos):     0.8060\n",
      "  MRR@20 (pos):      0.6888\n",
      "\n",
      "=== Breakdown by Retrieval Source ===\n",
      "\n",
      "Source: exact\n",
      "  n_pos:             44\n",
      "  Precision@1 (pos): 0.7273\n",
      "  Hits@20 (pos):     0.7273\n",
      "\n",
      "Source: hybrid\n",
      "  n_pos:             23\n",
      "  Precision@1 (pos): 0.4783\n",
      "  Hits@20 (pos):     0.9565\n",
      "\n",
      "Saved merged CSV to: runs_toEvaluate/predictions_and_gold/merged_eval.csv\n",
      "\n",
      "[DONE]\n",
      "\n",
      "Return code: 0\n",
      "\n",
      "[DONE] Evaluation finished.\n",
      "Saved log: runs_toEvaluate/predictions_and_gold/evaluation.log\n",
      "Saved merged CSV: runs_toEvaluate/predictions_and_gold/merged_eval.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "\n",
    "eval_cmd = [\n",
    "    sys.executable, str(EVAL_SCRIPT),\n",
    "    \"--test-split\", str(gold_path),\n",
    "    \"--predictions\", str(pred_path),\n",
    "    \"--k\", str(int(K)),\n",
    "    \"--gt-gold-col\", str(GT_GOLD_COL),\n",
    "    \"--gt-match-col\", str(GT_MATCH_COL),\n",
    "    \"--gt-text-col\", str(GT_TEXT_COL),\n",
    "    \"--pred-text-col\", str(PRED_TEXT_COL),\n",
    "]\n",
    "\n",
    "if GT_ID_COL is not None:\n",
    "    eval_cmd += [\"--gt-id-col\", str(GT_ID_COL)]\n",
    "if PRED_ID_COL is not None:\n",
    "    eval_cmd += [\"--pred-id-col\", str(PRED_ID_COL)]\n",
    "\n",
    "if SAVE_MERGED:\n",
    "    eval_cmd += [\"--out-merged\", str(merged_out_path)]\n",
    "\n",
    "if SAVE_METRICS:\n",
    "    eval_cmd += [\"--out-metrics\", str(metrics_out_path)]\n",
    "    if APPEND_METRICS:\n",
    "        eval_cmd += [\"--append-metrics\"]\n",
    "    if config_txt_path is not None:\n",
    "        eval_cmd += [\"--config\", str(config_txt_path)]\n",
    "\n",
    "run_cmd(eval_cmd, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nEvaluation completed.\")\n",
    "print(\"Metrics CSV:\", metrics_out_path.resolve() if SAVE_METRICS else \"(not saved)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
