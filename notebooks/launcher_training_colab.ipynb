{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_md",
   "metadata": {},
   "source": [
    "# Training Launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purpose_md",
   "metadata": {},
   "source": [
    "## Purpose of this notebook\n",
    "This notebook is a script-first, notebook-as-launcher interface for running the **End-to-End Training Pipeline** on Colab.\n",
    "It will:\n",
    "- clone the repository,\n",
    "- install dependencies,\n",
    "- load the required raw ontology files (Source OWL, Target OWL, Reference Alignment RDF),\n",
    "- run the pipeline via `training.py`,\n",
    "- package the processed datasets and trained model into a zip file for download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outputs_md",
   "metadata": {},
   "source": [
    "## What is produced by this notebook\n",
    "\n",
    "Running this notebook executes the `training.py` script, which performs two main stages:\n",
    "\n",
    "1. **Dataset Generation**:\n",
    "   - Parses raw `.owl` files into textual representations (CSVs).\n",
    "   - Merges them with the reference alignment to create positive/negative training pairs.\n",
    "2. **Model Training** (Bi-Encoder or Cross-Encoder):\n",
    "   - Fine-tunes a BERT-based model on the generated dataset.\n",
    "\n",
    "**Outputs:**\n",
    "- A **processed dataset CSV** used for training.\n",
    "- **Source/Target CSVs** containing the parsed textual features.\n",
    "- A **Trained Model** directory (saved in HuggingFace format).\n",
    "- An **Alignment Visualization** (optional).\n",
    "\n",
    "All outputs are saved in a unique timestamped directory under `outputs/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sep_1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clone_md",
   "metadata": {},
   "source": [
    "### Clone repository\n",
    "\n",
    "This cell clones the project repository into the Colab VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone_code",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "REPO_URL=\"https://github.com/adsp-polito/2025-P13-Ontology-Alignment.git\"\n",
    "REPO_DIR=\"2025-P13-Ontology-Alignment\"\n",
    "\n",
    "# Optional: lock to a specific commit for reproducibility\n",
    "COMMIT=\"\"\n",
    "\n",
    "rm -rf \"$REPO_DIR\"\n",
    "git clone \"$REPO_URL\" \"$REPO_DIR\"\n",
    "cd \"$REPO_DIR\"\n",
    "\n",
    "if [ -n \"$COMMIT\" ]; then\n",
    "  git checkout \"$COMMIT\"\n",
    "fi\n",
    "\n",
    "echo \"Checked out commit:\"\n",
    "git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enter_dir_md",
   "metadata": {},
   "source": [
    "### Enter the repo directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enter_dir_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 2025-P13-Ontology-Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install_md",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_code",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "python -m pip install --upgrade pip\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sep_2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gpu_check_md",
   "metadata": {},
   "source": [
    "### Check runtime (CPU/GPU)\n",
    "Training requires a GPU for reasonable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu_check_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sep_3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_md",
   "metadata": {},
   "source": [
    "### Provide required input artifacts\n",
    "`training.py` needs three specific files to build the dataset and train:\n",
    "1. **Source Ontology** (e.g., `sweet.owl`)\n",
    "2. **Target Ontology** (e.g., `envo.owl`)\n",
    "3. **Reference Alignment** (e.g., `envo-sweet.rdf` - OAEI format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upload_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Please upload: Source Ontology (.owl), Target Ontology (.owl), and Alignment (.rdf)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn, content in uploaded.items():\n",
    "    out_path = os.path.join(\"data\", fn)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sep_4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_dir_md",
   "metadata": {},
   "source": [
    "### Configure output directory for this training run\n",
    "Creates a unique output directory using a timestamp-based run ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_dir_code",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "RUN_ID=\"training_run_$(date +%Y%m%d_%H%M%S)\"\n",
    "OUT_DIR=\"outputs/${RUN_ID}\"\n",
    "mkdir -p \"$OUT_DIR\"\n",
    "\n",
    "# Save dir path for Python access\n",
    "echo \"$OUT_DIR\" > outputs/LAST_TRAINING_RUN_DIR.txt\n",
    "\n",
    "echo \"OUT_DIR=$OUT_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_params_md",
   "metadata": {},
   "source": [
    "### Configure Training Parameters\n",
    "\n",
    "Here you define the arguments for `training.py`. \n",
    "**Note:** Ensure `SRC_PATH`, `TGT_PATH`, and `ALIGN_PATH` match the filenames you uploaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_params_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Input Paths (Must match uploaded files)\n",
    "# -----------------------\n",
    "SRC_PATH = \"data/sweet.owl\"   # Update filename if needed\n",
    "TGT_PATH = \"data/envo.owl\"    # Update filename if needed\n",
    "ALIGN_PATH = \"data/envo-sweet.rdf\" # Update filename if needed\n",
    "\n",
    "# -----------------------\n",
    "# Ontology Filters (Prefixes)\n",
    "# -----------------------\n",
    "# Optional: Use if you want to filter classes by IRI prefix\n",
    "SRC_PREFIX = None # \"http://sweetontology.net/\"\n",
    "TGT_PREFIX = None # \"http://purl.obolibrary.org/obo/ENVO_\"\n",
    "\n",
    "# -----------------------\n",
    "# Text Generation Configuration\n",
    "# -----------------------\n",
    "# These flags control what text is added to the training examples\n",
    "USE_DESCRIPTION = False\n",
    "USE_SYNONYMS = False\n",
    "USE_PARENTS = False\n",
    "USE_EQUIVALENT = False\n",
    "USE_DISJOINT = False\n",
    "\n",
    "# -----------------------\n",
    "# Model Configuration\n",
    "# -----------------------\n",
    "# options: \"bi-encoder\" or \"cross-encoder\"\n",
    "MODEL_TYPE = \"bi-encoder\" \n",
    "\n",
    "# HuggingFace Model ID\n",
    "MODEL_NAME = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "# -----------------------\n",
    "# Visualization\n",
    "# -----------------------\n",
    "VISUALIZE = False  # Set to True to generate a graph visualization of alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_md",
   "metadata": {},
   "source": [
    "### Run Training Pipeline\n",
    "\n",
    "This cell constructs the CLI command and executes `training.py`. It handles:\n",
    "1. **Ontology Loading**: Parsing OWL files.\n",
    "2. **Text Generation**: Applying the flags (descriptions, synonyms, etc.).\n",
    "3. **Dataset Building**: Merging with alignments.\n",
    "4. **Training**: Fine-tuning the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases (optional)\n",
    "wandb.login() # Follow prompts to login\n",
    "# os.environ[\"WANDB_MODE\"] = \"disabled\" # Uncomment to disable W&B logging\n",
    "\n",
    "# Read output directory from previous step\n",
    "with open(\"outputs/LAST_TRAINING_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "# Define Output Paths\n",
    "out_src_csv = f\"{out_dir}/source_ontology.csv\"\n",
    "out_tgt_csv = f\"{out_dir}/target_ontology.csv\"\n",
    "out_dataset_csv = f\"{out_dir}/training_dataset.csv\"\n",
    "model_out_dir = f\"{out_dir}/models/{MODEL_TYPE}_custom/\"\n",
    "log_path = f\"{out_dir}/training.log\"\n",
    "\n",
    "# Build Command\n",
    "cmd = [\n",
    "    \"python\", \"training.py\",\n",
    "    \"--src\", SRC_PATH,\n",
    "    \"--tgt\", TGT_PATH,\n",
    "    \"--align\", ALIGN_PATH,\n",
    "    \"--out-src\", out_src_csv,\n",
    "    \"--out-tgt\", out_tgt_csv,\n",
    "    \"--out-dataset\", out_dataset_csv,\n",
    "    \"--model-type\", MODEL_TYPE,\n",
    "    \"--model-name\", MODEL_NAME,\n",
    "    \"--model-output-dir\", model_out_dir\n",
    "]\n",
    "\n",
    "# Add Optional Arguments\n",
    "if SRC_PREFIX:\n",
    "    cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "if TGT_PREFIX:\n",
    "    cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "# Add Boolean Flags\n",
    "if USE_DESCRIPTION: cmd.append(\"--src-use-description\")\n",
    "if USE_SYNONYMS: cmd.append(\"--src-use-synonyms\")\n",
    "if USE_PARENTS: cmd.append(\"--src-use-parents\")\n",
    "if USE_EQUIVALENT: cmd.append(\"--src-use-equivalent\")\n",
    "if USE_DISJOINT: cmd.append(\"--src-use-disjoint\")\n",
    "if VISUALIZE: cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "print(\"Running command:\\n\", \" \".join(cmd))\n",
    "print(f\"\\nLogs are being written to: {log_path} ...\")\n",
    "\n",
    "# Run execution\n",
    "with open(log_path, \"w\") as log_file:\n",
    "    proc = subprocess.run(cmd, stdout=log_file, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Process finished with return code:\", proc.returncode)\n",
    "\n",
    "if proc.returncode != 0:\n",
    "    print(\"!!! Error occurred. Printing last 20 lines of log:\")\n",
    "    os.system(f\"tail -n 20 {log_path}\")\n",
    "else:\n",
    "    print(f\"Training complete. Model saved to: {model_out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_md",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_data_md",
   "metadata": {},
   "source": [
    "### Inspect Generated Training Data\n",
    "Let's look at the dataset that was built and used for training. This confirms that text, synonyms, and descriptions were merged correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_data_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"outputs/LAST_TRAINING_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "dataset_csv = f\"{out_dir}/training_dataset.csv\"\n",
    "if os.path.exists(dataset_csv):\n",
    "    df = pd.read_csv(dataset_csv)\n",
    "    print(f\"Training Dataset Shape: {df.shape}\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Dataset CSV not found. Did the run fail?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sep_5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "package_md",
   "metadata": {},
   "source": [
    "### Package and Download Results\n",
    "This zips the entire output directory (logs, CSVs, and the trained model) for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "package_code",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "OUT_DIR=$(cat outputs/LAST_TRAINING_RUN_DIR.txt)\n",
    "ZIP_PATH=\"${OUT_DIR}.zip\"\n",
    "\n",
    "echo \"Zipping $OUT_DIR ...\"\n",
    "zip -r -q \"$ZIP_PATH\" \"$OUT_DIR\"\n",
    "echo \"Created zip: $ZIP_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "with open(\"outputs/LAST_TRAINING_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "files.download(f\"{out_dir}.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
