{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb8dd6a",
   "metadata": {},
   "source": [
    "# Training Launcher\n",
    "\n",
    "## Purpose of this notebook\n",
    "This notebook is a **script-first, notebook-as-launcher** interface for running the **end-to-end ontology alignment training pipeline** on Colab.\n",
    "\n",
    "It will:\n",
    "- clone the repository,\n",
    "- install dependencies,\n",
    "- load the required input artifacts (OWL/RDF),\n",
    "- run the pipeline via `training.py` in one of three modes,\n",
    "- package outputs (CSVs, logs, models) into a ZIP for download.\n",
    "\n",
    "## What is produced by this notebook\n",
    "Depending on the selected mode, the notebook produces:\n",
    "\n",
    "### Mode = `full` (build dataset + train)\n",
    "- Source ontology CSV (textual features)\n",
    "- Target ontology CSV (textual features)\n",
    "- Training dataset CSV (pairs + label)\n",
    "- Trained model directory (bi-encoder or cross-encoder)\n",
    "- Execution logs\n",
    "\n",
    "### Mode = `build-dataset`\n",
    "- Source ontology CSV\n",
    "- Target ontology CSV\n",
    "- Training dataset CSV\n",
    "- Execution logs\n",
    "\n",
    "### Mode = `train-only` (only train from CSV)\n",
    "- Trained model directory\n",
    "- Execution logs\n",
    "\n",
    "All outputs are saved under a unique timestamped directory in `outputs/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746df2cd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6387d1b3",
   "metadata": {},
   "source": [
    "### Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85ed4f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "REPO_URL=\"https://github.com/adsp-polito/2025-P13-Ontology-Alignment.git\"\n",
    "REPO_DIR=\"2025-P13-Ontology-Alignment\"\n",
    "\n",
    "# Optional: lock to a specific commit for full reproducibility\n",
    "COMMIT=\"\"   # e.g. \"4ffd790\"\n",
    "\n",
    "rm -rf \"$REPO_DIR\"\n",
    "git clone \"$REPO_URL\" \"$REPO_DIR\"\n",
    "cd \"$REPO_DIR\"\n",
    "\n",
    "if [ -n \"$COMMIT\" ]; then\n",
    "  git checkout \"$COMMIT\"\n",
    "fi\n",
    "\n",
    "echo \"Checked out commit:\"\n",
    "git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305227e",
   "metadata": {},
   "source": [
    "### Enter the repository directory\n",
    "\n",
    "Colab runs each `%%bash` cell in its own subshell, so directory changes do not persist across cells.\n",
    "We use `%cd` to move into the cloned repository for all subsequent Python cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 2025-P13-Ontology-Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34910f42",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8461a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "python -m pip install --upgrade pip\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc8260",
   "metadata": {},
   "source": [
    "## Check runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef1ac3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c61334",
   "metadata": {},
   "source": [
    "## Choose run mode\n",
    "\n",
    "Select exactly one mode:\n",
    "\n",
    "- `full`: build dataset + train  \n",
    "- `build-dataset`: only build the dataset CSVs (no training)  \n",
    "- `train-only`: train from an existing dataset CSV (no ontology loading)\n",
    "\n",
    "The notebook will create a unique output directory under `outputs/` for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2567f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one: \"full\", \"build-dataset\", \"train-only\"\n",
    "RUN_MODE = \"full\"\n",
    "\n",
    "# W&B logging (optional)\n",
    "USE_WANDB = False\n",
    "\n",
    "# Number of epochs (used for full/train-only)\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Model choice (used for full/train-only)\n",
    "MODEL_TYPE = \"bi-encoder\"  # \"bi-encoder\" or \"cross-encoder\"\n",
    "MODEL_NAME = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\"\n",
    "\n",
    "# Hyperparameter values (used for full/train-only; ignored if tuning is enabled)\n",
    "USE_FIXED_HYPERPARAMS = False # Set to True to use fixed hyperparameters\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Tuning choice (used for full/train-only)\n",
    "HYPERPARAMETER_TUNING = True\n",
    "N_TRIALS = 20  # Number of trials for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96629ed4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e29580",
   "metadata": {},
   "source": [
    "## Provide required input artifacts\n",
    "\n",
    "### For `full` and `build-dataset`\n",
    "You must provide:\n",
    "1. **Source Ontology** (`.owl`)\n",
    "2. **Target Ontology** (`.owl`)\n",
    "3. **Reference Alignment** (`.rdf`, OAEI-like format)\n",
    "\n",
    "Upload them manually (recommended for Colab).\n",
    "\n",
    "### For `train-only`\n",
    "You must provide:\n",
    "1. **Training dataset CSV** with columns: `source_text`, `target_text`, `match`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "print(\"Upload required files for the selected mode.\")\n",
    "print(\"- full/build-dataset: source.owl, target.owl, alignment.rdf\")\n",
    "print(\"- train-only: training_dataset.csv\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn, content in uploaded.items():\n",
    "    out_path = os.path.join(\"data\", fn)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1035c32",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941f726",
   "metadata": {},
   "source": [
    "## Set input file paths\n",
    "\n",
    "Assign the correct filenames to the variables below.\n",
    "These paths will be passed to `training.py` as CLI arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2dbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- For full/build-dataset ----\n",
    "SRC_PATH = \"data/sweet.owl\"\n",
    "TGT_PATH = \"data/envo.owl\"\n",
    "ALIGN_PATH = \"data/envo-sweet.rdf\"\n",
    "\n",
    "# ---- For train-only ----\n",
    "DATASET_CSV = \"data/training_dataset.csv\"  # must exist if RUN_MODE=\"train-only\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3da89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c46bf3",
   "metadata": {},
   "source": [
    "## Configure dataset-building options (for `full` and `build-dataset`)\n",
    "\n",
    "### Ontology filters (IRI prefixes)\n",
    "Optionally filter classes by IRI prefix. Use `None` to disable filtering.\n",
    "\n",
    "### Source text configuration\n",
    "These flags control which fields are included in the **source** textual representation (SHORT_TEXT):\n",
    "- description\n",
    "- synonyms\n",
    "- parents\n",
    "- equivalent classes\n",
    "- disjoint classes\n",
    "\n",
    "The target text (RICH_TEXT) is built by the repository pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Ontology Filters (Prefixes)\n",
    "# -----------------------\n",
    "SRC_PREFIX = None  # e.g. \"http://sweetontology.net/\"\n",
    "TGT_PREFIX = None  # e.g. \"http://purl.obolibrary.org/obo/ENVO_\"\n",
    "\n",
    "# -----------------------\n",
    "# Source Text Configuration\n",
    "# -----------------------\n",
    "USE_DESCRIPTION = False\n",
    "USE_SYNONYMS = False\n",
    "USE_PARENTS = False\n",
    "USE_EQUIVALENT = False\n",
    "USE_DISJOINT = False\n",
    "\n",
    "# -----------------------\n",
    "# Visualization (optional)\n",
    "# -----------------------\n",
    "VISUALIZE = False  # True -> create alignment visualization (if supported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4d4c0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f007b8",
   "metadata": {},
   "source": [
    "## Configure output directory for this run\n",
    "\n",
    "We create a unique run directory using a timestamp-based run ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "RUN_ID=\"training_run_$(date +%Y%m%d_%H%M%S)\"\n",
    "OUT_DIR=\"outputs/${RUN_ID}\"\n",
    "mkdir -p \"$OUT_DIR\"\n",
    "\n",
    "echo \"$OUT_DIR\" > outputs/LAST_TRAINING_RUN_DIR.txt\n",
    "echo \"OUT_DIR=$OUT_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9edd4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f3872",
   "metadata": {},
   "source": [
    "## Run the pipeline via `training.py`\n",
    "\n",
    "This cell builds a CLI command and runs `training.py` in the selected mode.\n",
    "\n",
    "It writes logs to `training.log` inside the run directory.\n",
    "\n",
    "- In `full` mode: builds dataset + trains a model  \n",
    "- In `build-dataset` mode: builds dataset and exits  \n",
    "- In `train-only` mode: trains from an existing dataset CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Optional W&B behavior\n",
    "if not USE_WANDB:\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "else:\n",
    "    import wandb\n",
    "    wandb.login()\n",
    "\n",
    "# Read output directory\n",
    "with open(\"outputs/LAST_TRAINING_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "# Define output paths (always under OUT_DIR)\n",
    "out_src_csv = f\"{out_dir}/source_ontology.csv\"\n",
    "out_tgt_csv = f\"{out_dir}/target_ontology.csv\"\n",
    "out_dataset_csv = f\"{out_dir}/training_dataset.csv\"\n",
    "model_out_dir = f\"{out_dir}/models/{MODEL_TYPE}_custom/\"\n",
    "log_path = f\"{out_dir}/training.log\"\n",
    "\n",
    "cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "# Mode: full or build-dataset => requires ontologies + alignment + output CSV paths\n",
    "if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "    cmd += [\"--src\", SRC_PATH, \"--tgt\", TGT_PATH, \"--align\", ALIGN_PATH]\n",
    "    cmd += [\"--out-src\", out_src_csv, \"--out-tgt\", out_tgt_csv, \"--out-dataset\", out_dataset_csv]\n",
    "\n",
    "    if SRC_PREFIX:\n",
    "        cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "    if TGT_PREFIX:\n",
    "        cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "    if USE_DESCRIPTION: cmd.append(\"--src-use-description\")\n",
    "    if USE_SYNONYMS: cmd.append(\"--src-use-synonyms\")\n",
    "    if USE_PARENTS: cmd.append(\"--src-use-parents\")\n",
    "    if USE_EQUIVALENT: cmd.append(\"--src-use-equivalent\")\n",
    "    if USE_DISJOINT: cmd.append(\"--src-use-disjoint\")\n",
    "    if VISUALIZE: cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "# Mode: full or train-only => requires model args\n",
    "if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "    cmd += [\"--model-type\", MODEL_TYPE, \"--model-name\", MODEL_NAME, \"--model-output-dir\", model_out_dir]\n",
    "    cmd += [\"--num-epochs\", str(NUM_EPOCHS)]\n",
    "    \n",
    "    if HYPERPARAMETER_TUNING:\n",
    "        cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "    elif USE_FIXED_HYPERPARAMS:\n",
    "        cmd += [\"--learning-rate\", str(LEARNING_RATE)]\n",
    "        cmd += [\"--batch-size\", str(BATCH_SIZE)]\n",
    "        cmd += [\"--weight-decay\", str(WEIGHT_DECAY)]\n",
    "\n",
    "# Mode: train-only => requires dataset CSV\n",
    "if RUN_MODE == \"train-only\":\n",
    "    cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "print(\"Running command:\\n\", \" \".join(cmd))\n",
    "print(f\"\\nLogs: {log_path}\")\n",
    "\n",
    "# Create dirs\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(model_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Run and capture logs\n",
    "with open(log_path, \"w\") as log_file:\n",
    "    proc = subprocess.run(cmd, stdout=log_file, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Return code:\", proc.returncode)\n",
    "\n",
    "if proc.returncode != 0:\n",
    "    print(\"!!! Error occurred. Showing last 60 lines of log:\")\n",
    "    os.system(f\"tail -n 60 {log_path}\")\n",
    "else:\n",
    "    print(f\"Run completed successfully. Outputs are under: {out_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b21b9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeda81f",
   "metadata": {},
   "source": [
    "## Inspect generated dataset (optional)\n",
    "\n",
    "This section is meaningful only if the run produced `training_dataset.csv`\n",
    "(i.e., mode = `full` or `build-dataset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "with open(\"outputs/LAST_TRAINING_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "dataset_csv = f\"{out_dir}/training_dataset.csv\"\n",
    "\n",
    "if os.path.exists(dataset_csv):\n",
    "    df = pd.read_csv(dataset_csv)\n",
    "    print(\"Training dataset shape:\", df.shape)\n",
    "    display(df.head(10))\n",
    "    print(\"\\nLabel distribution:\")\n",
    "    display(df[\"match\"].value_counts())\n",
    "else:\n",
    "    print(\"No training_dataset.csv found for this run (expected in train-only mode).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69680da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda535dd",
   "metadata": {},
   "source": [
    "## Package outputs for download\n",
    "\n",
    "This zips the entire run directory (logs, CSVs, and model artifacts) into a single file\n",
    "and downloads it to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66469d1a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "OUT_DIR=$(cat outputs/LAST_TRAINING_RUN_DIR.txt)\n",
    "ZIP_PATH=\"${OUT_DIR}.zip\"\n",
    "\n",
    "echo \"Zipping $OUT_DIR ...\"\n",
    "zip -r -q \"$ZIP_PATH\" \"$OUT_DIR\"\n",
    "echo \"Created zip: $ZIP_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "with open(\"outputs/LAST_TRAINING_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "files.download(f\"{out_dir}.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
