{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe50aeb6",
   "metadata": {},
   "source": [
    "# Unified Launcher: Training → Offline Bundle → Inference\n",
    "\n",
    "This notebook runs the **full pipeline** in a single session:\n",
    "\n",
    "1) **Training** (`training.py`)\n",
    "   - Builds the training dataset (if needed)\n",
    "   - Splits into train/val/test (in `full` mode)\n",
    "   - Trains the selected model\n",
    "   - Writes artifacts to a unique run directory under `outputs/`\n",
    "\n",
    "2) **Offline preprocessing** (`build_ontology_bundle.py`)\n",
    "   - Builds the internal ontology CSV (`iri, local_name, label, text`)\n",
    "   - Builds the `offline_bundle.pkl` used for retrieval\n",
    "   - Computes semantic embeddings using a bi-encoder (for hybrid retrieval)\n",
    "\n",
    "3) **Inference** (`run_inference.py`)\n",
    "   - Loads the offline bundle + ontology CSV\n",
    "   - Retrieves candidates (lexical or hybrid)\n",
    "   - Scores candidates with the trained cross-encoder\n",
    "   - Produces a predictions CSV (optionally storing top-N candidates)\n",
    "\n",
    "**Important:**  \n",
    "Do **not** zip/download intermediate artifacts if your goal is to run inference immediately after training.  \n",
    "Zipping is only useful to export results to your local machine at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80413d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b750e9c",
   "metadata": {},
   "source": [
    "## Setup — Repository and Dependencies\n",
    "\n",
    "This cell prepares the execution environment and makes the notebook **fully reproducible** on platforms such as Google Colab or similar hosted environments.\n",
    "\n",
    "Specifically, the cell performs the following steps:\n",
    "\n",
    "1. **Clones the project repository** (or reuses it if it is already present).\n",
    "2. **Sets the working directory** to the repository root, so that the core scripts\n",
    "   (`training.py`, `build_ontology_bundle.py`, `run_inference.py`) can be executed\n",
    "   directly from the notebook.\n",
    "3. **Installs Python dependencies** from `requirements.txt`, if the file is present.\n",
    "4. **Runs sanity checks** to ensure that the main framework scripts are available\n",
    "   in the expected locations.\n",
    "\n",
    "After this cell completes successfully, the notebook can safely execute the full\n",
    "pipeline: **training → offline bundle → inference**, in a single, unified workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP (clone repo + install deps)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/adsp-polito/2025-P13-Ontology-Alignment.git\"  # <-- cambia se serve\n",
    "REPO_DIR = Path(\"repo\").resolve()\n",
    "\n",
    "def sh(cmd: str):\n",
    "    print(\"\\n$\", cmd)\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "# 1) Clone (or reuse)\n",
    "if not REPO_DIR.exists():\n",
    "    sh(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
    "else:\n",
    "    print(\"Repo already present at:\", REPO_DIR)\n",
    "\n",
    "# 2) Move into repo\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"CWD:\", Path(\".\").resolve())\n",
    "\n",
    "# 3) Install requirements (best-effort)\n",
    "if Path(\"requirements.txt\").exists():\n",
    "    sh(\"pip -q install -r requirements.txt\")\n",
    "else:\n",
    "    print(\"No requirements.txt found. Skipping pip install.\")\n",
    "\n",
    "# 4) Sanity checks: scripts must exist\n",
    "for p in [\"training.py\", \"build_ontology_bundle.py\", \"run_inference.py\"]:\n",
    "    if not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"Missing {p} in repo root: {Path('.').resolve()}\")\n",
    "\n",
    "print(\"Setup OK: repo + scripts found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae35da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166d008",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set all parameters below.  \n",
    "The pipeline will create a unique run directory in `outputs/` and reuse it across all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION (unified training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# Run id / output layout\n",
    "# -----------------------------\n",
    "RUN_ID = f\"unified_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUT_DIR = Path(\"outputs\") / RUN_ID\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = OUT_DIR / \"training\"\n",
    "OFFLINE_DIR = OUT_DIR / \"offline\"\n",
    "INFER_DIR = OUT_DIR / \"inference\"\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OFFLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INFER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Training mode and model\n",
    "# -----------------------------\n",
    "RUN_MODE = \"full\"  # \"full\" | \"build-dataset\" | \"train-only\"\n",
    "MODEL_TYPE = \"cross-encoder\"  # keep this if you want inference at the end\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "HYPERPARAMETER_TUNING = False\n",
    "N_TRIALS = 5\n",
    "\n",
    "USE_FIXED_HYPERPARAMS = True\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "SPLIT_RATIOS = \"0.75,0.15,0.10\"\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs for dataset building\n",
    "# -----------------------------\n",
    "SRC_PATH = \"data/sweet.owl\"\n",
    "TGT_PATH = \"data/envo.owl\"\n",
    "ALIGN_PATH = \"data/envo-sweet.rdf\"\n",
    "\n",
    "SRC_PREFIX = None\n",
    "TGT_PREFIX = None  # e.g. \"http://purl.obolibrary.org/obo/ENVO_\"\n",
    "\n",
    "USE_DESCRIPTION = True\n",
    "USE_SYNONYMS = True\n",
    "USE_PARENTS = True\n",
    "USE_EQUIVALENT = True\n",
    "USE_DISJOINT = True\n",
    "\n",
    "VISUALIZE = False\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical outputs of STEP 1\n",
    "# -----------------------------\n",
    "OUT_SRC_CSV = str(TRAIN_DIR / \"source_ontology.csv\")\n",
    "OUT_TGT_CSV = str(TRAIN_DIR / \"target_ontology.csv\")\n",
    "OUT_DATASET_CSV = str(TRAIN_DIR / \"training_dataset.csv\")\n",
    "\n",
    "TRAIN_SPLIT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".train.csv\"))\n",
    "VAL_SPLIT_CSV   = str(Path(OUT_DATASET_CSV).with_suffix(\".val.csv\"))\n",
    "TEST_SPLIT_CSV  = str(Path(OUT_DATASET_CSV).with_suffix(\".test.csv\"))\n",
    "\n",
    "# train-only mode\n",
    "DATASET_CSV = TRAIN_SPLIT_CSV\n",
    "\n",
    "# model outputs\n",
    "MODEL_OUT_DIR = str(TRAIN_DIR / \"models\" / f\"{MODEL_TYPE}_custom\")\n",
    "FINAL_CROSS_ENCODER_DIR = str(Path(MODEL_OUT_DIR) / \"final_cross_encoder_model\")\n",
    "\n",
    "# -----------------------------\n",
    "# Offline bundle builder\n",
    "# -----------------------------\n",
    "OFFLINE_EXPORT_CSV = None\n",
    "OFFLINE_ONT_PATH = TGT_PATH\n",
    "OFFLINE_PREFIX = TGT_PREFIX\n",
    "\n",
    "CROSS_TOKENIZER_NAME = MODEL_NAME\n",
    "\n",
    "BI_ENCODER_MODEL_ID = \"allenai/scibert_scivocab_uncased\"\n",
    "OFFLINE_SEMANTIC_BATCH_SIZE = 64\n",
    "OFFLINE_SEMANTIC_MAX_LENGTH = 256\n",
    "OFFLINE_NO_SEMANTIC_NORMALIZE = False\n",
    "\n",
    "ONTOLOGY_INTERNAL_CSV = str(OFFLINE_DIR / \"ontology_internal.csv\")\n",
    "OFFLINE_BUNDLE_PKL = str(OFFLINE_DIR / \"offline_bundle.pkl\")\n",
    "\n",
    "# -----------------------------\n",
    "# Inference\n",
    "# -----------------------------\n",
    "INFER_INPUT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".test.queries.csv\"))  # Option B queries\n",
    "INFER_OUT_CSV = str(INFER_DIR / \"predictions.csv\")\n",
    "\n",
    "RETRIEVAL_COL = \"source_label\"\n",
    "SCORING_COL = \"source_text\"\n",
    "ID_COL = \"source_iri\"\n",
    "\n",
    "INFER_MODE = \"hybrid\"\n",
    "RETRIEVAL_LEXICAL_TOP_K = 100\n",
    "RETRIEVAL_SEMANTIC_TOP_K = 100\n",
    "RETRIEVAL_MERGED_TOP_K = 150\n",
    "HYBRID_RATIO_SEMANTIC = 0.5\n",
    "SEMANTIC_BATCH_SIZE = 64\n",
    "\n",
    "CROSS_TOP_K = 20\n",
    "CROSS_BATCH_SIZE = 32\n",
    "CROSS_MAX_LENGTH = 256\n",
    "\n",
    "KEEP_TOP_N = 0\n",
    "\n",
    "print(\"Config OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a4c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac28b2",
   "metadata": {},
   "source": [
    "## Run pipeline (3 steps)\n",
    "\n",
    "This section executes:\n",
    "1) `training.py`\n",
    "2) `build_ontology_bundle.py`\n",
    "3) `run_inference.py`\n",
    "\n",
    "Logs are written under the run directory.\n",
    "The notebook stops immediately if any step fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN PIPELINE (training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "def print_tail(path: Path, n=120):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"[tail] log not found: {p}\")\n",
    "        return\n",
    "    lines = p.read_text(errors=\"replace\").splitlines()\n",
    "    print(\"\\n\".join(lines[-n:]))\n",
    "\n",
    "def run_cmd(cmd, log_path: Path, cwd: Path):\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"CWD:\", cwd)\n",
    "    print(\"Log:\", log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        proc = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, cwd=str(cwd))\n",
    "\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"!!! Error occurred. Last lines of log:\")\n",
    "        print_tail(log_path, n=120)\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}. See log: {log_path}\")\n",
    "    return proc.returncode\n",
    "\n",
    "# Guardrails\n",
    "if RUN_MODE == \"full\" and MODEL_TYPE != \"cross-encoder\":\n",
    "    raise ValueError(\"RUN_MODE='full' ends with inference => needs MODEL_TYPE='cross-encoder'.\")\n",
    "\n",
    "if HYPERPARAMETER_TUNING and RUN_MODE != \"full\":\n",
    "    raise ValueError(\"--tune only allowed in RUN_MODE='full'.\")\n",
    "\n",
    "Path(MODEL_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1) TRAINING\n",
    "# -----------------------------\n",
    "train_log = TRAIN_DIR / \"training.log\"\n",
    "\n",
    "train_cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "    train_cmd += [\"--src\", SRC_PATH, \"--tgt\", TGT_PATH, \"--align\", ALIGN_PATH]\n",
    "    train_cmd += [\"--out-src\", OUT_SRC_CSV, \"--out-tgt\", OUT_TGT_CSV, \"--out-dataset\", OUT_DATASET_CSV]\n",
    "    train_cmd += [\"--split-ratios\", SPLIT_RATIOS]\n",
    "\n",
    "    if SRC_PREFIX:\n",
    "        train_cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "    if TGT_PREFIX:\n",
    "        train_cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "    if USE_DESCRIPTION: train_cmd.append(\"--src-use-description\")\n",
    "    if USE_SYNONYMS: train_cmd.append(\"--src-use-synonyms\")\n",
    "    if USE_PARENTS: train_cmd.append(\"--src-use-parents\")\n",
    "    if USE_EQUIVALENT: train_cmd.append(\"--src-use-equivalent\")\n",
    "    if USE_DISJOINT: train_cmd.append(\"--src-use-disjoint\")\n",
    "    if VISUALIZE: train_cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "    train_cmd += [\"--model-type\", MODEL_TYPE, \"--model-name\", MODEL_NAME, \"--model-output-dir\", MODEL_OUT_DIR]\n",
    "    train_cmd += [\"--num-epochs\", str(NUM_EPOCHS)]\n",
    "\n",
    "    if HYPERPARAMETER_TUNING:\n",
    "        train_cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "    elif USE_FIXED_HYPERPARAMS:\n",
    "        train_cmd += [\"--learning-rate\", str(LEARNING_RATE)]\n",
    "        train_cmd += [\"--batch-size\", str(BATCH_SIZE)]\n",
    "        train_cmd += [\"--weight-decay\", str(WEIGHT_DECAY)]\n",
    "\n",
    "if RUN_MODE == \"train-only\":\n",
    "    train_cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "run_cmd(train_cmd, train_log, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nTraining completed.\")\n",
    "print(\"Dataset CSV:\", OUT_DATASET_CSV)\n",
    "print(\"Train split:\", TRAIN_SPLIT_CSV)\n",
    "print(\"Val split:\", VAL_SPLIT_CSV)\n",
    "print(\"Test split:\", TEST_SPLIT_CSV)\n",
    "print(\"Cross-encoder dir:\", FINAL_CROSS_ENCODER_DIR)\n",
    "\n",
    "CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2) OFFLINE BUNDLE\n",
    "# -----------------------------\n",
    "offline_log = OFFLINE_DIR / \"offline_bundle.log\"\n",
    "\n",
    "offline_cmd = [\n",
    "    \"python\", \"build_ontology_bundle.py\",\n",
    "    \"--out-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "    \"--out-bundle\", OFFLINE_BUNDLE_PKL,\n",
    "    \"--tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--bi-encoder-model-id\", BI_ENCODER_MODEL_ID,\n",
    "    \"--semantic-batch-size\", str(OFFLINE_SEMANTIC_BATCH_SIZE),\n",
    "    \"--semantic-max-length\", str(OFFLINE_SEMANTIC_MAX_LENGTH),\n",
    "]\n",
    "if OFFLINE_NO_SEMANTIC_NORMALIZE:\n",
    "    offline_cmd.append(\"--no-semantic-normalize\")\n",
    "\n",
    "if OFFLINE_EXPORT_CSV:\n",
    "    offline_cmd += [\"--export-csv\", OFFLINE_EXPORT_CSV]\n",
    "else:\n",
    "    offline_cmd += [\"--ont-path\", OFFLINE_ONT_PATH]\n",
    "    if OFFLINE_PREFIX:\n",
    "        offline_cmd += [\"--prefix\", OFFLINE_PREFIX]\n",
    "\n",
    "run_cmd(offline_cmd, offline_log, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nOffline bundle completed.\")\n",
    "print(\"Ontology internal CSV:\", ONTOLOGY_INTERNAL_CSV)\n",
    "print(\"Offline bundle PKL:\", OFFLINE_BUNDLE_PKL)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3) INFERENCE\n",
    "# -----------------------------\n",
    "infer_log = INFER_DIR / \"inference.log\"\n",
    "\n",
    "if not Path(INFER_INPUT_CSV).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"INFER_INPUT_CSV not found: {INFER_INPUT_CSV}\\n\"\n",
    "        \"In full/build-dataset mode, training should generate *.test.queries.csv. \"\n",
    "        \"If you want a custom query file, set INFER_INPUT_CSV to its path.\"\n",
    "    )\n",
    "\n",
    "infer_cmd = [\n",
    "    \"python\", \"run_inference.py\",\n",
    "    \"--bundle\", OFFLINE_BUNDLE_PKL,\n",
    "    \"--ontology-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "    \"--input-csv\", INFER_INPUT_CSV,\n",
    "    \"--out-csv\", INFER_OUT_CSV,\n",
    "    \"--mode\", INFER_MODE,\n",
    "    \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "    \"--retrieval-col\", RETRIEVAL_COL,\n",
    "    \"--retrieval-lexical-top-k\", str(RETRIEVAL_LEXICAL_TOP_K),\n",
    "    \"--retrieval-semantic-top-k\", str(RETRIEVAL_SEMANTIC_TOP_K),\n",
    "    \"--retrieval-merged-top-k\", str(RETRIEVAL_MERGED_TOP_K),\n",
    "    \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEMANTIC),\n",
    "    \"--semantic-batch-size\", str(SEMANTIC_BATCH_SIZE),\n",
    "    \"--cross-top-k\", str(CROSS_TOP_K),\n",
    "    \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "    \"--cross-max-length\", str(CROSS_MAX_LENGTH),\n",
    "    \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "]\n",
    "if SCORING_COL:\n",
    "    infer_cmd += [\"--scoring-col\", SCORING_COL]\n",
    "if ID_COL:\n",
    "    infer_cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "run_cmd(infer_cmd, infer_log, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nUnified pipeline completed successfully.\")\n",
    "print(\"Outputs:\")\n",
    "print(\" - Training:\", TRAIN_DIR)\n",
    "print(\" - Offline bundle:\", OFFLINE_DIR)\n",
    "print(\" - Inference:\", INFER_DIR)\n",
    "print(\"Predictions CSV:\", INFER_OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306fb11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf0e62",
   "metadata": {},
   "source": [
    "## Optional: package outputs for download (run only at the end)\n",
    "\n",
    "This is only needed if you want to export artifacts locally.\n",
    "Skip it if you are iterating quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ed6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "ZIP_PATH = str(OUT_DIR) + \".zip\"\n",
    "print(\"Zipping:\", OUT_DIR, \"->\", ZIP_PATH)\n",
    "\n",
    "# Make a zip of the whole run dir (simple + safe)\n",
    "shutil.make_archive(str(OUT_DIR), \"zip\", root_dir=str(OUT_DIR))\n",
    "print(\"Created:\", ZIP_PATH)\n",
    "\n",
    "# To download (Colab)\n",
    "from google.colab import files\n",
    "files.download(ZIP_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
