{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe50aeb6",
   "metadata": {},
   "source": [
    "# Unified Launcher: Training → Offline Bundle → Inference\n",
    "\n",
    "This notebook runs the **full pipeline** in a single session:\n",
    "\n",
    "1) **Training** (`training.py`)\n",
    "   - Builds the training dataset (if needed)\n",
    "   - Splits into train/val/test (in `full` mode)\n",
    "   - Trains the selected model\n",
    "   - Writes artifacts to a unique run directory under `outputs/`\n",
    "\n",
    "2) **Offline preprocessing** (`build_ontology_bundle.py`)\n",
    "   - Builds the internal ontology CSV (`iri, local_name, label, text`)\n",
    "   - Builds the `offline_bundle.pkl` used for retrieval\n",
    "   - Computes semantic embeddings using a bi-encoder (for hybrid retrieval)\n",
    "\n",
    "3) **Inference** (`run_inference.py`)\n",
    "   - Loads the offline bundle + ontology CSV\n",
    "   - Retrieves candidates (lexical or hybrid)\n",
    "   - Scores candidates with the trained cross-encoder\n",
    "   - Produces a predictions CSV (optionally storing top-N candidates)\n",
    "\n",
    "**Important:**  \n",
    "Do **not** zip/download intermediate artifacts if your goal is to run inference immediately after training.  \n",
    "Zipping is only useful to export results to your local machine at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80413d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166d008",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set all parameters below.  \n",
    "The pipeline will create a unique run directory in `outputs/` and reuse it across all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION (unified training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Run id / output layout\n",
    "# -----------------------------\n",
    "RUN_ID = f\"unified_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUT_DIR = Path(\"outputs\") / RUN_ID\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subfolders\n",
    "TRAIN_DIR = OUT_DIR / \"training\"\n",
    "OFFLINE_DIR = OUT_DIR / \"offline\"\n",
    "INFER_DIR = OUT_DIR / \"inference\"\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OFFLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INFER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Training mode and model\n",
    "# -----------------------------\n",
    "RUN_MODE = \"full\"  # \"full\" | \"build-dataset\" | \"train-only\"\n",
    "# IMPORTANT:\n",
    "# - Inference always uses a cross-encoder scorer (run_inference.py).\n",
    "# - So if you want the pipeline to end with inference, you should either:\n",
    "#   (A) train a cross-encoder (recommended), or\n",
    "#   (B) provide an external cross-encoder scorer model path/id.\n",
    "MODEL_TYPE = \"cross-encoder\"  # \"bi-encoder\" | \"cross-encoder\"\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Hyperparameter tuning (allowed only with RUN_MODE=\"full\")\n",
    "HYPERPARAMETER_TUNING = False\n",
    "N_TRIALS = 5\n",
    "\n",
    "# Fixed hyperparams (used only if tuning disabled)\n",
    "USE_FIXED_HYPERPARAMS = True\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Split ratios used by training.py (only for full/build-dataset)\n",
    "SPLIT_RATIOS = \"0.75,0.15,0.10\"  # Train/val/test ratios (must sum to 1.0)\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs for training dataset building (full/build-dataset)\n",
    "# -----------------------------\n",
    "SRC_PATH = \"data/sweet.owl\"\n",
    "TGT_PATH = \"data/envo.owl\"\n",
    "ALIGN_PATH = \"data/envo-sweet.rdf\"\n",
    "\n",
    "SRC_PREFIX = None\n",
    "TGT_PREFIX = None  # e.g., \"http://purl.obolibrary.org/obo/ENVO_\"\n",
    "\n",
    "USE_DESCRIPTION = True\n",
    "USE_SYNONYMS = True\n",
    "USE_PARENTS = True\n",
    "USE_EQUIVALENT = True\n",
    "USE_DISJOINT = True\n",
    "\n",
    "# Visualization can be slow / fragile in hosted notebooks\n",
    "VISUALIZE = False\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical outputs of STEP 1 (training.py writes these)\n",
    "# -----------------------------\n",
    "OUT_SRC_CSV = str(TRAIN_DIR / \"source_ontology.csv\")\n",
    "OUT_TGT_CSV = str(TRAIN_DIR / \"target_ontology.csv\")\n",
    "OUT_DATASET_CSV = str(TRAIN_DIR / \"training_dataset.csv\")  # canonical dataset path for this RUN\n",
    "\n",
    "# Splits that training.py produces in build-dataset/full mode\n",
    "TRAIN_SPLIT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".train.csv\"))\n",
    "VAL_SPLIT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".val.csv\"))\n",
    "TEST_SPLIT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".test.csv\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs for train-only\n",
    "# -----------------------------\n",
    "# train-only assumes the provided CSV is the TRAINING set (no splitting).\n",
    "# Keep it explicit: if you run train-only, set DATASET_CSV to the training CSV you want.\n",
    "DATASET_CSV = TRAIN_SPLIT_CSV  # sensible default in unified pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Model output dir (STEP 1)\n",
    "# -----------------------------\n",
    "MODEL_OUT_DIR = str(TRAIN_DIR / \"models\" / f\"{MODEL_TYPE}_custom\")\n",
    "\n",
    "# After training, we will use this as cross-encoder scorer for inference\n",
    "# (it is a local path produced by SentenceTransformers `model.save(...)`)\n",
    "FINAL_CROSS_ENCODER_DIR = str(Path(MODEL_OUT_DIR) / \"final_cross_encoder_model\")\n",
    "\n",
    "# -----------------------------\n",
    "# Offline bundle builder (build_ontology_bundle.py)\n",
    "# -----------------------------\n",
    "# You can build from OWL/RDF or from an already-exported CSV\n",
    "OFFLINE_EXPORT_CSV = None          # e.g., \"data/ontology_internal.csv\" OR None\n",
    "OFFLINE_ONT_PATH = TGT_PATH        # typically the target ontology used for retrieval\n",
    "OFFLINE_PREFIX = TGT_PREFIX\n",
    "\n",
    "# Tokenizer for lexical retrieval must match the cross-encoder tokenizer you want to use\n",
    "CROSS_TOKENIZER_NAME = MODEL_NAME  # keep aligned with your scorer tokenizer\n",
    "\n",
    "# bi-encoder used ONLY to compute semantic embeddings for hybrid retrieval\n",
    "BI_ENCODER_MODEL_ID = \"allenai/scibert_scivocab_uncased\"\n",
    "OFFLINE_SEMANTIC_BATCH_SIZE = 64\n",
    "OFFLINE_SEMANTIC_MAX_LENGTH = 256\n",
    "OFFLINE_NO_SEMANTIC_NORMALIZE = False\n",
    "\n",
    "# Offline outputs\n",
    "ONTOLOGY_INTERNAL_CSV = str(OFFLINE_DIR / \"ontology_internal.csv\")\n",
    "OFFLINE_BUNDLE_PKL = str(OFFLINE_DIR / \"offline_bundle.pkl\")\n",
    "\n",
    "# -----------------------------\n",
    "# Inference (run_inference.py)\n",
    "# -----------------------------\n",
    "# Use the test split generated by STEP 1 (full/build-dataset)\n",
    "INFER_INPUT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".test.queries.csv\"))\n",
    "INFER_OUT_CSV = str(INFER_DIR / \"predictions.csv\")\n",
    "\n",
    "# Your test split columns:\n",
    "# source_iri, target_iri, source_label, target_label, source_text, target_text, sample_type, match\n",
    "RETRIEVAL_COL = \"source_label\"   # exact + lexical retrieval\n",
    "SCORING_COL = \"source_text\"      # cross-encoder scorer + semantic retrieval query text\n",
    "ID_COL = \"source_iri\"            # carry through an id per attribute\n",
    "\n",
    "INFER_MODE = \"hybrid\"  # \"lexical\" | \"hybrid\"\n",
    "\n",
    "RETRIEVAL_LEXICAL_TOP_K = 100\n",
    "RETRIEVAL_SEMANTIC_TOP_K = 100\n",
    "RETRIEVAL_MERGED_TOP_K = 150\n",
    "HYBRID_RATIO_SEMANTIC = 0.5\n",
    "SEMANTIC_BATCH_SIZE = 64\n",
    "\n",
    "CROSS_TOP_K = 20\n",
    "CROSS_BATCH_SIZE = 32\n",
    "CROSS_MAX_LENGTH = 256\n",
    "\n",
    "KEEP_TOP_N = 0\n",
    "\n",
    "print(\"Config OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a4c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac28b2",
   "metadata": {},
   "source": [
    "## Run pipeline (3 steps)\n",
    "\n",
    "This section executes:\n",
    "1) `training.py`\n",
    "2) `build_ontology_bundle.py`\n",
    "3) `run_inference.py`\n",
    "\n",
    "Logs are written under the run directory.\n",
    "The notebook stops immediately if any step fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN PIPELINE (training -> offline -> inference)\n",
    "# ============================================\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_cmd(cmd, log_path: Path):\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"Log:\", log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        proc = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT)\n",
    "\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"!!! Error occurred. Showing last 80 lines of log:\")\n",
    "        os.system(f\"tail -n 80 {log_path}\")\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}. See log: {log_path}\")\n",
    "    return proc.returncode\n",
    "\n",
    "# Guardrails for coherence (avoid silent nonsense)\n",
    "if RUN_MODE == \"full\" and MODEL_TYPE != \"cross-encoder\":\n",
    "    raise ValueError(\n",
    "        \"RUN_MODE='full' runs inference at the end, which needs a cross-encoder scorer. \"\n",
    "        \"Set MODEL_TYPE='cross-encoder' (recommended) or modify the pipeline to provide an external scorer.\"\n",
    "    )\n",
    "\n",
    "if HYPERPARAMETER_TUNING and RUN_MODE != \"full\":\n",
    "    raise ValueError(\"--tune is only allowed in RUN_MODE='full'.\")\n",
    "\n",
    "Path(MODEL_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1) TRAINING\n",
    "# -----------------------------\n",
    "train_log = TRAIN_DIR / \"training.log\"\n",
    "\n",
    "train_cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "# Mode: full or build-dataset => requires ontologies + alignment + output CSV paths\n",
    "if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "    train_cmd += [\"--src\", SRC_PATH, \"--tgt\", TGT_PATH, \"--align\", ALIGN_PATH]\n",
    "    train_cmd += [\"--out-src\", OUT_SRC_CSV, \"--out-tgt\", OUT_TGT_CSV, \"--out-dataset\", OUT_DATASET_CSV]\n",
    "    train_cmd += [\"--split-ratios\", SPLIT_RATIOS]\n",
    "\n",
    "    if SRC_PREFIX:\n",
    "        train_cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "    if TGT_PREFIX:\n",
    "        train_cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "    if USE_DESCRIPTION: train_cmd.append(\"--src-use-description\")\n",
    "    if USE_SYNONYMS: train_cmd.append(\"--src-use-synonyms\")\n",
    "    if USE_PARENTS: train_cmd.append(\"--src-use-parents\")\n",
    "    if USE_EQUIVALENT: train_cmd.append(\"--src-use-equivalent\")\n",
    "    if USE_DISJOINT: train_cmd.append(\"--src-use-disjoint\")\n",
    "    if VISUALIZE: train_cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "# Mode: full or train-only => requires model args\n",
    "if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "    train_cmd += [\"--model-type\", MODEL_TYPE, \"--model-name\", MODEL_NAME, \"--model-output-dir\", MODEL_OUT_DIR]\n",
    "    train_cmd += [\"--num-epochs\", str(NUM_EPOCHS)]\n",
    "\n",
    "    if HYPERPARAMETER_TUNING:\n",
    "        train_cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "    elif USE_FIXED_HYPERPARAMS:\n",
    "        train_cmd += [\"--learning-rate\", str(LEARNING_RATE)]\n",
    "        train_cmd += [\"--batch-size\", str(BATCH_SIZE)]\n",
    "        train_cmd += [\"--weight-decay\", str(WEIGHT_DECAY)]\n",
    "\n",
    "# Mode: train-only => requires dataset CSV (assumed train set)\n",
    "if RUN_MODE == \"train-only\":\n",
    "    train_cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "run_cmd(train_cmd, train_log)\n",
    "\n",
    "print(\"\\nTraining completed.\")\n",
    "print(\"Dataset CSV:\", OUT_DATASET_CSV)\n",
    "print(\"Train split:\", TRAIN_SPLIT_CSV)\n",
    "print(\"Val split:\", VAL_SPLIT_CSV)\n",
    "print(\"Test split:\", TEST_SPLIT_CSV)\n",
    "print(\"Model out dir:\", MODEL_OUT_DIR)\n",
    "\n",
    "# Cross-encoder scorer directory for inference\n",
    "CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2) OFFLINE BUNDLE\n",
    "# -----------------------------\n",
    "offline_log = OFFLINE_DIR / \"offline_bundle.log\"\n",
    "\n",
    "offline_cmd = [\n",
    "    \"python\", \"build_ontology_bundle.py\",\n",
    "    \"--out-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "    \"--out-bundle\", OFFLINE_BUNDLE_PKL,\n",
    "    \"--tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--bi-encoder-model-id\", BI_ENCODER_MODEL_ID,\n",
    "    \"--semantic-batch-size\", str(OFFLINE_SEMANTIC_BATCH_SIZE),\n",
    "    \"--semantic-max-length\", str(OFFLINE_SEMANTIC_MAX_LENGTH),\n",
    "]\n",
    "\n",
    "if OFFLINE_NO_SEMANTIC_NORMALIZE:\n",
    "    offline_cmd.append(\"--no-semantic-normalize\")\n",
    "\n",
    "# Choose ontology source: either export-csv OR ont-path\n",
    "if OFFLINE_EXPORT_CSV:\n",
    "    offline_cmd += [\"--export-csv\", OFFLINE_EXPORT_CSV]\n",
    "else:\n",
    "    offline_cmd += [\"--ont-path\", OFFLINE_ONT_PATH]\n",
    "    if OFFLINE_PREFIX:\n",
    "        offline_cmd += [\"--prefix\", OFFLINE_PREFIX]\n",
    "\n",
    "run_cmd(offline_cmd, offline_log)\n",
    "\n",
    "print(\"\\nOffline bundle completed.\")\n",
    "print(\"Ontology internal CSV:\", ONTOLOGY_INTERNAL_CSV)\n",
    "print(\"Offline bundle PKL:\", OFFLINE_BUNDLE_PKL)\n",
    "\n",
    "if RUN_MODE == \"build-dataset\":\n",
    "    print(\"\\nRUN_MODE='build-dataset': dataset + splits + queries/gold generated. Skipping inference (no trained model).\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3) INFERENCE\n",
    "# -----------------------------\n",
    "infer_log = INFER_DIR / \"inference.log\"\n",
    "\n",
    "if not Path(CROSS_ENCODER_MODEL_ID).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cross-encoder model not found: {CROSS_ENCODER_MODEL_ID}\\n\"\n",
    "        \"If you want to run inference with an external scorer, set CROSS_ENCODER_MODEL_ID accordingly.\"\n",
    "    )\n",
    "\n",
    "# If RUN_MODE is train-only, INFER_INPUT_CSV might not exist unless you set it explicitly.\n",
    "if not Path(INFER_INPUT_CSV).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"INFER_INPUT_CSV not found: {INFER_INPUT_CSV}\\n\"\n",
    "        \"If RUN_MODE='train-only', set INFER_INPUT_CSV manually to the CSV you want to run inference on.\"\n",
    "    )\n",
    "\n",
    "infer_cmd = [\n",
    "    \"python\", \"run_inference.py\",\n",
    "    \"--bundle\", OFFLINE_BUNDLE_PKL,\n",
    "    \"--ontology-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "    \"--input-csv\", INFER_INPUT_CSV,\n",
    "    \"--out-csv\", INFER_OUT_CSV,\n",
    "    \"--mode\", INFER_MODE,\n",
    "    \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "    \"--retrieval-col\", RETRIEVAL_COL,\n",
    "    \"--retrieval-lexical-top-k\", str(RETRIEVAL_LEXICAL_TOP_K),\n",
    "    \"--retrieval-semantic-top-k\", str(RETRIEVAL_SEMANTIC_TOP_K),\n",
    "    \"--retrieval-merged-top-k\", str(RETRIEVAL_MERGED_TOP_K),\n",
    "    \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEMANTIC),\n",
    "    \"--semantic-batch-size\", str(SEMANTIC_BATCH_SIZE),\n",
    "    \"--cross-top-k\", str(CROSS_TOP_K),\n",
    "    \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "    \"--cross-max-length\", str(CROSS_MAX_LENGTH),\n",
    "    \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "]\n",
    "\n",
    "if SCORING_COL:\n",
    "    infer_cmd += [\"--scoring-col\", SCORING_COL]\n",
    "if ID_COL:\n",
    "    infer_cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "run_cmd(infer_cmd, infer_log)\n",
    "\n",
    "print(\"\\nUnified pipeline completed successfully.\")\n",
    "print(\"Outputs:\")\n",
    "print(\" - Training:\", TRAIN_DIR)\n",
    "print(\" - Offline bundle:\", OFFLINE_DIR)\n",
    "print(\" - Inference:\", INFER_DIR)\n",
    "print(\"Predictions CSV:\", INFER_OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306fb11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf0e62",
   "metadata": {},
   "source": [
    "## Optional: package outputs for download (run only at the end)\n",
    "\n",
    "This is only needed if you want to export artifacts locally.\n",
    "Skip it if you are iterating quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ed6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "ZIP_PATH = str(OUT_DIR) + \".zip\"\n",
    "print(\"Zipping:\", OUT_DIR, \"->\", ZIP_PATH)\n",
    "\n",
    "# Make a zip of the whole run dir (simple + safe)\n",
    "shutil.make_archive(str(OUT_DIR), \"zip\", root_dir=str(OUT_DIR))\n",
    "print(\"Created:\", ZIP_PATH)\n",
    "\n",
    "# To download (Colab)\n",
    "from google.colab import files\n",
    "files.download(ZIP_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
