{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe50aeb6",
   "metadata": {},
   "source": [
    "# Unified Ontology Alignment Pipeline  \n",
    "### Training · Offline Indexing · Inference (Modular & Restorable)\n",
    "\n",
    "This notebook provides a **unified, modular pipeline** for ontology alignment based on transformer models (cross-encoder + bi-encoder), designed to be **fully reproducible**, **restartable**, and **flexible** across different execution scenarios.\n",
    "\n",
    "The pipeline supports:\n",
    "- dataset construction and model training,\n",
    "- offline preprocessing for efficient inference,\n",
    "- large-scale inference with configurable retrieval and scoring,\n",
    "- restoration of previously trained models and offline artifacts.\n",
    "\n",
    "The notebook is designed for **Colab-first usage**, but the logic mirrors a production-ready CLI pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## What you can do with this notebook\n",
    "\n",
    "Depending on your needs, you can use this notebook to:\n",
    "\n",
    "- **Run the full pipeline end-to-end**\n",
    "  - build the training dataset,\n",
    "  - train a cross-encoder,\n",
    "  - build the offline semantic index,\n",
    "  - run inference on test or custom queries.\n",
    "\n",
    "- **Run only specific stages**\n",
    "  - dataset construction + training only,\n",
    "  - offline preprocessing only,\n",
    "  - inference only (using previously generated artifacts).\n",
    "\n",
    "- **Restore and reuse artifacts**\n",
    "  - load a previously trained cross-encoder,\n",
    "  - load a precomputed offline bundle,\n",
    "  - run inference without re-training or re-indexing.\n",
    "\n",
    "This avoids unnecessary recomputation and makes the notebook suitable for:\n",
    "- long Colab sessions,\n",
    "- interrupted runs,\n",
    "- collaborative workflows,\n",
    "- reproducible experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline stages (conceptual)\n",
    "\n",
    "The pipeline is logically divided into **three independent stages**:\n",
    "\n",
    "1. **Training & Dataset Construction**\n",
    "   - Loads source/target ontologies and gold alignments.\n",
    "   - Builds a labeled dataset with positives, hard negatives, and random negatives.\n",
    "   - Trains a cross-encoder model for fine-grained scoring.\n",
    "\n",
    "2. **Offline Preprocessing**\n",
    "   - Builds an internal ontology representation.\n",
    "   - Computes and stores semantic embeddings using a bi-encoder.\n",
    "   - Produces an offline bundle optimized for fast retrieval during inference.\n",
    "\n",
    "3. **Inference**\n",
    "   - Retrieves candidate matches using lexical and/or semantic search.\n",
    "   - Scores candidates using the trained cross-encoder.\n",
    "   - Outputs ranked predictions for each input query.\n",
    "\n",
    "Each stage can be executed **independently** or **skipped and restored**.\n",
    "\n",
    "---\n",
    "\n",
    "## Execution modes\n",
    "\n",
    "The notebook supports multiple execution modes through explicit flags.\n",
    "\n",
    "Typical usage patterns include:\n",
    "\n",
    "### Full pipeline (from scratch)\n",
    "- Training ✔\n",
    "- Offline preprocessing ✔\n",
    "- Inference ✔\n",
    "\n",
    "### Training only\n",
    "- Training ✔\n",
    "- Offline preprocessing ✘\n",
    "- Inference ✘\n",
    "\n",
    "### Offline preprocessing only\n",
    "- Training ✘\n",
    "- Offline preprocessing ✔\n",
    "- Inference ✘\n",
    "\n",
    "### Inference only (recommended for experimentation)\n",
    "- Training ✘ (model restored)\n",
    "- Offline preprocessing ✘ (bundle restored)\n",
    "- Inference ✔\n",
    "\n",
    "This makes it easy to:\n",
    "- train once,\n",
    "- reuse models and offline artifacts many times,\n",
    "- experiment with different inference parameters or input queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Artifact restoration philosophy\n",
    "\n",
    "Instead of hardcoding paths or forcing recomputation, this notebook allows you to:\n",
    "\n",
    "- **restore model artifacts** (trained cross-encoder),\n",
    "- **restore offline artifacts** (semantic bundle + ontology CSV),\n",
    "- **override inference inputs** (custom query files, different schemas).\n",
    "\n",
    "All overrides are performed **inside dedicated cells**, keeping:\n",
    "- configuration centralized,\n",
    "- logic explicit,\n",
    "- side effects local and controlled.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook structure (high level)\n",
    "\n",
    "1. **Setup**\n",
    "   - Clone repository and install dependencies.\n",
    "\n",
    "2. **Configuration**\n",
    "   - Centralized definition of paths, models, hyperparameters, and defaults.\n",
    "\n",
    "3. **Run Mode Flags**\n",
    "   - Choose which stages to execute and which artifacts to restore.\n",
    "\n",
    "4. **Optional Restore Cells**\n",
    "   - Restore offline inputs (for offline preprocessing).\n",
    "   - Restore offline artifacts (skip preprocessing).\n",
    "   - Restore model + inference inputs (skip training).\n",
    "\n",
    "5. **Execution Cells**\n",
    "   - Training stage.\n",
    "   - Offline preprocessing stage.\n",
    "   - Inference stage.\n",
    "\n",
    "6. **Export & Download**\n",
    "   - Zip and download results (predictions, gold alignments, artifacts).\n",
    "\n",
    "---\n",
    "\n",
    "## Design goals\n",
    "\n",
    "This notebook is designed with the following principles in mind:\n",
    "\n",
    "- **Modularity** – each stage is independent and reusable.\n",
    "- **Reproducibility** – all artifacts are explicit and versionable.\n",
    "- **Efficiency** – avoid recomputation whenever possible.\n",
    "- **Clarity** – configuration, execution, and restoration are clearly separated.\n",
    "\n",
    "If you follow the intended flow, you should never need to modify the core execution cells—only the configuration and run flags.\n",
    "\n",
    "---\n",
    "\n",
    "*You are now ready to configure and run the pipeline according to your needs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80413d8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b750e9c",
   "metadata": {},
   "source": [
    "## Setup — Repository and Dependencies\n",
    "\n",
    "This cell prepares the execution environment and makes the notebook **fully reproducible** on platforms such as Google Colab or similar hosted environments.\n",
    "\n",
    "Specifically, the cell performs the following steps:\n",
    "\n",
    "1. **Clones the project repository** (or reuses it if it is already present).\n",
    "2. **Sets the working directory** to the repository root, so that the core scripts\n",
    "   (`training.py`, `build_ontology_bundle.py`, `run_inference.py`) can be executed\n",
    "   directly from the notebook.\n",
    "3. **Installs Python dependencies** from `requirements.txt`, if the file is present.\n",
    "4. **Runs sanity checks** to ensure that the main framework scripts are available\n",
    "   in the expected locations.\n",
    "\n",
    "After this cell completes successfully, the notebook can safely execute the full\n",
    "pipeline: **training → offline bundle → inference**, in a single, unified workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP (clone repo + install deps)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/adsp-polito/2025-P13-Ontology-Alignment.git\"  # <-- cambia se serve\n",
    "REPO_DIR = Path(\"repo\").resolve()\n",
    "\n",
    "def sh(cmd: str):\n",
    "    print(\"\\n$\", cmd)\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "# 1) Clone (or reuse)\n",
    "if not REPO_DIR.exists():\n",
    "    sh(f\"git clone {REPO_URL} {REPO_DIR}\")\n",
    "else:\n",
    "    print(\"Repo already present at:\", REPO_DIR)\n",
    "\n",
    "# 2) Move into repo\n",
    "os.chdir(REPO_DIR)\n",
    "print(\"CWD:\", Path(\".\").resolve())\n",
    "\n",
    "# 3) Install requirements (best-effort)\n",
    "if Path(\"requirements.txt\").exists():\n",
    "    sh(\"pip -q install -r requirements.txt\")\n",
    "else:\n",
    "    print(\"No requirements.txt found. Skipping pip install.\")\n",
    "\n",
    "# 4) Sanity checks: scripts must exist\n",
    "for p in [\"training.py\", \"build_ontology_bundle.py\", \"run_inference.py\"]:\n",
    "    if not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"Missing {p} in repo root: {Path('.').resolve()}\")\n",
    "\n",
    "print(\"Setup OK: repo + scripts found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae35da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166d008",
   "metadata": {},
   "source": [
    "## Configuration (always run this cell)\n",
    "\n",
    "This cell defines the **canonical configuration** of the pipeline.\n",
    "\n",
    "**This cell must always be executed**, regardless of which stages you plan to run\n",
    "(training, offline preprocessing, inference, or any combination of them).\n",
    "\n",
    "### What this cell does\n",
    "\n",
    "- Defines a **unique run directory** under `outputs/` (`RUN_ID`)\n",
    "- Sets **default paths**, **model choices**, and **hyperparameters**\n",
    "- Establishes the **canonical variable names** used by all subsequent cells\n",
    "\n",
    "All downstream cells (training, offline preprocessing, inference, restore) **depend on these variables being defined first**.\n",
    "\n",
    "### Important design principle: defaults + overrides\n",
    "\n",
    "This configuration cell provides **sane defaults** assuming a **full pipeline run**:\n",
    "\n",
    "- dataset construction + training\n",
    "- offline preprocessing\n",
    "- inference on the test split\n",
    "\n",
    "However, **not all variables defined here are always used as-is**.\n",
    "\n",
    "Depending on the selected run mode, some variables will be **intentionally overridden** later:\n",
    "\n",
    "- If you **restore a trained model**,  \n",
    "  `CROSS_ENCODER_MODEL_ID` will be overwritten in the *Restore Model* cell.\n",
    "\n",
    "- If you **restore offline artifacts**,  \n",
    "  `OFFLINE_BUNDLE_PKL` and `ONTOLOGY_INTERNAL_CSV` will be overwritten in the *Restore Offline Artifacts* cell.\n",
    "\n",
    "- If you **run inference on a custom query file**,  \n",
    "  `INFER_INPUT_CSV`, `RETRIEVAL_COL`, `SCORING_COL`, and `ID_COL` may be overridden in the *Inference Restore / Override* cell.\n",
    "\n",
    "This is **by design**:\n",
    "- defaults live here,\n",
    "- overrides live **only** in the cells that also load the corresponding artifacts,\n",
    "- no hidden or implicit state changes.\n",
    "\n",
    "### What you should (and should not) change here\n",
    "\n",
    "**You should edit this cell to**:\n",
    "- change ontology paths,\n",
    "- change model names,\n",
    "- tune hyperparameters,\n",
    "- adjust retrieval/scoring defaults,\n",
    "- control output layout.\n",
    "\n",
    "**You should NOT edit this cell to**:\n",
    "- point to restored models or offline bundles,\n",
    "- change inference inputs for a specific run.\n",
    "\n",
    "Those actions belong to the dedicated *restore / override* cells below.\n",
    "\n",
    "### Mental model\n",
    "\n",
    "Think of this cell as:\n",
    "\n",
    "> “The baseline configuration of the experiment.”\n",
    "\n",
    "All other cells either:\n",
    "- **use it as-is**, or\n",
    "- **explicitly override parts of it**, in a visible and controlled way.\n",
    "\n",
    "If something goes wrong later, this cell is always the first place to look.\n",
    "\n",
    "---\n",
    "\n",
    "When this cell finishes executing, the notebook is in a **well-defined initial state**, ready for:\n",
    "- full execution,\n",
    "- partial execution,\n",
    "- or artifact restoration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c133caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION (unified training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "\n",
    "# -----------------------------\n",
    "# Run id / output layout\n",
    "# -----------------------------\n",
    "RUN_ID = f\"unified_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUT_DIR = Path(\"outputs\") / RUN_ID\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_DIR = OUT_DIR / \"training\"\n",
    "OFFLINE_DIR = OUT_DIR / \"offline\"\n",
    "INFER_DIR = OUT_DIR / \"inference\"\n",
    "TRAIN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OFFLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INFER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RUN_ID:\", RUN_ID)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Training mode and model\n",
    "# -----------------------------\n",
    "RUN_MODE = \"full\"  # \"full\" | \"build-dataset\" | \"train-only\"\n",
    "MODEL_TYPE = \"cross-encoder\"  # keep this if you want inference at the end\n",
    "MODEL_NAME = \"allenai/scibert_scivocab_uncased\"  # This is the model used for both cross-encoder and bi-encoder. The bi-encoder is only used for offline retrieval.\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "HYPERPARAMETER_TUNING = False\n",
    "N_TRIALS = 5\n",
    "\n",
    "USE_FIXED_HYPERPARAMS = True\n",
    "LEARNING_RATE = 3e-5\n",
    "BATCH_SIZE = 16\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "SPLIT_RATIOS = \"0.75,0.15,0.10\"\n",
    "\n",
    "# -----------------------------\n",
    "# Inputs for dataset building\n",
    "# -----------------------------\n",
    "SRC_PATH = \"data/sweet.owl\"\n",
    "TGT_PATH = \"data/envo.owl\"\n",
    "ALIGN_PATH = \"data/envo-sweet.rdf\"\n",
    "\n",
    "SRC_PREFIX = None\n",
    "TGT_PREFIX = None  # e.g. \"http://purl.obolibrary.org/obo/ENVO_\"\n",
    "\n",
    "USE_DESCRIPTION = True\n",
    "USE_SYNONYMS = True\n",
    "USE_PARENTS = True\n",
    "USE_EQUIVALENT = True\n",
    "USE_DISJOINT = True\n",
    "\n",
    "VISUALIZE = False\n",
    "\n",
    "# -----------------------------\n",
    "# Canonical outputs of STEP 1\n",
    "# -----------------------------\n",
    "OUT_SRC_CSV = str(TRAIN_DIR / \"source_ontology.csv\")\n",
    "OUT_TGT_CSV = str(TRAIN_DIR / \"target_ontology.csv\")\n",
    "OUT_DATASET_CSV = str(TRAIN_DIR / \"training_dataset.csv\")\n",
    "\n",
    "TRAIN_SPLIT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".train.csv\"))\n",
    "VAL_SPLIT_CSV   = str(Path(OUT_DATASET_CSV).with_suffix(\".val.csv\"))\n",
    "TEST_SPLIT_CSV  = str(Path(OUT_DATASET_CSV).with_suffix(\".test.csv\"))\n",
    "\n",
    "# train-only mode\n",
    "DATASET_CSV = TRAIN_SPLIT_CSV\n",
    "\n",
    "# model outputs\n",
    "MODEL_OUT_DIR = str(TRAIN_DIR / \"models\" / f\"{MODEL_TYPE}_custom\")\n",
    "FINAL_CROSS_ENCODER_DIR = str(Path(MODEL_OUT_DIR) / \"final_cross_encoder_model\")\n",
    "\n",
    "# -----------------------------\n",
    "# Offline bundle builder\n",
    "# -----------------------------\n",
    "OFFLINE_EXPORT_CSV = None\n",
    "OFFLINE_ONT_PATH = TGT_PATH\n",
    "OFFLINE_PREFIX = TGT_PREFIX\n",
    "\n",
    "CROSS_TOKENIZER_NAME = MODEL_NAME\n",
    "\n",
    "BI_ENCODER_MODEL_ID = \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\" # or \"allenai/scibert_scivocab_uncased\"\n",
    "OFFLINE_SEMANTIC_BATCH_SIZE = 64\n",
    "OFFLINE_SEMANTIC_MAX_LENGTH = 256\n",
    "OFFLINE_NO_SEMANTIC_NORMALIZE = False\n",
    "\n",
    "ONTOLOGY_INTERNAL_CSV = str(OFFLINE_DIR / \"ontology_internal.csv\")\n",
    "OFFLINE_BUNDLE_PKL = str(OFFLINE_DIR / \"offline_bundle.pkl\")\n",
    "\n",
    "# -----------------------------\n",
    "# Inference\n",
    "# -----------------------------\n",
    "CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "\n",
    "INFER_INPUT_CSV = str(Path(OUT_DATASET_CSV).with_suffix(\".test.queries.csv\"))\n",
    "INFER_OUT_CSV = str(INFER_DIR / \"predictions.csv\")\n",
    "\n",
    "RETRIEVAL_COL = \"source_label\"\n",
    "SCORING_COL = \"source_text\"\n",
    "ID_COL = \"source_iri\"\n",
    "\n",
    "INFER_MODE = \"hybrid\"\n",
    "RETRIEVAL_LEXICAL_TOP_K = 100\n",
    "RETRIEVAL_SEMANTIC_TOP_K = 100\n",
    "RETRIEVAL_MERGED_TOP_K = 150\n",
    "HYBRID_RATIO_SEMANTIC = 0.5\n",
    "SEMANTIC_BATCH_SIZE = 64\n",
    "\n",
    "CROSS_TOP_K = 20\n",
    "CROSS_BATCH_SIZE = 32\n",
    "CROSS_MAX_LENGTH = 256\n",
    "\n",
    "KEEP_TOP_N = 0\n",
    "\n",
    "print(\"Config OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda77322",
   "metadata": {},
   "source": [
    "## Run Mode Flags (what to execute in this session)\n",
    "\n",
    "This cell defines **which stages of the pipeline will be executed in the current session**  \n",
    "and whether previously generated artifacts should be **restored instead of rebuilt**.\n",
    "\n",
    "Think of it as the **execution control panel** of the notebook.\n",
    "\n",
    "### Main execution toggles\n",
    "\n",
    "These flags control **which pipeline stages are executed**:\n",
    "\n",
    "- **`DO_TRAINING`**  \n",
    "  Build the dataset (if needed) and train the model.\n",
    "\n",
    "- **`DO_OFFLINE`**  \n",
    "  Build the offline artifacts used for retrieval  \n",
    "  (ontology internal CSV + offline bundle).\n",
    "\n",
    "- **`DO_INFERENCE`**  \n",
    "  Run inference on a query CSV using:\n",
    "  - a trained or restored cross-encoder\n",
    "  - offline retrieval artifacts\n",
    "\n",
    "You can run:\n",
    "- the **full pipeline** (all `True`),\n",
    "- **only a subset** of stages,\n",
    "- or **only inference** on restored artifacts.\n",
    "\n",
    "### Restore toggles (skip building, load artifacts)\n",
    "\n",
    "These flags allow you to **reuse artifacts from a previous run** (or another machine)\n",
    "instead of rebuilding them.\n",
    "\n",
    "- **`RESTORE_MODEL`**  \n",
    "  Skip training and **load a previously trained cross-encoder**  \n",
    "  (and optionally override inference input/schema).\n",
    "\n",
    "- **`RESTORE_OFFLINE`**  \n",
    "  Skip offline preprocessing and **load an existing offline bundle**\n",
    "  (ontology CSV + semantic index).\n",
    "\n",
    "Each restore cell:\n",
    "- loads the required files,\n",
    "- **overwrites the corresponding configuration variables**, and\n",
    "- makes the notebook ready for inference without rerunning earlier stages.\n",
    "\n",
    "### Guardrails and coherence checks\n",
    "\n",
    "The checks below enforce **logical consistency**:\n",
    "\n",
    "- Inference **always requires offline artifacts**  \n",
    "  → either build them (`DO_OFFLINE=True`) or restore them (`RESTORE_OFFLINE=True`).\n",
    "\n",
    "- Inference **requires a model**  \n",
    "  → either train it (`DO_TRAINING=True`) or restore it (`RESTORE_MODEL=True`).\n",
    "\n",
    "Warnings are printed when:\n",
    "- you enable both *build* and *restore* for the same stage  \n",
    "  (the **last executed cell wins**, by design)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN MODE FLAGS (choose what to run today)\n",
    "# ============================================\n",
    "\n",
    "# Main toggles: what stages to execute\n",
    "DO_TRAINING  = True\n",
    "DO_OFFLINE   = True\n",
    "DO_INFERENCE = True\n",
    "\n",
    "# Restore toggles: if True, skip building that stage and load artifacts instead\n",
    "RESTORE_MODEL   = False   # restores cross-encoder + (optionally) custom inference input CSV/schema\n",
    "RESTORE_OFFLINE = False   # restores offline bundle + ontology CSV\n",
    "\n",
    "# -----------------------------\n",
    "# Guardrails (keep logic coherent)\n",
    "# -----------------------------\n",
    "if DO_INFERENCE and not (DO_OFFLINE or RESTORE_OFFLINE):\n",
    "    print(\n",
    "        \"Note: DO_INFERENCE=True without DO_OFFLINE/RESTORE_OFFLINE.\\n\"\n",
    "        \"Assuming offline artifacts already exist on disk \"\n",
    "        \"(from a previous run or manual configuration).\"\n",
    "    )\n",
    "\n",
    "if DO_INFERENCE and not (DO_TRAINING or RESTORE_MODEL):\n",
    "    print(\n",
    "        \"Note: inference requires CROSS_ENCODER_MODEL_ID to be set \"\n",
    "        \"(either via training or restore).\"\n",
    "    )\n",
    "\n",
    "# Coherence hints\n",
    "if DO_TRAINING and RESTORE_MODEL:\n",
    "    print(\"Note: DO_TRAINING=True but RESTORE_MODEL=True. Training will run; restore can overwrite the model id if executed after training.\")\n",
    "if DO_OFFLINE and RESTORE_OFFLINE:\n",
    "    print(\"Note: DO_OFFLINE=True but RESTORE_OFFLINE=True. Offline will run; restore can overwrite the offline paths if executed after offline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fbdbcb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e260e31",
   "metadata": {},
   "source": [
    "### Download helpers (config.txt + ZIP packaging)\n",
    "\n",
    "This cell defines **utility functions** used by all download steps in the notebook.\n",
    "\n",
    "What these helpers do:\n",
    "\n",
    "- **`config.txt` generation**\n",
    "  - Captures the *effective configuration of the run* (after Configuration, Restore, and Overrides)\n",
    "  - Includes model choices, training settings, offline bundle parameters, and inference knobs\n",
    "  - Ensures full reproducibility of results outside Colab\n",
    "\n",
    "- **ZIP creation**\n",
    "  - Packages selected output artifacts (e.g. predictions, gold, bundles)\n",
    "  - Automatically includes the generated `config.txt`\n",
    "  - Produces a single, shareable archive for analysis or collaboration\n",
    "\n",
    "Design notes:\n",
    "\n",
    "- Helpers are defined **once** and reused by all download cells\n",
    "- `config.txt` always reflects the **final state of global variables**\n",
    "- No computation is triggered here — this cell only defines functions\n",
    "\n",
    "This keeps the notebook clean, modular, and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD HELPERS (config.txt + zip)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "def write_config_txt(config_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Creates a config.txt in config_dir describing the current run configuration.\n",
    "    Returns the path to the created file.\n",
    "    \"\"\"\n",
    "    config_dir = Path(config_dir)\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_lines = [\n",
    "        \"# Ontology Alignment – Run Configuration\",\n",
    "        f\"# Generated on: {datetime.now().isoformat()}\",\n",
    "        \"\",\n",
    "        \"[Run]\",\n",
    "        f\"RUN_ID = {globals().get('RUN_ID', None)}\",\n",
    "        f\"OUT_DIR = {globals().get('OUT_DIR', None)}\",\n",
    "        f\"RUN_MODE = {globals().get('RUN_MODE', None)}\",\n",
    "        \"\",\n",
    "        \"[Model]\",\n",
    "        f\"MODEL_TYPE = {globals().get('MODEL_TYPE', None)}\",\n",
    "        f\"MODEL_NAME = {globals().get('MODEL_NAME', None)}\",\n",
    "        f\"CROSS_ENCODER_MODEL_ID = {globals().get('CROSS_ENCODER_MODEL_ID', None)}\",\n",
    "        f\"BI_ENCODER_MODEL_ID = {globals().get('BI_ENCODER_MODEL_ID', None)}\",\n",
    "        f\"CROSS_TOKENIZER_NAME = {globals().get('CROSS_TOKENIZER_NAME', None)}\",\n",
    "        \"\",\n",
    "        \"[Training]\",\n",
    "        f\"NUM_EPOCHS = {globals().get('NUM_EPOCHS', None)}\",\n",
    "        f\"LEARNING_RATE = {globals().get('LEARNING_RATE', None)}\",\n",
    "        f\"BATCH_SIZE = {globals().get('BATCH_SIZE', None)}\",\n",
    "        f\"WEIGHT_DECAY = {globals().get('WEIGHT_DECAY', None)}\",\n",
    "        f\"SPLIT_RATIOS = {globals().get('SPLIT_RATIOS', None)}\",\n",
    "        \"\",\n",
    "        \"[Offline]\",\n",
    "        f\"OFFLINE_ONT_PATH = {globals().get('OFFLINE_ONT_PATH', None)}\",\n",
    "        f\"OFFLINE_PREFIX = {globals().get('OFFLINE_PREFIX', None)}\",\n",
    "        f\"OFFLINE_SEMANTIC_BATCH_SIZE = {globals().get('OFFLINE_SEMANTIC_BATCH_SIZE', None)}\",\n",
    "        f\"OFFLINE_SEMANTIC_MAX_LENGTH = {globals().get('OFFLINE_SEMANTIC_MAX_LENGTH', None)}\",\n",
    "        f\"OFFLINE_BUNDLE_PKL = {globals().get('OFFLINE_BUNDLE_PKL', None)}\",\n",
    "        f\"ONTOLOGY_INTERNAL_CSV = {globals().get('ONTOLOGY_INTERNAL_CSV', None)}\",\n",
    "        \"\",\n",
    "        \"[Inference]\",\n",
    "        f\"INFER_MODE = {globals().get('INFER_MODE', None)}\",\n",
    "        f\"INFER_INPUT_CSV = {globals().get('INFER_INPUT_CSV', None)}\",\n",
    "        f\"INFER_OUT_CSV = {globals().get('INFER_OUT_CSV', None)}\",\n",
    "        f\"RETRIEVAL_COL = {globals().get('RETRIEVAL_COL', None)}\",\n",
    "        f\"SCORING_COL = {globals().get('SCORING_COL', None)}\",\n",
    "        f\"ID_COL = {globals().get('ID_COL', None)}\",\n",
    "        f\"RETRIEVAL_LEXICAL_TOP_K = {globals().get('RETRIEVAL_LEXICAL_TOP_K', None)}\",\n",
    "        f\"RETRIEVAL_SEMANTIC_TOP_K = {globals().get('RETRIEVAL_SEMANTIC_TOP_K', None)}\",\n",
    "        f\"RETRIEVAL_MERGED_TOP_K = {globals().get('RETRIEVAL_MERGED_TOP_K', None)}\",\n",
    "        f\"HYBRID_RATIO_SEMANTIC = {globals().get('HYBRID_RATIO_SEMANTIC', None)}\",\n",
    "        f\"CROSS_TOP_K = {globals().get('CROSS_TOP_K', None)}\",\n",
    "        f\"CROSS_BATCH_SIZE = {globals().get('CROSS_BATCH_SIZE', None)}\",\n",
    "        f\"CROSS_MAX_LENGTH = {globals().get('CROSS_MAX_LENGTH', None)}\",\n",
    "        f\"KEEP_TOP_N = {globals().get('KEEP_TOP_N', None)}\",\n",
    "    ]\n",
    "\n",
    "    config_path = config_dir / \"config.txt\"\n",
    "    config_path.write_text(\"\\n\".join(config_lines))\n",
    "    return config_path\n",
    "\n",
    "\n",
    "def make_zip_with_config(\n",
    "    zip_path: Path,\n",
    "    files_to_include: list[Path],\n",
    "    config_dir: Path | None = None,\n",
    "    download: bool = True,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Creates a zip that includes:\n",
    "      - all files in files_to_include that exist\n",
    "      - a config.txt written into config_dir (defaults to zip_path.parent)\n",
    "    Returns the zip path.\n",
    "    \"\"\"\n",
    "    zip_path = Path(zip_path)\n",
    "    zip_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if config_dir is None:\n",
    "        config_dir = zip_path.parent\n",
    "    config_path = write_config_txt(Path(config_dir))\n",
    "\n",
    "    existing_files = []\n",
    "    for p in files_to_include:\n",
    "        p = Path(p)\n",
    "        if p.exists():\n",
    "            existing_files.append(p)\n",
    "\n",
    "    if not existing_files:\n",
    "        raise FileNotFoundError(\"None of the requested files exist. Nothing to zip.\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        # artifacts\n",
    "        for p in existing_files:\n",
    "            z.write(p, arcname=p.name)\n",
    "        # config\n",
    "        z.write(config_path, arcname=\"config.txt\")\n",
    "\n",
    "    print(\"Created ZIP:\", zip_path)\n",
    "    print(\"Included files:\", [p.name for p in existing_files], \"+ config.txt\")\n",
    "\n",
    "    if download:\n",
    "        try:\n",
    "            from google.colab import files as colab_files\n",
    "        except ImportError as e:\n",
    "            raise RuntimeError(\"files.download() is Colab-only.\") from e\n",
    "        colab_files.download(str(zip_path))\n",
    "\n",
    "    return zip_path\n",
    "\n",
    "\n",
    "print(\"Download helpers OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a4c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac28b2",
   "metadata": {},
   "source": [
    "## 1) Run full pipeline (Dataset Construction & Training -> Offline Preprocessing -> Inference)\n",
    "\n",
    "This section executes:\n",
    "1) `training.py`\n",
    "2) `build_ontology_bundle.py`\n",
    "3) `run_inference.py`\n",
    "\n",
    "Logs are written under the run directory.\n",
    "The notebook stops immediately if any step fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ea85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN PIPELINE (training -> offline -> inference)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "def print_tail(path: Path, n=120):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"[tail] log not found: {p}\")\n",
    "        return\n",
    "    lines = p.read_text(errors=\"replace\").splitlines()\n",
    "    print(\"\\n\".join(lines[-n:]))\n",
    "\n",
    "def run_cmd(cmd, log_path: Path, cwd: Path):\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"CWD:\", cwd)\n",
    "    print(\"Log:\", log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        proc = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, cwd=str(cwd))\n",
    "\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"!!! Error occurred. Last lines of log:\")\n",
    "        print_tail(log_path, n=120)\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}. See log: {log_path}\")\n",
    "    return proc.returncode\n",
    "\n",
    "# Guardrails\n",
    "if RUN_MODE == \"full\" and MODEL_TYPE != \"cross-encoder\":\n",
    "    raise ValueError(\"RUN_MODE='full' ends with inference => needs MODEL_TYPE='cross-encoder'.\")\n",
    "\n",
    "if HYPERPARAMETER_TUNING and RUN_MODE != \"full\":\n",
    "    raise ValueError(\"--tune only allowed in RUN_MODE='full'.\")\n",
    "\n",
    "Path(MODEL_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1) TRAINING\n",
    "# -----------------------------\n",
    "train_log = TRAIN_DIR / \"training.log\"\n",
    "\n",
    "train_cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "    train_cmd += [\"--src\", SRC_PATH, \"--tgt\", TGT_PATH, \"--align\", ALIGN_PATH]\n",
    "    train_cmd += [\"--out-src\", OUT_SRC_CSV, \"--out-tgt\", OUT_TGT_CSV, \"--out-dataset\", OUT_DATASET_CSV]\n",
    "    train_cmd += [\"--split-ratios\", SPLIT_RATIOS]\n",
    "\n",
    "    if SRC_PREFIX:\n",
    "        train_cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "    if TGT_PREFIX:\n",
    "        train_cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "    if USE_DESCRIPTION: train_cmd.append(\"--src-use-description\")\n",
    "    if USE_SYNONYMS: train_cmd.append(\"--src-use-synonyms\")\n",
    "    if USE_PARENTS: train_cmd.append(\"--src-use-parents\")\n",
    "    if USE_EQUIVALENT: train_cmd.append(\"--src-use-equivalent\")\n",
    "    if USE_DISJOINT: train_cmd.append(\"--src-use-disjoint\")\n",
    "    if VISUALIZE: train_cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "    train_cmd += [\"--model-type\", MODEL_TYPE, \"--model-name\", MODEL_NAME, \"--model-output-dir\", MODEL_OUT_DIR]\n",
    "    train_cmd += [\"--num-epochs\", str(NUM_EPOCHS)]\n",
    "\n",
    "    if HYPERPARAMETER_TUNING:\n",
    "        train_cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "    elif USE_FIXED_HYPERPARAMS:\n",
    "        train_cmd += [\"--learning-rate\", str(LEARNING_RATE)]\n",
    "        train_cmd += [\"--batch-size\", str(BATCH_SIZE)]\n",
    "        train_cmd += [\"--weight-decay\", str(WEIGHT_DECAY)]\n",
    "\n",
    "if RUN_MODE == \"train-only\":\n",
    "    train_cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "run_cmd(train_cmd, train_log, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nTraining completed.\")\n",
    "print(\"Dataset CSV:\", OUT_DATASET_CSV)\n",
    "print(\"Train split:\", TRAIN_SPLIT_CSV)\n",
    "print(\"Val split:\", VAL_SPLIT_CSV)\n",
    "print(\"Test split:\", TEST_SPLIT_CSV)\n",
    "print(\"Cross-encoder dir:\", FINAL_CROSS_ENCODER_DIR)\n",
    "\n",
    "CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2) OFFLINE BUNDLE\n",
    "# -----------------------------\n",
    "offline_log = OFFLINE_DIR / \"offline_bundle.log\"\n",
    "\n",
    "offline_cmd = [\n",
    "    \"python\", \"build_ontology_bundle.py\",\n",
    "    \"--out-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "    \"--out-bundle\", OFFLINE_BUNDLE_PKL,\n",
    "    \"--tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--bi-encoder-model-id\", BI_ENCODER_MODEL_ID,\n",
    "    \"--semantic-batch-size\", str(OFFLINE_SEMANTIC_BATCH_SIZE),\n",
    "    \"--semantic-max-length\", str(OFFLINE_SEMANTIC_MAX_LENGTH),\n",
    "]\n",
    "if OFFLINE_NO_SEMANTIC_NORMALIZE:\n",
    "    offline_cmd.append(\"--no-semantic-normalize\")\n",
    "\n",
    "if OFFLINE_EXPORT_CSV:\n",
    "    offline_cmd += [\"--export-csv\", OFFLINE_EXPORT_CSV]\n",
    "else:\n",
    "    offline_cmd += [\"--ont-path\", OFFLINE_ONT_PATH]\n",
    "    if OFFLINE_PREFIX:\n",
    "        offline_cmd += [\"--prefix\", OFFLINE_PREFIX]\n",
    "\n",
    "run_cmd(offline_cmd, offline_log, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nOffline bundle completed.\")\n",
    "print(\"Ontology internal CSV:\", ONTOLOGY_INTERNAL_CSV)\n",
    "print(\"Offline bundle PKL:\", OFFLINE_BUNDLE_PKL)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3) INFERENCE\n",
    "# -----------------------------\n",
    "infer_log = INFER_DIR / \"inference.log\"\n",
    "\n",
    "if not Path(INFER_INPUT_CSV).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"INFER_INPUT_CSV not found: {INFER_INPUT_CSV}\\n\"\n",
    "        \"In full/build-dataset mode, training should generate *.test.queries.csv. \"\n",
    "        \"If you want a custom query file, set INFER_INPUT_CSV to its path.\"\n",
    "    )\n",
    "\n",
    "infer_cmd = [\n",
    "    \"python\", \"run_inference.py\",\n",
    "    \"--bundle\", OFFLINE_BUNDLE_PKL,\n",
    "    \"--ontology-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "    \"--input-csv\", INFER_INPUT_CSV,\n",
    "    \"--out-csv\", INFER_OUT_CSV,\n",
    "    \"--mode\", INFER_MODE,\n",
    "    \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "    \"--retrieval-col\", RETRIEVAL_COL,\n",
    "    \"--retrieval-lexical-top-k\", str(RETRIEVAL_LEXICAL_TOP_K),\n",
    "    \"--retrieval-semantic-top-k\", str(RETRIEVAL_SEMANTIC_TOP_K),\n",
    "    \"--retrieval-merged-top-k\", str(RETRIEVAL_MERGED_TOP_K),\n",
    "    \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEMANTIC),\n",
    "    \"--semantic-batch-size\", str(SEMANTIC_BATCH_SIZE),\n",
    "    \"--cross-top-k\", str(CROSS_TOP_K),\n",
    "    \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "    \"--cross-max-length\", str(CROSS_MAX_LENGTH),\n",
    "    \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "]\n",
    "if SCORING_COL:\n",
    "    infer_cmd += [\"--scoring-col\", SCORING_COL]\n",
    "if ID_COL:\n",
    "    infer_cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "run_cmd(infer_cmd, infer_log, cwd=REPO_ROOT)\n",
    "\n",
    "print(\"\\nUnified pipeline completed successfully.\")\n",
    "print(\"Outputs:\")\n",
    "print(\" - Training:\", TRAIN_DIR)\n",
    "print(\" - Offline bundle:\", OFFLINE_DIR)\n",
    "print(\" - Inference:\", INFER_DIR)\n",
    "print(\"Predictions CSV:\", INFER_OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c39a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a10bc",
   "metadata": {},
   "source": [
    "### 1.1) Download inference outputs, gold standard, and configuration\n",
    "\n",
    "This cell is a **utility step for result extraction**.  \n",
    "It is designed to be used **after a full pipeline run**, where both predictions and gold labels are available.\n",
    "\n",
    "What the cell does:\n",
    "\n",
    "- Locates the inference output file:\n",
    "  - `predictions.csv` generated by `run_inference.py`\n",
    "- Locates the corresponding gold standard file:\n",
    "  - `*.test.gold.csv` generated during dataset construction in `training.py`\n",
    "- Verifies that both files exist and stops with a clear error if any is missing\n",
    "- Creates a single ZIP archive (`predictions_and_gold.zip`) that contains:\n",
    "  - `predictions.csv`\n",
    "  - `*.test.gold.csv`\n",
    "  - `config.txt`, a snapshot of the **effective configuration** used for the run (models, parameters, paths)\n",
    "\n",
    "Why this is useful:\n",
    "\n",
    "- Enables quick local evaluation and debugging without rerunning training or offline preprocessing\n",
    "- Makes it easy to share results with collaborators in a **self-contained and reproducible bundle**\n",
    "- Ensures that predictions and ground truth are always paired with the exact configuration that produced them\n",
    "\n",
    "This cell does **not** modify any artifacts and does **not** trigger computation.  \n",
    "It only packages files that were already produced by previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD ARTIFACTS (predictions + gold + config.txt)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Expected paths from configuration ---\n",
    "pred_path = Path(INFER_OUT_CSV)  # outputs/.../inference/predictions.csv\n",
    "gold_path = Path(str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\")))\n",
    "# outputs/.../training_dataset.test.gold.csv\n",
    "\n",
    "# --- Sanity checks ---\n",
    "missing = [str(p) for p in [pred_path, gold_path] if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"Missing required file(s):\\n\"\n",
    "        + \"\\n\".join(f\" - {m}\" for m in missing)\n",
    "        + \"\\n\\nNotes:\\n\"\n",
    "        \" - predictions.csv is produced by the inference stage.\\n\"\n",
    "        \" - *.test.gold.csv is produced in full/build-dataset training runs.\\n\"\n",
    "    )\n",
    "\n",
    "print(\"Found predictions:\", pred_path)\n",
    "print(\"Found gold:\", gold_path)\n",
    "\n",
    "# --- Create ZIP (artifacts + config.txt) ---\n",
    "zip_path = pred_path.parent / \"predictions_and_gold.zip\"\n",
    "\n",
    "make_zip_with_config(\n",
    "    zip_path=zip_path,\n",
    "    files_to_include=[pred_path, gold_path],\n",
    "    config_dir=pred_path.parent,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db09ffc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876b616",
   "metadata": {},
   "source": [
    "## 2) Stage-by-stage execution (Training → Offline Bundle → Inference)\n",
    "\n",
    "This notebook supports two ways of running the pipeline:\n",
    "\n",
    "1) **Full pipeline (one-click)**: runs Training, then Offline Bundle building, then Inference.\n",
    "2) **Stage-by-stage (3 separate cells)**: runs the same steps, but split into independent stages so you can re-run only what you need.\n",
    "\n",
    "The pipeline has a natural dependency chain:\n",
    "\n",
    "Each stage can either:\n",
    "- **produce artifacts**, or\n",
    "- **consume restored artifacts**, depending on the run flags.\n",
    "\n",
    "### Stage 1 — Training / Dataset Construction\n",
    "\n",
    "**Purpose**\n",
    "- Build the training dataset and splits.\n",
    "- Optionally train a cross-encoder model.\n",
    "\n",
    "**Executed when**\n",
    "- `DO_TRAINING = True`\n",
    "\n",
    "**What it does**\n",
    "- If `RUN_MODE` is `full` or `build-dataset`:\n",
    "  - builds `training_dataset.csv`\n",
    "  - generates `*.train.csv`, `*.val.csv`, `*.test.csv`\n",
    "  - generates `*.test.queries.csv` (default inference input)\n",
    "- If `RUN_MODE` is `full` or `train-only`:\n",
    "  - trains a cross-encoder model\n",
    "\n",
    "**Artifacts produced**\n",
    "- `training_dataset.csv`\n",
    "- `training_dataset.{train,val,test}.csv`\n",
    "- `training_dataset.test.queries.csv`\n",
    "- `final_cross_encoder_model/`\n",
    "\n",
    "**Alternative**\n",
    "- If `RESTORE_MODEL = True`, this stage can be skipped and the model loaded from disk instead.\n",
    "\n",
    "### Stage 2 — Offline Bundle Builder\n",
    "\n",
    "**Purpose**\n",
    "- Build the retrieval infrastructure for the target ontology:\n",
    "  - lexical index\n",
    "  - optional semantic embeddings\n",
    "\n",
    "**Executed when**\n",
    "- `DO_OFFLINE = True`\n",
    "\n",
    "**What it does**\n",
    "- Processes the target ontology\n",
    "- Builds the offline retrieval bundle\n",
    "\n",
    "**Artifacts produced**\n",
    "- `ontology_internal.csv`\n",
    "- `offline_bundle.pkl`\n",
    "\n",
    "**Notes**\n",
    "- This stage is independent of training once the ontology is fixed.\n",
    "- It is typically run once per ontology configuration.\n",
    "\n",
    "**Alternative**\n",
    "- If `RESTORE_OFFLINE = True`, this stage can be skipped and the offline artifacts loaded instead.\n",
    "\n",
    "### Stage 3 — Inference\n",
    "\n",
    "**Purpose**\n",
    "- Run retrieval + scoring for each query in `INFER_INPUT_CSV`.\n",
    "\n",
    "**Executed when**\n",
    "- `DO_INFERENCE = True`\n",
    "\n",
    "**What it does**\n",
    "- Loads:\n",
    "  - a trained or restored cross-encoder\n",
    "  - a built or restored offline bundle\n",
    "- Runs:\n",
    "  - lexical or hybrid retrieval\n",
    "  - cross-encoder scoring\n",
    "- Produces predictions\n",
    "\n",
    "**Artifacts produced**\n",
    "- `predictions.csv`\n",
    "- optional top-N candidate columns (if enabled)\n",
    "\n",
    "**This is the stage you re-run most often**, because it’s where you typically change:\n",
    "- `INFER_MODE` (`lexical` vs `hybrid`)\n",
    "- retrieval top-k values and hybrid ratio\n",
    "- scoring batch size / max length\n",
    "- input CSV and column schema\n",
    "\n",
    "---\n",
    "\n",
    "### Practical workflows\n",
    "\n",
    "**Full pipeline (from scratch)**\n",
    "- Enable all stages\n",
    "- No restore flags\n",
    "- One run produces all artifacts\n",
    "\n",
    "**Inference-only iteration**\n",
    "- Disable Training and Offline\n",
    "- Enable Inference\n",
    "- Restore model + offline artifacts\n",
    "- Change inference parameters freely\n",
    "\n",
    "**Offline preprocessing once, inference many times**\n",
    "- Run Offline once\n",
    "- Zip artifacts\n",
    "- Restore them in future sessions\n",
    "\n",
    "---\n",
    "\n",
    "### Key design principle\n",
    "\n",
    "- The **Configuration cell** defines defaults.\n",
    "- The **Run Mode Flags** decide *what to execute*.\n",
    "- The **Restore cells** decide *where artifacts come from*.\n",
    "- The **Inference stage** always consumes the *currently active* artifacts, whether built or restored.\n",
    "\n",
    "This design keeps the notebook:\n",
    "- reproducible,\n",
    "- modular,\n",
    "- and efficient for Colab-based experimentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fb8b10",
   "metadata": {},
   "source": [
    "## Helpers and execution environment\n",
    "\n",
    "This cell defines small helper utilities used by all execution stages:\n",
    "\n",
    "- `run_cmd(...)`  \n",
    "  A thin wrapper around `subprocess.run` that:\n",
    "  - executes CLI scripts (`training.py`, `build_ontology_bundle.py`, `run_inference.py`)\n",
    "  - redirects stdout/stderr to a log file\n",
    "  - stops the notebook immediately if a command fails\n",
    "\n",
    "- `print_tail(...)`  \n",
    "  Convenience helper to print the last lines of a log file when an error occurs,\n",
    "  making debugging easier in Colab sessions.\n",
    "\n",
    "In addition, this cell disables **Weights & Biases (wandb)** by default:\n",
    "- avoids interactive login prompts in Colab\n",
    "- keeps runs fully offline and reproducible\n",
    "\n",
    "> This cell should be run **once** before executing any pipeline stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e46f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# HELPERS (run_cmd + logs) + ENV\n",
    "# ============================================\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def print_tail(path: Path, n=120):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"[tail] log not found: {p}\")\n",
    "        return\n",
    "    lines = p.read_text(errors=\"replace\").splitlines()\n",
    "    print(\"\\n\".join(lines[-n:]))\n",
    "\n",
    "def run_cmd(cmd, log_path: Path, cwd: Path):\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"CWD:\", cwd)\n",
    "    print(\"Log:\", log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        proc = subprocess.run(cmd, stdout=f, stderr=subprocess.STDOUT, cwd=str(cwd))\n",
    "\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"!!! Error occurred. Last lines of log:\")\n",
    "        print_tail(log_path, n=120)\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}. See log: {log_path}\")\n",
    "    return proc.returncode\n",
    "\n",
    "# Disable wandb in Colab unless you explicitly want it\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "print(\"Helpers OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fdd175",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ee8c8",
   "metadata": {},
   "source": [
    "## Stage 1 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN TRAINING STAGE\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def print_tail(path: Path, n=120):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"[tail] log not found: {p}\")\n",
    "        return\n",
    "    lines = p.read_text(errors=\"replace\").splitlines()\n",
    "    print(\"\\n\".join(lines[-n:]))\n",
    "\n",
    "def run_cmd(cmd, log_path: Path, cwd: Path):\n",
    "    print(\"\\nRunning command:\\n\", \" \".join(cmd))\n",
    "    print(\"CWD:\", cwd)\n",
    "    print(\"Log:\", log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Force-disable W&B for the subprocess (robust) ---\n",
    "    env = os.environ.copy()\n",
    "    env[\"WANDB_MODE\"] = \"disabled\"     # wandb will not try to log in / sync\n",
    "    env[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "    with open(log_path, \"w\") as f:\n",
    "        proc = subprocess.run(\n",
    "            cmd,\n",
    "            stdout=f,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            cwd=str(cwd),\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "    print(\"Return code:\", proc.returncode)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"!!! Error occurred. Last lines of log:\")\n",
    "        print_tail(log_path, n=120)\n",
    "        raise RuntimeError(f\"Command failed with return code {proc.returncode}. See log: {log_path}\")\n",
    "    return proc.returncode\n",
    "\n",
    "if not DO_TRAINING:\n",
    "    print(\"Skipping training (DO_TRAINING=False).\")\n",
    "else:\n",
    "    # Disable wandb in Colab unless you explicitly want it\n",
    "    os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "    os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "    Path(MODEL_OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_log = TRAIN_DIR / \"training.log\"\n",
    "    train_cmd = [\"python\", \"training.py\", \"--mode\", RUN_MODE]\n",
    "\n",
    "    if RUN_MODE in {\"full\", \"build-dataset\"}:\n",
    "        train_cmd += [\"--src\", SRC_PATH, \"--tgt\", TGT_PATH, \"--align\", ALIGN_PATH]\n",
    "        train_cmd += [\"--out-src\", OUT_SRC_CSV, \"--out-tgt\", OUT_TGT_CSV, \"--out-dataset\", OUT_DATASET_CSV]\n",
    "        train_cmd += [\"--split-ratios\", SPLIT_RATIOS]\n",
    "\n",
    "        if SRC_PREFIX:\n",
    "            train_cmd += [\"--src-prefix\", SRC_PREFIX]\n",
    "        if TGT_PREFIX:\n",
    "            train_cmd += [\"--tgt-prefix\", TGT_PREFIX]\n",
    "\n",
    "        if USE_DESCRIPTION: train_cmd.append(\"--src-use-description\")\n",
    "        if USE_SYNONYMS: train_cmd.append(\"--src-use-synonyms\")\n",
    "        if USE_PARENTS: train_cmd.append(\"--src-use-parents\")\n",
    "        if USE_EQUIVALENT: train_cmd.append(\"--src-use-equivalent\")\n",
    "        if USE_DISJOINT: train_cmd.append(\"--src-use-disjoint\")\n",
    "        if VISUALIZE: train_cmd.append(\"--visualize-alignments\")\n",
    "\n",
    "    if RUN_MODE in {\"full\", \"train-only\"}:\n",
    "        train_cmd += [\"--model-type\", MODEL_TYPE, \"--model-name\", MODEL_NAME, \"--model-output-dir\", MODEL_OUT_DIR]\n",
    "        train_cmd += [\"--num-epochs\", str(NUM_EPOCHS)]\n",
    "\n",
    "        if HYPERPARAMETER_TUNING:\n",
    "            train_cmd += [\"--tune\", \"--n-trials\", str(N_TRIALS)]\n",
    "        elif USE_FIXED_HYPERPARAMS:\n",
    "            train_cmd += [\"--learning-rate\", str(LEARNING_RATE)]\n",
    "            train_cmd += [\"--batch-size\", str(BATCH_SIZE)]\n",
    "            train_cmd += [\"--weight-decay\", str(WEIGHT_DECAY)]\n",
    "\n",
    "    if RUN_MODE == \"train-only\":\n",
    "        train_cmd += [\"--dataset-csv\", DATASET_CSV]\n",
    "\n",
    "    run_cmd(train_cmd, train_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nTraining completed.\")\n",
    "    print(\"Dataset CSV:\", OUT_DATASET_CSV)\n",
    "    print(\"Train split:\", TRAIN_SPLIT_CSV)\n",
    "    print(\"Val split:\", VAL_SPLIT_CSV)\n",
    "    print(\"Test split:\", TEST_SPLIT_CSV)\n",
    "    print(\"Model out dir:\", MODEL_OUT_DIR)\n",
    "\n",
    "    # Default scorer for inference (can be overridden by restore cell)\n",
    "    CROSS_ENCODER_MODEL_ID = FINAL_CROSS_ENCODER_DIR\n",
    "    print(\"CROSS_ENCODER_MODEL_ID set to:\", CROSS_ENCODER_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0dd32",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819280d",
   "metadata": {},
   "source": [
    "## Stage 2 - Offline Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550497a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3495267",
   "metadata": {},
   "source": [
    "### RESTORE OFFLINE INPUTS (Ontology or Export CSV)\n",
    "Use this only if you want to RUN offline preprocessing (DO_OFFLINE=True) but your ontology/export files are not already present in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f035a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "# --- IF guard ---\n",
    "if not DO_OFFLINE:\n",
    "    print(\"DO_OFFLINE=False -> skipping offline input restore cell.\")\n",
    "else:\n",
    "    # Choose what you want to restore:\n",
    "    #   - \"ontology\"  => upload OWL/RDF files and set OFFLINE_ONT_PATH\n",
    "    #   - \"export_csv\"=> upload an already-exported ontology_internal.csv and set OFFLINE_EXPORT_CSV\n",
    "    OFFLINE_INPUT_MODE = \"ontology\"  # \"ontology\" | \"export_csv\"\n",
    "\n",
    "    # Optional: if you prefer to point to an existing path instead of uploading\n",
    "    OFFLINE_INPUT_SRC = None  # e.g. \"/content/myfiles/sweet.owl\" OR None to upload\n",
    "\n",
    "    RESTORE_ROOT = Path(OUT_DIR) / \"restored\"\n",
    "    RESTORED_OFFLINE_INPUT_DIR = RESTORE_ROOT / \"offline_inputs\"\n",
    "    RESTORED_OFFLINE_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _extract_zip_to(zip_path: Path, dest_dir: Path) -> None:\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(dest_dir)\n",
    "\n",
    "    def _find_first(root: Path, patterns: list[str]) -> Path:\n",
    "        for pat in patterns:\n",
    "            hits = list(root.rglob(pat))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "        raise FileNotFoundError(f\"Could not find any of {patterns} under: {root}\")\n",
    "\n",
    "    # ---- Acquire input files ----\n",
    "    if OFFLINE_INPUT_SRC is None:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()  # upload either: a single file OR a zip containing the files\n",
    "        name = next(iter(uploaded.keys()))\n",
    "        p = Path(name).resolve()\n",
    "\n",
    "        # Clean previous restore\n",
    "        if RESTORED_OFFLINE_INPUT_DIR.exists():\n",
    "            shutil.rmtree(RESTORED_OFFLINE_INPUT_DIR)\n",
    "        RESTORED_OFFLINE_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if p.suffix.lower() == \".zip\":\n",
    "            print(\"Extracting:\", p, \"->\", RESTORED_OFFLINE_INPUT_DIR)\n",
    "            _extract_zip_to(p, RESTORED_OFFLINE_INPUT_DIR)\n",
    "            restore_root = RESTORED_OFFLINE_INPUT_DIR\n",
    "        else:\n",
    "            # single file uploaded\n",
    "            shutil.move(str(p), str(RESTORED_OFFLINE_INPUT_DIR / p.name))\n",
    "            restore_root = RESTORED_OFFLINE_INPUT_DIR\n",
    "    else:\n",
    "        restore_root = Path(OFFLINE_INPUT_SRC).expanduser().resolve()\n",
    "        if not restore_root.exists():\n",
    "            raise FileNotFoundError(f\"OFFLINE_INPUT_SRC not found: {restore_root}\")\n",
    "\n",
    "    # ---- Override configuration vars used by offline stage ----\n",
    "    if OFFLINE_INPUT_MODE == \"ontology\":\n",
    "        ont_path = _find_first(restore_root, [\"*.owl\", \"*.rdf\", \"*.ttl\", \"*.obo\"])\n",
    "        OFFLINE_ONT_PATH = str(ont_path)\n",
    "        OFFLINE_EXPORT_CSV = None  # ensure ontology mode\n",
    "        print(\"Restored offline INPUT (ontology):\")\n",
    "        print(\"   OFFLINE_ONT_PATH   =\", OFFLINE_ONT_PATH)\n",
    "        print(\"   OFFLINE_EXPORT_CSV =\", OFFLINE_EXPORT_CSV)\n",
    "\n",
    "    elif OFFLINE_INPUT_MODE == \"export_csv\":\n",
    "        export_csv = _find_first(restore_root, [\"*.csv\"])\n",
    "        OFFLINE_EXPORT_CSV = str(export_csv)\n",
    "        # When using export CSV, OFFLINE_ONT_PATH is not needed\n",
    "        print(\"Restored offline INPUT (export CSV):\")\n",
    "        print(\"   OFFLINE_EXPORT_CSV =\", OFFLINE_EXPORT_CSV)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"OFFLINE_INPUT_MODE must be 'ontology' or 'export_csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1fa05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d02362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN OFFLINE BUNDLE STAGE\n",
    "# ============================================\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "if not DO_OFFLINE:\n",
    "    print(\"Skipping offline bundle build (DO_OFFLINE=False).\")\n",
    "else:\n",
    "    offline_log = OFFLINE_DIR / \"offline_bundle.log\"\n",
    "\n",
    "    offline_cmd = [\n",
    "        \"python\", \"build_ontology_bundle.py\",\n",
    "        \"--out-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "        \"--out-bundle\", OFFLINE_BUNDLE_PKL,\n",
    "        \"--tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "        \"--bi-encoder-model-id\", BI_ENCODER_MODEL_ID,\n",
    "        \"--semantic-batch-size\", str(OFFLINE_SEMANTIC_BATCH_SIZE),\n",
    "        \"--semantic-max-length\", str(OFFLINE_SEMANTIC_MAX_LENGTH),\n",
    "    ]\n",
    "\n",
    "    if OFFLINE_NO_SEMANTIC_NORMALIZE:\n",
    "        offline_cmd.append(\"--no-semantic-normalize\")\n",
    "\n",
    "    if OFFLINE_EXPORT_CSV:\n",
    "        offline_cmd += [\"--export-csv\", OFFLINE_EXPORT_CSV]\n",
    "    else:\n",
    "        offline_cmd += [\"--ont-path\", OFFLINE_ONT_PATH]\n",
    "        if OFFLINE_PREFIX:\n",
    "            offline_cmd += [\"--prefix\", OFFLINE_PREFIX]\n",
    "\n",
    "    run_cmd(offline_cmd, offline_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nOffline bundle completed.\")\n",
    "    print(\"Ontology internal CSV:\", ONTOLOGY_INTERNAL_CSV)\n",
    "    print(\"Offline bundle PKL:\", OFFLINE_BUNDLE_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a246a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9cf12f",
   "metadata": {},
   "source": [
    "## Stage 3 - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deb1cbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f02ffe",
   "metadata": {},
   "source": [
    "### Optional: Restore artifacts for inference-only runs\n",
    "\n",
    "The following cells are **optional** and are only needed when you want to run **Inference without rebuilding artifacts in the current session**.\n",
    "\n",
    "They allow you to restore previously generated outputs (e.g. from another Colab session, a teammate, or a downloaded ZIP) and **override the corresponding configuration variables locally**, keeping everything self-contained.\n",
    "\n",
    "You typically use these cells when:\n",
    "- you already trained a model elsewhere and just want to run inference\n",
    "- you already built the offline bundle and want to reuse it\n",
    "- you want to run inference on a custom input CSV without re-running training or offline preprocessing\n",
    "\n",
    "If you are running the **full pipeline in this notebook session**, you can safely skip these cells.\n",
    "\n",
    "The restore logic is split into two parts:\n",
    "- **Offline artifacts restore** (ontology CSV + offline bundle)\n",
    "- **Model + inference input restore** (cross-encoder + optional custom query CSV)\n",
    "\n",
    "Both cells automatically override the variables used by the inference stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESTORE OFFLINE ARTIFACTS (offline bundle + ontology CSV)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "if not (DO_INFERENCE and RESTORE_OFFLINE):\n",
    "    print(\"Skipping: RESTORE_OFFLINE is False (or DO_INFERENCE is False).\")\n",
    "else:\n",
    "    # If you want to skip upload and point to an existing folder, set this:\n",
    "    # Example: OFFLINE_RESTORE_SRC = \"/content/my_run/offline\"\n",
    "    OFFLINE_RESTORE_SRC = None  # str path OR None to upload a .zip\n",
    "\n",
    "    RESTORE_ROOT = Path(OUT_DIR) / \"restored\"\n",
    "    RESTORED_OFFLINE_DIR = RESTORE_ROOT / \"offline\"\n",
    "\n",
    "    def _extract_zip_to(zip_path: Path, dest_dir: Path) -> None:\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(dest_dir)\n",
    "\n",
    "    def _find_first(root: Path, pattern: str) -> Path:\n",
    "        hits = list(root.rglob(pattern))\n",
    "        if not hits:\n",
    "            raise FileNotFoundError(f\"Could not find '{pattern}' under: {root}\")\n",
    "        return hits[0]\n",
    "\n",
    "    # ---- Acquire artifacts ----\n",
    "    if OFFLINE_RESTORE_SRC is None:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()  # choose a zip that contains offline_bundle.pkl and ontology_internal.csv\n",
    "        zip_name = next(iter(uploaded.keys()))\n",
    "        zip_path = Path(zip_name).resolve()\n",
    "\n",
    "        # Clean previous restore to avoid stale artifacts\n",
    "        if RESTORED_OFFLINE_DIR.exists():\n",
    "            shutil.rmtree(RESTORED_OFFLINE_DIR)\n",
    "        RESTORED_OFFLINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"Extracting:\", zip_path, \"->\", RESTORED_OFFLINE_DIR)\n",
    "        _extract_zip_to(zip_path, RESTORED_OFFLINE_DIR)\n",
    "        restore_root = RESTORED_OFFLINE_DIR\n",
    "    else:\n",
    "        restore_root = Path(OFFLINE_RESTORE_SRC).expanduser().resolve()\n",
    "        if not restore_root.exists():\n",
    "            raise FileNotFoundError(f\"OFFLINE_RESTORE_SRC not found: {restore_root}\")\n",
    "\n",
    "    # ---- Locate required files ----\n",
    "    bundle_pkl = _find_first(restore_root, \"offline_bundle.pkl\")\n",
    "    onto_csv = _find_first(restore_root, \"ontology_internal.csv\")\n",
    "\n",
    "    # ---- Override variables used by inference stage ----\n",
    "    OFFLINE_BUNDLE_PKL = str(bundle_pkl)\n",
    "    ONTOLOGY_INTERNAL_CSV = str(onto_csv)\n",
    "\n",
    "    print(\"Restored offline artifacts:\")\n",
    "    print(\"   OFFLINE_BUNDLE_PKL    =\", OFFLINE_BUNDLE_PKL)\n",
    "    print(\"   ONTOLOGY_INTERNAL_CSV =\", ONTOLOGY_INTERNAL_CSV)\n",
    "    print(\"   Bundle exists:\", Path(OFFLINE_BUNDLE_PKL).exists())\n",
    "    print(\"   CSV exists:\", Path(ONTOLOGY_INTERNAL_CSV).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a861e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESTORE MODEL + INFERENCE INPUT (for inference-only runs)\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "if not (DO_INFERENCE and RESTORE_MODEL):\n",
    "    print(\"Skipping: RESTORE_MODEL is False (or DO_INFERENCE is False).\")\n",
    "else:\n",
    "    # -----------------------------\n",
    "    # A) RESTORE MODEL (Cross-Encoder)\n",
    "    # -----------------------------\n",
    "\n",
    "    # If you want to skip upload and point to an existing folder, set this:\n",
    "    # Example: MODEL_RESTORE_SRC = \"/content/my_model/final_cross_encoder_model\"\n",
    "    MODEL_RESTORE_SRC = None  # str path OR None to upload a .zip\n",
    "\n",
    "    RESTORE_ROOT = Path(OUT_DIR) / \"restored\"\n",
    "    RESTORED_MODEL_DIR = RESTORE_ROOT / \"model\"\n",
    "\n",
    "    def _extract_zip_to(zip_path: Path, dest_dir: Path) -> None:\n",
    "        dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "            z.extractall(dest_dir)\n",
    "\n",
    "    def _find_cross_encoder_dir(root: Path) -> Path:\n",
    "        \"\"\"\n",
    "        Locate a SentenceTransformers CrossEncoder saved folder.\n",
    "        We accept a directory containing config.json (minimal proxy).\n",
    "        \"\"\"\n",
    "        if (root / \"config.json\").exists():\n",
    "            return root\n",
    "\n",
    "        candidates = list(root.rglob(\"config.json\"))\n",
    "        if not candidates:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find config.json under: {root}\\n\"\n",
    "                \"Your zip/folder should include the saved model directory (SentenceTransformers CrossEncoder).\"\n",
    "            )\n",
    "        return candidates[0].parent\n",
    "\n",
    "    # Acquire model artifacts\n",
    "    if MODEL_RESTORE_SRC is None:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()  # zip containing final_cross_encoder_model/...\n",
    "        zip_name = next(iter(uploaded.keys()))\n",
    "        zip_path = Path(zip_name).resolve()\n",
    "\n",
    "        if RESTORED_MODEL_DIR.exists():\n",
    "            shutil.rmtree(RESTORED_MODEL_DIR)\n",
    "        RESTORED_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        print(\"Extracting model zip:\", zip_path, \"->\", RESTORED_MODEL_DIR)\n",
    "        _extract_zip_to(zip_path, RESTORED_MODEL_DIR)\n",
    "        model_restore_root = RESTORED_MODEL_DIR\n",
    "    else:\n",
    "        model_restore_root = Path(MODEL_RESTORE_SRC).expanduser().resolve()\n",
    "        if not model_restore_root.exists():\n",
    "            raise FileNotFoundError(f\"MODEL_RESTORE_SRC not found: {model_restore_root}\")\n",
    "\n",
    "    cross_dir = _find_cross_encoder_dir(model_restore_root)\n",
    "\n",
    "    # Override variable used by inference stage\n",
    "    CROSS_ENCODER_MODEL_ID = str(cross_dir)\n",
    "\n",
    "    print(\"Restored cross-encoder model dir:\")\n",
    "    print(\"   CROSS_ENCODER_MODEL_ID =\", CROSS_ENCODER_MODEL_ID)\n",
    "    print(\"   Contains config.json:\", (Path(CROSS_ENCODER_MODEL_ID) / \"config.json\").exists())\n",
    "\n",
    "    # Optional: keep tokenizer aligned with restored model\n",
    "    # If you know the tokenizer id used, set it here; otherwise keep existing CROSS_TOKENIZER_NAME from config.\n",
    "    # CROSS_TOKENIZER_NAME = \"allenai/scibert_scivocab_uncased\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # B) (OPTIONAL) RESTORE / OVERRIDE INFERENCE INPUT + SCHEMA\n",
    "    # -----------------------------\n",
    "    # If you want to run inference on a custom CSV, enable this and upload it.\n",
    "    RESTORE_INFER_INPUT = False  # set True to upload your own input CSV (queries/attributes)\n",
    "\n",
    "    if not RESTORE_INFER_INPUT:\n",
    "        print(\"Inference input not restored (RESTORE_INFER_INPUT=False). Using INFER_INPUT_CSV from Configuration:\")\n",
    "        print(\"   INFER_INPUT_CSV =\", INFER_INPUT_CSV)\n",
    "        print(\"   RETRIEVAL_COL =\", RETRIEVAL_COL, \"| SCORING_COL =\", SCORING_COL, \"| ID_COL =\", ID_COL)\n",
    "    else:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()  # upload your custom input CSV\n",
    "        csv_name = next(iter(uploaded.keys()))\n",
    "        csv_path = Path(csv_name).resolve()\n",
    "\n",
    "        # Override input path\n",
    "        INFER_INPUT_CSV = str(csv_path)\n",
    "\n",
    "        # Override schema to match your custom CSV\n",
    "        # IMPORTANT: set these to columns that actually exist in your file.\n",
    "        # - RETRIEVAL_COL: used for exact+lexical retrieval\n",
    "        # - SCORING_COL: used as query text for semantic retrieval + cross-encoder scoring\n",
    "        # - ID_COL: carried through as identifier\n",
    "        RETRIEVAL_COL = \"source_label\"\n",
    "        SCORING_COL = \"source_text\"\n",
    "        ID_COL = \"source_iri\"\n",
    "\n",
    "        print(\"Restored/overrode inference input:\")\n",
    "        print(\"   INFER_INPUT_CSV =\", INFER_INPUT_CSV)\n",
    "        print(\"   RETRIEVAL_COL =\", RETRIEVAL_COL, \"| SCORING_COL =\", SCORING_COL, \"| ID_COL =\", ID_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079bc72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c967b14",
   "metadata": {},
   "source": [
    "### Inference parameter overrides (no restore required)\n",
    "\n",
    "This cell allows you to **change inference parameters even if you did NOT restore any artifacts**.\n",
    "\n",
    "If you already have valid artifacts on disk (e.g. from a previous full pipeline run in the same session), you can:\n",
    "\n",
    "- modify `INFER_INPUT_CSV` to point to a different query file\n",
    "- change the column schema (`RETRIEVAL_COL`, `SCORING_COL`, `ID_COL`)\n",
    "- tune retrieval and scoring parameters (top-k values, hybrid ratio, batch sizes, max length)\n",
    "- change the inference output path to avoid overwriting previous results\n",
    "\n",
    "No files are rebuilt and no artifacts are reloaded here:  \n",
    "this cell **only overrides variables** that will be consumed by the inference stage.\n",
    "\n",
    "If you skip both restore cells and run this override cell, inference will use:\n",
    "- the existing offline bundle\n",
    "- the existing trained model\n",
    "- the updated parameters defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779993ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INFERENCE OVERRIDES (quick tuning)\n",
    "# ============================================\n",
    "\n",
    "# Toggle: set True if you want to override defaults from Configuration\n",
    "OVERRIDE_INFERENCE_PARAMS = True\n",
    "\n",
    "if not OVERRIDE_INFERENCE_PARAMS:\n",
    "    print(\"Skipping inference overrides (OVERRIDE_INFERENCE_PARAMS=False).\")\n",
    "else:\n",
    "    # --- Input/Output ---\n",
    "    # If you want to run on a custom CSV, set it here (must exist on disk)\n",
    "    # INFER_INPUT_CSV = \"data/my_custom_queries.csv\"\n",
    "    # INFER_OUT_CSV   = str(INFER_DIR / \"predictions_custom.csv\")\n",
    "\n",
    "    # --- Column schema (must match your INFER_INPUT_CSV) ---\n",
    "    # Used for exact/lexical retrieval (typically a short label)\n",
    "    RETRIEVAL_COL = \"source_label\"\n",
    "    # Used for semantic retrieval + cross-encoder scoring (typically richer text)\n",
    "    SCORING_COL = \"source_text\"\n",
    "    # Carried through as identifier\n",
    "    ID_COL = \"source_iri\"\n",
    "\n",
    "    # --- Retrieval / scoring knobs ---\n",
    "    INFER_MODE = \"hybrid\"  # \"lexical\" | \"hybrid\"\n",
    "\n",
    "    RETRIEVAL_LEXICAL_TOP_K  = 100\n",
    "    RETRIEVAL_SEMANTIC_TOP_K = 100\n",
    "    RETRIEVAL_MERGED_TOP_K   = 150\n",
    "    HYBRID_RATIO_SEMANTIC    = 0.5  # 0..1\n",
    "\n",
    "    SEMANTIC_BATCH_SIZE = 64\n",
    "\n",
    "    CROSS_TOP_K       = 20\n",
    "    CROSS_BATCH_SIZE  = 32\n",
    "    CROSS_MAX_LENGTH  = 256\n",
    "\n",
    "    KEEP_TOP_N = 0  # 0 = only best prediction, >0 keeps extra columns\n",
    "\n",
    "    # --- Print final effective config (sanity) ---\n",
    "    print(\"Inference parameters set:\")\n",
    "    print(\"  INFER_INPUT_CSV =\", INFER_INPUT_CSV)\n",
    "    print(\"  INFER_OUT_CSV   =\", INFER_OUT_CSV)\n",
    "    print(\"  MODE            =\", INFER_MODE)\n",
    "    print(\"  RETRIEVAL_COL   =\", RETRIEVAL_COL)\n",
    "    print(\"  SCORING_COL     =\", SCORING_COL)\n",
    "    print(\"  ID_COL          =\", ID_COL)\n",
    "    print(\"  Lex/Sem/Merged  =\", RETRIEVAL_LEXICAL_TOP_K, RETRIEVAL_SEMANTIC_TOP_K, RETRIEVAL_MERGED_TOP_K)\n",
    "    print(\"  Hybrid ratio    =\", HYBRID_RATIO_SEMANTIC)\n",
    "    print(\"  Cross top-k     =\", CROSS_TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636254b",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "\n",
    "This cell executes the **inference stage only**, using whatever artifacts are currently available in the session.\n",
    "\n",
    "Important notes:\n",
    "\n",
    "- This cell **does not build or restore artifacts**.\n",
    "  It only consumes what already exists on disk or was restored earlier.\n",
    "- You can freely change inference parameters (input file, columns, top-k values, batch sizes, etc.)\n",
    "  before running this cell, without retraining or rebuilding the offline bundle.\n",
    "- This is the cell you typically re-run multiple times during experimentation.\n",
    "\n",
    "If `DO_INFERENCE=False`, the cell is skipped safely without side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RUN INFERENCE STAGE\n",
    "# ============================================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "if not DO_INFERENCE:\n",
    "    print(\"Skipping inference (DO_INFERENCE=False).\")\n",
    "else:\n",
    "    # Final sanity checks\n",
    "    if \"CROSS_ENCODER_MODEL_ID\" not in globals() or CROSS_ENCODER_MODEL_ID is None:\n",
    "        raise ValueError(\n",
    "            \"CROSS_ENCODER_MODEL_ID is not set. \"\n",
    "            \"Run training (DO_TRAINING=True) or restore model (RESTORE_MODEL=True).\"\n",
    "        )\n",
    "\n",
    "    if not Path(OFFLINE_BUNDLE_PKL).exists():\n",
    "        raise FileNotFoundError(f\"OFFLINE_BUNDLE_PKL not found: {OFFLINE_BUNDLE_PKL}\")\n",
    "    if not Path(ONTOLOGY_INTERNAL_CSV).exists():\n",
    "        raise FileNotFoundError(f\"ONTOLOGY_INTERNAL_CSV not found: {ONTOLOGY_INTERNAL_CSV}\")\n",
    "    if not Path(INFER_INPUT_CSV).exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"INFER_INPUT_CSV not found: {INFER_INPUT_CSV}\\n\"\n",
    "            \"Set INFER_INPUT_CSV to your custom file, or enable RESTORE_INFER_INPUT inside the restore cell.\"\n",
    "        )\n",
    "\n",
    "    infer_log = INFER_DIR / \"inference.log\"\n",
    "\n",
    "    infer_cmd = [\n",
    "        \"python\", \"run_inference.py\",\n",
    "        \"--bundle\", OFFLINE_BUNDLE_PKL,\n",
    "        \"--ontology-csv\", ONTOLOGY_INTERNAL_CSV,\n",
    "        \"--input-csv\", INFER_INPUT_CSV,\n",
    "        \"--out-csv\", INFER_OUT_CSV,\n",
    "        \"--mode\", INFER_MODE,\n",
    "        \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "        \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "        \"--retrieval-col\", RETRIEVAL_COL,\n",
    "        \"--retrieval-lexical-top-k\", str(RETRIEVAL_LEXICAL_TOP_K),\n",
    "        \"--retrieval-semantic-top-k\", str(RETRIEVAL_SEMANTIC_TOP_K),\n",
    "        \"--retrieval-merged-top-k\", str(RETRIEVAL_MERGED_TOP_K),\n",
    "        \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEMANTIC),\n",
    "        \"--semantic-batch-size\", str(SEMANTIC_BATCH_SIZE),\n",
    "        \"--cross-top-k\", str(CROSS_TOP_K),\n",
    "        \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "        \"--cross-max-length\", str(CROSS_MAX_LENGTH),\n",
    "        \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "    ]\n",
    "\n",
    "    if SCORING_COL:\n",
    "        infer_cmd += [\"--scoring-col\", SCORING_COL]\n",
    "    if ID_COL:\n",
    "        infer_cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "    run_cmd(infer_cmd, infer_log, cwd=REPO_ROOT)\n",
    "\n",
    "    print(\"\\nInference completed.\")\n",
    "    print(\"Predictions CSV:\", INFER_OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2afa4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a0dfd",
   "metadata": {},
   "source": [
    "### 2.1) Download predictions and (optional) gold standard\n",
    "\n",
    "This cell is a **results export utility**.\n",
    "\n",
    "It allows you to download the outputs of the inference stage in a **single, self-contained ZIP archive** that also includes the effective run configuration.\n",
    "\n",
    "**What this cell does:**\n",
    "\n",
    "- Always includes:\n",
    "  - `predictions.csv` produced by the inference stage\n",
    "- Optionally includes a gold standard file:\n",
    "  - automatically, if `*.test.gold.csv` exists (full / build-dataset pipeline), or\n",
    "  - manually, if you provide a custom gold CSV path in the cell\n",
    "- Always generates:\n",
    "  - a ZIP archive containing:\n",
    "    - `predictions.csv`\n",
    "    - the gold CSV (if available)\n",
    "    - `config.txt`, capturing the **exact configuration** used for the run\n",
    "\n",
    "**How to use it:**\n",
    "\n",
    "- **Full pipeline**  \n",
    "  Just run the cell: the gold file is detected automatically.\n",
    "- **Inference-only / external evaluation**  \n",
    "  Provide a path to your own gold CSV (or leave it empty).\n",
    "- **No gold available**  \n",
    "  Leave the gold path as `None`: the ZIP will still be created with predictions + config.\n",
    "\n",
    "This cell does **not** run any computation.  \n",
    "It only packages artifacts already produced by previous stages, together with their configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD ARTIFACTS (predictions + optional gold + config)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab-only helper\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"This cell is intended for Google Colab (google.colab not available).\") from e\n",
    "\n",
    "# =====================================================\n",
    "# USER OVERRIDE (optional)\n",
    "# =====================================================\n",
    "# If you have a gold file produced elsewhere, set it here.\n",
    "# Example:\n",
    "# GOLD_PATH_OVERRIDE = \"/content/my_eval/gold_truth.csv\"\n",
    "GOLD_PATH_OVERRIDE = None  # str path or None\n",
    "\n",
    "# =====================================================\n",
    "# REQUIRED: predictions\n",
    "# =====================================================\n",
    "pred_path = Path(INFER_OUT_CSV)\n",
    "\n",
    "if not pred_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"predictions file not found:\\n - {pred_path}\\n\\n\"\n",
    "        \"Notes:\\n\"\n",
    "        \" - predictions.csv is produced by the inference stage.\\n\"\n",
    "        \" - Run Stage 3 (Inference) first, or set INFER_OUT_CSV correctly.\"\n",
    "    )\n",
    "\n",
    "print(\"Found predictions:\", pred_path)\n",
    "\n",
    "# =====================================================\n",
    "# OPTIONAL: gold\n",
    "# =====================================================\n",
    "gold_candidates = []\n",
    "\n",
    "# 1) Explicit user-provided gold\n",
    "if GOLD_PATH_OVERRIDE is not None:\n",
    "    gold_candidates.append(Path(GOLD_PATH_OVERRIDE))\n",
    "\n",
    "# 2) Default full-pipeline gold location\n",
    "gold_candidates.append(\n",
    "    Path(str(Path(OUT_DATASET_CSV).with_suffix(\".test.gold.csv\")))\n",
    ")\n",
    "\n",
    "gold_path = next((p for p in gold_candidates if p.exists()), None)\n",
    "\n",
    "if gold_path is None:\n",
    "    print(\n",
    "        \"\\nGold file NOT found (this is OK for inference-only runs).\\n\"\n",
    "        \"Tried the following paths:\\n\"\n",
    "        + \"\\n\".join(f\" - {p}\" for p in gold_candidates)\n",
    "    )\n",
    "else:\n",
    "    print(\"Found gold:\", gold_path)\n",
    "\n",
    "# =====================================================\n",
    "# CREATE ZIP (with config.txt)\n",
    "# =====================================================\n",
    "files_to_zip = [pred_path]\n",
    "if gold_path is not None:\n",
    "    files_to_zip.append(gold_path)\n",
    "\n",
    "zip_name = \"predictions_and_gold.zip\" if gold_path else \"predictions_only.zip\"\n",
    "zip_path = pred_path.parent / zip_name\n",
    "\n",
    "make_zip_with_config(\n",
    "    zip_path=zip_path,\n",
    "    files_to_include=files_to_zip,\n",
    "    config_dir=pred_path.parent,   # config.txt lives next to outputs\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6306fb11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf0e62",
   "metadata": {},
   "source": [
    "## Optional: package outputs for download (run only at the end)\n",
    "\n",
    "This is only needed if you want to export artifacts locally.\n",
    "Skip it if you are iterating quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ed6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DOWNLOAD FULL RUN (outputs + config)\n",
    "# ============================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Colab-only helper\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"This cell is intended for Google Colab (google.colab not available).\") from e\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Zip the entire run directory + config.txt\n",
    "# -------------------------------------------------\n",
    "run_dir = Path(OUT_DIR)\n",
    "if not run_dir.exists():\n",
    "    raise FileNotFoundError(f\"OUT_DIR not found: {run_dir}\")\n",
    "\n",
    "zip_path = run_dir.with_suffix(\".zip\")\n",
    "\n",
    "print(\"Creating full run archive:\")\n",
    "print(\"  Source:\", run_dir)\n",
    "print(\"  Target:\", zip_path)\n",
    "\n",
    "# Collect all files under OUT_DIR (recursive)\n",
    "files_to_zip = [p for p in run_dir.rglob(\"*\") if p.is_file()]\n",
    "\n",
    "make_zip_with_config(\n",
    "    zip_path=zip_path,\n",
    "    files_to_include=files_to_zip,\n",
    "    config_dir=run_dir,   # config.txt written inside the run folder\n",
    "    download=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
