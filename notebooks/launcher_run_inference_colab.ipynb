{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a599820b",
   "metadata": {},
   "source": [
    "# Inference Launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1898011",
   "metadata": {},
   "source": [
    "## Purpose of this notebook\n",
    "This notebook is a script-first, notebook-as-launcher interface for running ontology-attribute alignment inference on Colab.\n",
    "It will:\n",
    "- clone the repository,\n",
    "- install dependencies,\n",
    "- load the required inference artifacts (offline bundle + ontology CSV + input attributes CSV),\n",
    "- run inference via the CLI script run_inference.py,\n",
    "- package outputs into a zip file for download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e58f9",
   "metadata": {},
   "source": [
    "## What is produced by this notebook\n",
    "\n",
    "Running this notebook produces the complete outputs of an ontology–attribute alignment inference run.\n",
    "\n",
    "Specifically, the notebook generates:\n",
    "- a predictions CSV containing one row per input attribute, including:\n",
    "    - the predicted ontology class IRI,\n",
    "    - the associated confidence score,\n",
    "    - metadata about the retrieval and scoring process,\n",
    "    - optionally, the top-N scored candidate classes per attribute;\n",
    "- a full execution log capturing:\n",
    "    - the exact command used to run run_inference.py,\n",
    "    - retrieval and scoring progress,\n",
    "    - potential warnings or errors;\n",
    "- a self-contained run directory, uniquely identified by a timestamp-based run ID, which includes:\n",
    "    - predictions.csv,\n",
    "    - run_inference.log,\n",
    "    - any additional artifacts produced during inference.\n",
    "\n",
    "All outputs are saved in a single run directory under outputs/ and are automatically packaged into a ZIP file, which can be downloaded to the local machine at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723684d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb1fd6",
   "metadata": {},
   "source": [
    "### Clone repository\n",
    "\n",
    "This cell clones the project repository into the Colab VM.\n",
    "Optionally, you can lock the code to a specific commit hash to ensure that future runs remain identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c71e5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "REPO_URL=\"https://github.com/adsp-polito/2025-P13-Ontology-Alignment.git\"\n",
    "REPO_DIR=\"2025-P13-Ontology-Alignment\"\n",
    "\n",
    "# Optional: lock to a specific commit for full reproducibility\n",
    "COMMIT=\"\"   # e.g. \"4ffd790\"\n",
    "\n",
    "rm -rf \"$REPO_DIR\"\n",
    "git clone \"$REPO_URL\" \"$REPO_DIR\"\n",
    "cd \"$REPO_DIR\"\n",
    "\n",
    "if [ -n \"$COMMIT\" ]; then\n",
    "  git checkout \"$COMMIT\"\n",
    "fi\n",
    "\n",
    "echo \"Checked out commit:\"\n",
    "git rev-parse HEAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cef0c9",
   "metadata": {},
   "source": [
    "### Enter the repo directory\n",
    "\n",
    "Colab runs each bash cell in its own subshell.\n",
    "To keep the notebook state consistent, we move into the cloned repository using %cd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15aeea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd 2025-P13-Ontology-Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee355b0f",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "This cell upgrades pip and installs all required Python packages from requirements.txt.\n",
    "This ensures the notebook can run in a clean Colab environment without relying on local state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cd3116",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "python -m pip install --upgrade pip\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb79395",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be08e12",
   "metadata": {},
   "source": [
    "### Check runtime (CPU/GPU)\n",
    "Inference can run on CPU, but it is typically much faster on GPU, especially for:\n",
    "- semantic retrieval (bi-encoder query encoding),\n",
    "- cross-encoder scoring (the expensive step).\n",
    "\n",
    "This cell confirms whether CUDA is available and prints the GPU name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590207d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4efc41c",
   "metadata": {},
   "source": [
    "### Provide required input artifacts\n",
    "run_inference.py needs three inputs:\n",
    "1.\tOffline bundle: offline_bundle.pkl.\n",
    "2.\tOntology CSV: internal_ontology.csv containing at least columns iri and text.\n",
    "3.\tInput CSV: a dataset containing the attribute strings to align."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a7961",
   "metadata": {},
   "source": [
    "#### Option A: Upload files manually (recommended for quick tests)\n",
    "\n",
    "This cell opens a file picker and lets you upload local files into the Colab VM.\n",
    "\n",
    "Important note about semantic embeddings:\n",
    "If your offline bundle contains a semantic index whose embeddings were saved separately (e.g., .npy), you must upload those files too, preserving the same relative paths, otherwise semantic retrieval will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "uploaded = files.upload()  # select offline_bundle.pkl, internal_ontology.csv, input.csv, and any embedding files if needed\n",
    "for fn, content in uploaded.items():\n",
    "    out_path = os.path.join(\"data\", fn)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac85be",
   "metadata": {},
   "source": [
    "#### Option B: Upload a zipped offline-bundle output directory\n",
    "\n",
    "If your offline-bundle launcher produced a full run directory (with offline_bundle.pkl, embeddings, logs, and internal_ontology.csv), it is safest to upload it as a .zip and unzip it here.\n",
    "This avoids path mismatches for embedding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os, zipfile\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "uploaded = files.upload()  # choose something like offline_bundle_run_YYYYMMDD_HHMMSS.zip\n",
    "zip_name = next(iter(uploaded.keys()))\n",
    "\n",
    "with open(zip_name, \"wb\") as f:\n",
    "    f.write(uploaded[zip_name])\n",
    "\n",
    "with zipfile.ZipFile(zip_name, \"r\") as z:\n",
    "    z.extractall(\"data\")\n",
    "\n",
    "print(\"Extracted zip into: data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb409c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe59e8",
   "metadata": {},
   "source": [
    "### Configure output directory for this inference run\n",
    "\n",
    "This creates a unique output directory using a timestamp-based run ID, so multiple runs do not overwrite each other.\n",
    "We also store the “latest output directory path” into a small text file so later cells can automatically reuse it without manual edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13744d7f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "\n",
    "RUN_ID=\"inference_run_$(date +%Y%m%d_%H%M%S)\"\n",
    "OUT_DIR=\"outputs/${RUN_ID}\"\n",
    "mkdir -p \"$OUT_DIR\"\n",
    "\n",
    "echo \"$OUT_DIR\" > outputs/LAST_INFERENCE_RUN_DIR.txt\n",
    "\n",
    "echo \"OUT_DIR=$OUT_DIR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd3579e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b633f",
   "metadata": {},
   "source": [
    "### Configure inference parameters\n",
    "\n",
    "This cell defines the parameters that will be passed to run_inference.py, including:\n",
    "- the paths to input artifacts,\n",
    "- which column contains attribute text,\n",
    "- retrieval mode (lexical or hybrid),\n",
    "- model IDs,\n",
    "- batch sizes and top-k values.\n",
    "\n",
    "You can edit these values without touching the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46372f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Paths inside Colab VM\n",
    "# -----------------------\n",
    "BUNDLE_PATH = \"data/offline_bundle.pkl\"\n",
    "ONTOLOGY_CSV = \"data/internal_ontology.csv\"\n",
    "INPUT_CSV = \"data/input.csv\"\n",
    "\n",
    "# -----------------------\n",
    "# Input schema\n",
    "# -----------------------\n",
    "ATTR_COL = \"attribute\"\n",
    "ID_COL = None  # set to something like \"attribute_id\" if you want to carry an ID through\n",
    "\n",
    "# -----------------------\n",
    "# Retrieval mode\n",
    "# -----------------------\n",
    "MODE = \"hybrid\"  # \"lexical\" or \"hybrid\"\n",
    "\n",
    "# -----------------------\n",
    "# Models (HF model ids)\n",
    "# -----------------------\n",
    "CROSS_TOKENIZER_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "CROSS_ENCODER_MODEL_ID = \"YOUR_CROSS_ENCODER_HF_ID\"  # REQUIRED\n",
    "\n",
    "# -----------------------\n",
    "# Device\n",
    "# -----------------------\n",
    "DEVICE = None  # None = auto, or \"cuda\", or \"cpu\"\n",
    "\n",
    "# -----------------------\n",
    "# Retrieval params\n",
    "# -----------------------\n",
    "RETR_LEX_TOPK = 120\n",
    "RETR_SEM_TOPK = 120\n",
    "RETR_MERGED_TOPK = 200\n",
    "HYBRID_RATIO_SEM = 0.5\n",
    "SEM_BATCH_SIZE = 64\n",
    "\n",
    "# -----------------------\n",
    "# Cross-encoder scoring params\n",
    "# -----------------------\n",
    "CROSS_TOPK = 20\n",
    "CROSS_BATCH_SIZE = 32\n",
    "CROSS_MAX_LEN = 256\n",
    "\n",
    "# -----------------------\n",
    "# Output detail\n",
    "# -----------------------\n",
    "KEEP_TOP_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1b880",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b550db8",
   "metadata": {},
   "source": [
    "### Run inference via the CLI script\n",
    "\n",
    "This cell runs the full inference pipeline by invoking the script run_inference.py with CLI arguments.\n",
    "\n",
    "What happens inside the script:\n",
    "- Stage A retrieval (exact + lexical + semantic depending on mode)\n",
    "- Stage B cross-encoder scoring for selected candidate pairs\n",
    "- A final predictions CSV is written to the run output directory\n",
    "\n",
    "All stdout/stderr is captured into a log file, so runs are traceable and debuggable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "with open(\"outputs/LAST_INFERENCE_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "pred_csv = f\"{out_dir}/predictions.csv\"\n",
    "log_path = f\"{out_dir}/run_inference.log\"\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"run_inference.py\",\n",
    "    \"--bundle\", BUNDLE_PATH,\n",
    "    \"--ontology-csv\", ONTOLOGY_CSV,\n",
    "    \"--input-csv\", INPUT_CSV,\n",
    "    \"--out-csv\", pred_csv,\n",
    "    \"--attr-col\", ATTR_COL,\n",
    "    \"--mode\", MODE,\n",
    "    \"--cross-tokenizer-name\", CROSS_TOKENIZER_NAME,\n",
    "    \"--cross-encoder-model-id\", CROSS_ENCODER_MODEL_ID,\n",
    "    \"--retrieval-lexical-top-k\", str(RETR_LEX_TOPK),\n",
    "    \"--retrieval-semantic-top-k\", str(RETR_SEM_TOPK),\n",
    "    \"--retrieval-merged-top-k\", str(RETR_MERGED_TOPK),\n",
    "    \"--hybrid-ratio-semantic\", str(HYBRID_RATIO_SEM),\n",
    "    \"--semantic-batch-size\", str(SEM_BATCH_SIZE),\n",
    "    \"--cross-top-k\", str(CROSS_TOPK),\n",
    "    \"--cross-batch-size\", str(CROSS_BATCH_SIZE),\n",
    "    \"--cross-max-length\", str(CROSS_MAX_LEN),\n",
    "    \"--keep-top-n\", str(KEEP_TOP_N),\n",
    "]\n",
    "\n",
    "if ID_COL is not None:\n",
    "    cmd += [\"--id-col\", ID_COL]\n",
    "\n",
    "if DEVICE is not None:\n",
    "    cmd += [\"--device\", DEVICE]\n",
    "\n",
    "print(\"Running command:\\n\", \" \".join(cmd))\n",
    "with open(log_path, \"w\") as log_file:\n",
    "    proc = subprocess.run(cmd, stdout=log_file, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Return code:\", proc.returncode)\n",
    "print(\"Predictions:\", pred_csv)\n",
    "print(\"Log:\", log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75d6c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8db5d",
   "metadata": {},
   "source": [
    "### Inspect the output\n",
    "\n",
    "This cell loads the generated predictions.csv and shows a preview.\n",
    "It is a sanity check to confirm the run produced results in the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7faf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"outputs/LAST_INFERENCE_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "pred_csv = f\"{out_dir}/predictions.csv\"\n",
    "df = pd.read_csv(pred_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e3f8a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa50a9",
   "metadata": {},
   "source": [
    "### Package outputs for download\n",
    "\n",
    "The inference run produces multiple useful files:\n",
    "- predictions.csv\n",
    "- run_inference.log\n",
    "\n",
    "To keep runs reproducible and shareable, we package the entire run directory into a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e37d66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "OUT_DIR=$(cat outputs/LAST_INFERENCE_RUN_DIR.txt)\n",
    "ZIP_PATH=\"${OUT_DIR}.zip\"\n",
    "zip -r \"$ZIP_PATH\" \"$OUT_DIR\"\n",
    "echo \"Created zip: $ZIP_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2fe51",
   "metadata": {},
   "source": [
    "This final cell downloads the zip file from the Colab VM to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "with open(\"outputs/LAST_INFERENCE_RUN_DIR.txt\") as f:\n",
    "    out_dir = f.read().strip()\n",
    "\n",
    "files.download(f\"{out_dir}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2e8522",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba319c",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- If semantic retrieval does not work, it’s usually because embeddings were saved separately and were not uploaded/unzipped with the correct paths. Best practice: upload/unzip the entire offline-bundle run directory.\n",
    "- If you hit GPU out-of-memory:\n",
    "- reduce CROSS_BATCH_SIZE (first knob),\n",
    "- reduce SEM_BATCH_SIZE,\n",
    "- reduce CROSS_TOPK (reduces the number of scored pairs),\n",
    "- optionally reduce RETR_*_TOPK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OAvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
